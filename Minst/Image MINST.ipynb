{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cProfile\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import cv2 as cv2\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog, graycomatrix, graycoprops\n",
    "from skimage.filters import gabor_kernel, gabor\n",
    "from skimage import img_as_float32\n",
    "from skimage.transform import resize\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('mnist_train.csv')\n",
    "test  = pd.read_csv('mnist_test.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "\n",
    "train_target = train['label']\n",
    "test_target = test['label']\n",
    "\n",
    "train_values = train.drop('label', axis=1)\n",
    "test_values = test.drop('label', axis=1)\n",
    "\n",
    "train_values = train_values.values.reshape(-1, 28, 28) \n",
    "test_values = test_values.values.reshape(-1, 28, 28)\n",
    "\n",
    "train_target_encoded = encoder.fit_transform(train_target.values.reshape(-1, 1)).toarray()\n",
    "test_target_encoded = encoder.transform(test_target.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(\"Shape of train_values:\", train_values.shape)\n",
    "print(\"Shape of train_target_encoded:\", train_target_encoded.shape)\n",
    "\n",
    "print(\"Shape of test_values:\", test_values.shape)\n",
    "print(\"Shape of test_target_encoded:\", test_target_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_values\n",
    "X_train = train_values.astype(np.uint8)\n",
    "\n",
    "y_train_ENC = train_target_encoded\n",
    "y_train = np.argmax(y_train_ENC, axis=1)\n",
    "\n",
    "X_test = test_values\n",
    "X_test = test_values.astype(np.uint8)\n",
    "\n",
    "y_test_ENC = test_target_encoded\n",
    "y_test = np.argmax(y_test_ENC, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacja histogramem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sprawdzenie i konwersja typu danych\n",
    "# if X_train.dtype != np.uint8:\n",
    "#     X_train = X_train.astype(np.float32)\n",
    "# if X_test.dtype != np.uint8:\n",
    "#     X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Sprawdzenie i ewentualna korekta kształtu danych\n",
    "if X_train.shape[1:] != (28, 28):\n",
    "    X_train = X_train.reshape(-1, 28, 28)\n",
    "if X_test.shape[1:] != (28, 28):\n",
    "    X_test = X_test.reshape(-1, 28, 28)\n",
    "\n",
    "# Wyrównywanie histogramu\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = cv2.equalizeHist(X_train[i])\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = cv2.equalizeHist(X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powiększanie i wygładzenie obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlarge_and_smooth(image, new_size):\n",
    "    \"\"\"Powiększa obraz za pomocą interpolacji bicubic.\"\"\"\n",
    "    enlarged_image = cv2.resize(image, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "    return enlarged_image\n",
    "\n",
    "# Sztuczne zwiększenie rozmiaru obrazu \n",
    "sample_indices = np.random.choice(X_train.shape[0], 5, replace=False)\n",
    "\n",
    "# Dla każdego wybranego obrazu\n",
    "for idx in sample_indices:\n",
    "    # Oryginalny obraz\n",
    "    original_image = X_train[idx]\n",
    "\n",
    "    # Powiększony i wygładzony obraz\n",
    "    enlarged_smoothed_image = enlarge_and_smooth(original_image, new_size=(42, 42))  # Przykładowy nowy rozmiar\n",
    "\n",
    "    # Wyświetl oryginalny i przekształcony obraz obok siebie\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    sns.heatmap(original_image, cmap=\"viridis\", ax=axes[0])\n",
    "    axes[0].set_title(f'Oryginalny obraz ({idx})')\n",
    "\n",
    "    sns.heatmap(enlarged_smoothed_image, cmap=\"viridis\", ax=axes[1])\n",
    "    axes[1].set_title(f'Powiększony i wygładzony obraz (Lanczos4) ({idx})')\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie modelu wykorzystując SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Baseline accuracy without transformations\n",
    "# X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(X_train, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "# qda_base = QDA()\n",
    "# qda_base.fit(X_train_base.reshape(X_train_base.shape[0], -1), y_train_base)  # Flatten images for QDA\n",
    "# y_pred_qda_base = qda_base.predict(X_val_base.reshape(X_val_base.shape[0], -1))\n",
    "# accuracy_qda_base = accuracy_score(y_val_base, y_pred_qda_base)\n",
    "\n",
    "# svm_base = SVC()\n",
    "# svm_base.fit(X_train_base.reshape(X_train_base.shape[0], -1), y_train_base)  # Flatten images for SVM\n",
    "# y_pred_svm_base = svm_base.predict(X_val_base.reshape(X_val_base.shape[0], -1))\n",
    "# accuracy_svm_base = accuracy_score(y_val_base, y_pred_svm_base)\n",
    "\n",
    "# print(\"Baseline QDA accuracy:\", accuracy_qda_base)\n",
    "# print(\"Baseline SVM accuracy:\", accuracy_svm_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline QDA accuracy: 0.5525\n",
    "\n",
    "\n",
    "Baseline SVM accuracy: 0.9793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deklaracja funkcji dla ekstrakcji cech z obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_hog_parallel(images, orientations=None, pixels_per_cell=None, cells_per_block=None, n_jobs=-1):\n",
    "    \"\"\"Oblicza deskryptory HOG dla obrazu.\"\"\"\n",
    "    \n",
    "     # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if orientations is None:\n",
    "        orientations = 9\n",
    "    if pixels_per_cell is None:\n",
    "        pixels_per_cell = (8, 8)\n",
    "    if cells_per_block is None:\n",
    "        cells_per_block = (2, 2)\n",
    "    \n",
    "    def calculate_hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block):\n",
    "        \n",
    "        image = img_as_float32(image)\n",
    "\n",
    "        fd = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                cells_per_block=cells_per_block, visualize=False, channel_axis=None)\n",
    "\n",
    "        return fd\n",
    "    \n",
    "    return Parallel(n_jobs=n_jobs)(delayed(calculate_hog)(image, orientations, pixels_per_cell, cells_per_block) for image in images)\n",
    "\n",
    "\n",
    "def apply_gabor_filters_parallel(images, thetas=None, sigmas=None, frequencies=None, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Stosuje filtry Gabora do wielu obrazów równolegle.\n",
    "\n",
    "    Argumenty:\n",
    "        images: Lista obrazów wejściowych.\n",
    "        thetas: (Opcjonalnie) Lista kątów orientacji filtrów Gabora (w radianach). Jeśli None, używane są wartości domyślne.\n",
    "        sigmas: (Opcjonalnie) Lista odchyleń standardowych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        frequencies: (Opcjonalnie) Lista częstotliwości przestrzennych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        n_jobs: Liczba rdzeni procesora do wykorzystania podczas zrównoleglania. -1 oznacza użycie wszystkich dostępnych rdzeni.\n",
    "\n",
    "    Zwraca:\n",
    "        Listę tablic NumPy zawierających obrazy przefiltrowane filtrami Gabora, o typie danych float32.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if thetas is None:\n",
    "        thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    if sigmas is None:\n",
    "        sigmas = [1, 3]\n",
    "    if frequencies is None:\n",
    "        frequencies = [0.05, 0.25]\n",
    "\n",
    "    def apply_gabor_filters_single(image):\n",
    "        image = img_as_float32(image)\n",
    "        filtered_images = [gabor(image, frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)[0]\n",
    "                           for theta in thetas for sigma in sigmas for frequency in frequencies]\n",
    "        return np.array(filtered_images, dtype=np.float32)\n",
    "\n",
    "    return Parallel(n_jobs=n_jobs)(delayed(apply_gabor_filters_single)(image) for image in images)\n",
    "\n",
    "\n",
    "def extract_features_HOG_Gabor(X, hog_orientations=None, hog_pixels_per_cell=None, hog_cells_per_block=None, \n",
    "                              gabor_thetas=None, gabor_sigmas=None, gabor_frequencies=None):\n",
    "    \"\"\"Generator zwracający cechy HOG i Gabora dla obrazów.\"\"\"\n",
    "\n",
    "    hog_features_list = calculate_hog_parallel(X, orientations=hog_orientations, pixels_per_cell=hog_pixels_per_cell, cells_per_block=hog_cells_per_block)\n",
    "    gabor_features_list = apply_gabor_filters_parallel(X, thetas=gabor_thetas, sigmas=gabor_sigmas, frequencies=gabor_frequencies)\n",
    "\n",
    "    for hog_features, gabor_features in zip(hog_features_list, gabor_features_list):\n",
    "        \n",
    "        gabor_features_flattened = np.array([img.flatten() for img in gabor_features])\n",
    "\n",
    "        # Dopasowujemy kształt hog_features do gabor_features_flattened\n",
    "        hog_features_reshaped = np.repeat(hog_features.reshape(1, -1), gabor_features_flattened.shape[0], axis=0)\n",
    "\n",
    "        # Łączymy cechy HOG i Gabora w poziomie\n",
    "        concatenated_features = np.hstack((hog_features_reshaped, gabor_features_flattened))\n",
    "\n",
    "        concatenated_features = concatenated_features.flatten()\n",
    "\n",
    "        yield concatenated_features\n",
    "\n",
    "#################################################################################################################################################\n",
    "################################################  Directional filters, ORB    ###################################################################\n",
    "\n",
    "def apply_directional_filters(image, kernel_sizes=[3, 5, 7]):\n",
    "    \"\"\"Stosuje filtry kierunkowe do obrazu.\"\"\"\n",
    "    filtered_images = []\n",
    "    for kernel_size in kernel_sizes:\n",
    "        kernel_horizontal = cv2.getDerivKernels(1, 0, kernel_size, normalize=True)\n",
    "        kernel_vertical = cv2.getDerivKernels(0, 1, kernel_size, normalize=True)\n",
    "        filtered_horizontal = cv2.filter2D(image, cv2.CV_32F, kernel_horizontal[0])\n",
    "        filtered_vertical = cv2.filter2D(image, cv2.CV_32F, kernel_vertical[0])\n",
    "        filtered_images.append(filtered_horizontal.flatten())\n",
    "        filtered_images.append(filtered_vertical.flatten())\n",
    "        if kernel_size % 2 == 0 or kernel_size > 31:\n",
    "            raise ValueError(f\"Nieprawidłowy rozmiar jądra: {kernel_size}. Rozmiar jądra musi być nieparzysty i nie większy niż 31.\")\n",
    "    return np.array(filtered_images).squeeze()\n",
    "\n",
    "def extract_orb_features(image, nfeatures=500):\n",
    "    \"\"\"Wyodrębnia cechy ORB z obrazu.\"\"\"\n",
    "    orb = cv2.ORB_create(nfeatures=nfeatures)  \n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        return np.zeros(nfeatures * 32)  \n",
    "    return descriptors.flatten()\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################################################################################\n",
    "################################################  GLCM, Zernike    ##############################################################################\n",
    "def extract_glcm_features(image, distances=[1], angles=[0], properties=['contrast', 'energy', 'homogeneity', 'correlation']):\n",
    "    \"\"\"Wyodrębnia cechy GLCM z obrazu.\"\"\"\n",
    "    if len(image.shape) > 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    glcm = graycomatrix(image, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    features = []\n",
    "    for prop in properties:\n",
    "        features.extend(graycoprops(glcm, prop).flatten())\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testowanie modelu dla HOG i filtrów Gabora i porównanie wydajności przy zastosowaniu redukcji wymiarów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_evaluate_models(X_train, y_train, use_pca=True):\n",
    "#     \"\"\"\n",
    "#     Trenuje i ocenia modele QDA i SVC, opcjonalnie z redukcją wymiarów PCA.\n",
    "\n",
    "#     Argumenty:\n",
    "#         X_train: Dane treningowe.\n",
    "#         y_train: Etykiety treningowe.\n",
    "#         use_pca: Flaga wskazująca, czy używać PCA (domyślnie True).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Ekstrakcja cech HOG i Gabor za pomocą generatora\n",
    "#     X_train_features = np.array(list(extract_features_HOG_Gabor(X_train)))\n",
    "\n",
    "#     # Podział na zbiór treningowy i walidacyjny\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train_features, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "#     # Redukcja wymiarów (opcjonalnie)\n",
    "#     if use_pca:\n",
    "#         pca_dim = PCA(n_components=50)\n",
    "#         X_train = pca_dim.fit_transform(X_train)\n",
    "#         X_val = pca_dim.transform(X_val)\n",
    "\n",
    "#     # Tworzenie i trenowanie modelu QDA\n",
    "#     qda = QDA()\n",
    "#     qda.fit(X_train, y_train)\n",
    "\n",
    "#     # Tworzenie i trenowanie modelu SVC\n",
    "#     svc = SVC(kernel='rbf')\n",
    "#     svc.fit(X_train, y_train)\n",
    "\n",
    "#     # Ocena modelu\n",
    "#     y_pred_qda = qda.predict(X_val)\n",
    "#     accuracy_qda = accuracy_score(y_val, y_pred_qda)\n",
    "\n",
    "#     y_pred_svc = svc.predict(X_val)\n",
    "#     accuracy_svc = accuracy_score(y_val, y_pred_svc)\n",
    "\n",
    "#     print(\"Dokładność QDA HOG i Gabor\", \"po PCA:\" if use_pca else \":\", accuracy_qda)\n",
    "#     print(\"Dokładność SVC HOG i Gabor\", \"po PCA:\" if use_pca else \":\", accuracy_svc)\n",
    "\n",
    "# # Test z PCA\n",
    "# train_and_evaluate_models(X_train, y_train, use_pca=True)\n",
    "#train_and_evaluate_models(X_train, y_train, use_pca=False)\n",
    "#cProfile.run('train_and_evaluate_models(X_train, y_train, use_pca=True)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokładność QDA HOG i Gabor       : 0.5734\n",
    "\n",
    "\n",
    "Dokładność SVC HOG i Gabor       : 0.987\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dokładność QDA HOG i Gabor po PCA: 0.9701\n",
    "\n",
    "\n",
    "Dokładność SVC HOG i Gabor po PCA: 0.9869\n",
    "\n",
    "Czas obliczeń dla PCA 3min vs 30min bez PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTUNA dla HOG, Gabor, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", module=\"optuna\")\n",
    "\n",
    "\n",
    "# def objective(trial, X_train, y_train):\n",
    "#     \"\"\"\n",
    "#     Funkcja celu dla optymalizacji Optuna. \n",
    "#     Przyjmuje próbę (trial), dane treningowe (X_train) i etykiety (y_train).\n",
    "#     Zwraca dokładność QDA i SVM na danych walidacyjnych po zastosowaniu sekwencji transformacji.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Parametry HOG\n",
    "#     orientations      = trial.suggest_int(\"hog_orientations\", 2, 16)\n",
    "#     pixels_per_cell   = trial.suggest_categorical(\"hog_pixels_per_cell\", [str((2, 2)), str((2, 3)), str((3, 2)), str((3, 3)), str((4, 4)), str((5, 5)),str((6, 6)), str((7, 7))])\n",
    "#     cells_per_block   = trial.suggest_categorical(\"hog_cells_per_block\", [str((1, 1)), str((2, 2)), str((3, 3)), str((4, 4))])\n",
    "\n",
    "#     # Parametry filtrów Gabora\n",
    "#     gabor_thetas      = trial.suggest_categorical(\"gabor_thetas\", ([0],  [np.pi / 14],  [np.pi / 12], [0, np.pi / 14 ], [0, np.pi / 12 ] ))\n",
    "#     gabor_sigmas      = trial.suggest_categorical(\"gabor_sigmas\", ([0.33], [0.66], [1], [0.33 , 1.25], [0.66, 2.35], [1, 3]))\n",
    "#     gabor_frequencies = trial.suggest_categorical(\"gabor_frequencies\", ([0.025], [0.05], [0.125], [0.025, 0.125], [0.05, 0.175], [0.05, 0.25]))\n",
    "        \n",
    "#     #PCA\n",
    "#     PCA_n_components  =  trial.suggest_int(\"PCA_Components\", 4, 228)\n",
    "#     kernel = trial.suggest_categorical(\"kernel_pca_kernel\", ['linear', 'poly', 'rbf', 'sigmoid', 'cosine'])\n",
    "#     gamma = None\n",
    "#     if kernel in ['poly', 'rbf']:\n",
    "#         gamma = trial.suggest_loguniform(\"kernel_pca_gamma\", 0.001, 1)\n",
    "        \n",
    "#     # Powiększanie i wygładzanie\n",
    "#     # X_train_split = np.array([enlarge_and_smooth(img, new_size) for img in X_train])\n",
    "#     # X_val = np.array([enlarge_and_smooth(img, new_size) for img in X_val])\n",
    "    \n",
    "#     # Ekstrakcja cech HOG i Gabor\n",
    "#     X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=orientations, \n",
    "#         hog_pixels_per_cell=eval(pixels_per_cell), # Konwersja stringa na krotkę\n",
    "#         hog_cells_per_block=eval(cells_per_block),  # Konwersja stringa na krotkę\n",
    "#         gabor_thetas=gabor_thetas, \n",
    "#         gabor_sigmas=gabor_sigmas, \n",
    "#         gabor_frequencies=gabor_frequencies\n",
    "#     )))\n",
    "\n",
    "#     # Podział na zbiór treningowy i walidacyjny\n",
    "#     X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train, test_size=0.2, random_state=69)\n",
    "    \n",
    "    \n",
    "#     # Kernel PCA\n",
    "#     kpca = KernelPCA(n_components=PCA_n_components, kernel=kernel, gamma=gamma)\n",
    "#     X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "#     X_val_reduced = kpca.transform(X_val_filters)\n",
    "                \n",
    "#     # SVC\n",
    "#     svc = SVC(kernel='rbf')\n",
    "#     svc.fit(X_train_reduced, y_train_filters)\n",
    "#     y_pred_svm = svc.predict(X_val_reduced)\n",
    "#     accuracy_svm = accuracy_score(y_val_filters, y_pred_svm)\n",
    "\n",
    "#     return accuracy_svm\n",
    "\n",
    "# # Optymalizacji\n",
    "                            \n",
    "# study = optuna.create_study(directions=[\"maximize\"], sampler=optuna.samplers.CmaEsSampler() ) \n",
    "# study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, n_jobs=1)\n",
    "\n",
    "\n",
    "# print(\"Best parameters:\", study.best_params)\n",
    "# print(\"Best QDA accuracy:\", study.best_trial.values[0]) \n",
    "# print(\"Best SVM accuracy:\", study.best_trial.values[1]) \n",
    "\n",
    "# # [I 2024-09-17 05:43:37,964] Trial 13 finished with value: 0.99133 and parameters: {'hog_orientations': 16, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(3, 3)', 'gabor_thetas': [0.2243994752564138], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.025], 'PCA_Components': 209, 'kernel_pca_kernel': 'cosine'}. Best is trial 13 with value: 0.9913333333333333.\n",
    "# # [I 2024-09-17 08:45:58,345] Trial 19 finished with value: 0.99175 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(2, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.025], 'PCA_Components': 132, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 19 with value: 0.99175.\n",
    "# # [I 2024-09-17 10:45:29,605] Trial 23 finished with value: 0.99225 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(3, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 150, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:24:59,082] Trial 28 finished with value: 0.99108 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(1, 1)', 'gabor_thetas': [0, 0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 190, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:54:44,248] Trial 29 finished with value: 0.9915 and parameters: {'hog_orientations': 7, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05], 'PCA_Components': 164, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 18:02:03,444] Trial 37 finished with value: 0.99066 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05, 0.175], 'PCA_Components': 169, 'kernel_pca_kernel': 'rbf', 'kernel_pca_gamma': 0.0032186977490984547}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 19:02:58,946] Trial 39 finished with value: 0.9915 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0, 0.2243994752564138], 'gabor_sigmas': [1, 3], 'gabor_frequencies': [0.125], 'PCA_Components': 148, 'kernel_pca_kernel': 'linear'}. Best is trial 23 with value: 0.99225.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykorzystanie wcześniej otrzymanch hiperparamatrów dla HOG i filtrów Gabora po KPCA w celu stworzenia wektora cech z obrazów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=6, \n",
    "#         hog_pixels_per_cell=eval('(3, 3)'), \n",
    "#         hog_cells_per_block=eval('(2, 2)'),  \n",
    "#         gabor_thetas=[0], \n",
    "#         gabor_sigmas=[1], \n",
    "#         gabor_frequencies=[0.05, 0.25]\n",
    "#     )))\n",
    "\n",
    "\n",
    "# kpca = KernelPCA(n_components=150, kernel='cosine')\n",
    "\n",
    "# X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train_ENC, test_size=0.2, random_state=69)\n",
    "\n",
    "# X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "# X_val_reduced = kpca.transform(X_val_filters)\n",
    "\n",
    "\n",
    "# np.save('X_train_reduced.npy', X_train_reduced)\n",
    "# np.save('X_val_reduced.npy', X_val_reduced)\n",
    "# np.save('y_train_filters.npy', y_train_filters)\n",
    "# np.save('y_val_filters.npy', y_val_filters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pierwszy model na sieci konwolucyjnej dla bazy z HOG i Gabora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LayerNormalization , BatchNormalization, PReLU , Input, ActivityRegularization, GlobalAveragePooling1D, SeparableConv1D \n",
    "# from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "# from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Lion\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import gc\n",
    "\n",
    "\n",
    "\n",
    "# X_train_reduced = np.load('X_train_reduced.npy')\n",
    "# X_val_reduced = np.load('X_val_reduced.npy')\n",
    "# y_train_filters = np.load('y_train_filters.npy')\n",
    "# y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "# X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "# X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "# def calculate_max_pool_size(input_length, prev_pool_size, min_output_size=2):\n",
    "#     calculated_pool_size = input_length // prev_pool_size\n",
    "#     return max(min_output_size, calculated_pool_size)\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     \"\"\"Funkcja celu dla Optuny.\"\"\"\n",
    "#     #os.environ['TF_NUM_INTRAOP_THREADS'] = '1' ## Czy Kers działa na pojedycznym wątku ????\n",
    "#     gc.collect()\n",
    "#     tf.keras.backend.clear_session()\n",
    "#     gc.collect()\n",
    "\n",
    "#     activation_functions = ['relu', 'elu', 'leaky_relu', 'tanh']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "\n",
    "#     # Definiowanie modelu\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(X_train_reduced.shape[1], 1)))\n",
    "\n",
    "#     # Pierwsza warstwa konwolucyjna\n",
    "#     model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_1', 48, 450),\n",
    "#                      kernel_size=trial.suggest_int('kernel_size_1', 2, 16),\n",
    "#                      #activation=activation,\n",
    "#                      padding='same', \n",
    "#                      #dilation_rate=trial.suggest_int('dilation_rate_1', 1, 4)\n",
    "#                      )\n",
    "#               )\n",
    "#     model.add(PReLU())                #$$$$$$$ LAYER przed PRelu\n",
    "#     model.add(LayerNormalization())   ######\n",
    "#     model.add(ActivityRegularization(l1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True), \n",
    "#                                      l2=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "#                                      )\n",
    "#               )\n",
    "#     pool_size_1 = trial.suggest_int('pool_size_1', 2, 10)\n",
    "#     model.add(MaxPooling1D(pool_size=pool_size_1))\n",
    "\n",
    "\n",
    "\n",
    "#     # Druga warstwa konwolucyjna\n",
    "#     input_length_2 = X_train_reduced.shape[1] // pool_size_1\n",
    "#     model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_2', 24, 400),\n",
    "#                      kernel_size=trial.suggest_int('kernel_size_2', 2, 14),\n",
    "#                      padding='same',\n",
    "#                      #activation=activation,\n",
    "#                      #dilation_rate=trial.suggest_int('dilation_rate_2', 1, 4)\n",
    "#                      )\n",
    "#               )\n",
    "#     model.add(PReLU())\n",
    "#     model.add(LayerNormalization())\n",
    "#     model.add(ActivityRegularization(l1=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True), \n",
    "#                                      l2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "#                                      )\n",
    "#               )\n",
    "#     pool_size_2 = trial.suggest_int('pool_size_2', 2, input_length_2 // 2)\n",
    "#     #pool_size_2 = trial.suggest_int('pool_size_2', 2, 10)\n",
    "#     model.add(MaxPooling1D(pool_size=pool_size_2))\n",
    "\n",
    "\n",
    "\n",
    "#     # Trzecia warstwa konwolucyjna\n",
    "#     #input_length_3 = input_length_2 // pool_size_2\n",
    "#     model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_3', 32, 300),\n",
    "#                      kernel_size=trial.suggest_int('kernel_size_3', 2, 12),\n",
    "#                      padding='same',\n",
    "#                      )\n",
    "#               )\n",
    "#     model.add(PReLU())\n",
    "#     model.add(LayerNormalization())\n",
    "#     model.add(Dropout(rate=trial.suggest_float('dropout_rate_3', 0.01, 0.5)\n",
    "#                       )\n",
    "#               )\n",
    "#     #pool_size_3 = trial.suggest_int('pool_size_3', 2, max(2, input_length_3 // 2))\n",
    "#     #pool_size_3 = trial.suggest_int('pool_size_3', 2, 8)\n",
    "#     #model.add(MaxPooling1D(pool_size=pool_size_3))\n",
    "\n",
    "#     # Warstwy Dense\n",
    "#     model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "#     model.add(Dense(units=trial.suggest_int('dense_units_1', 40, 300), activation=activation))\n",
    "#     model.add(Dropout(rate=trial.suggest_float('dropout_rate_4', 0.01, 0.75)))\n",
    "    \n",
    "#     model.add(Dense(units=trial.suggest_int('dense_units_2', 20, 200), activation=activation))\n",
    "#     model.add(Dropout(rate=trial.suggest_float('dropout_rate_5', 0.01, 0.65)))\n",
    "    \n",
    "#     model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#     # Optymalizator\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 1e-5, 1e-1, log=True)\n",
    "#     decay_steps = trial.suggest_int(\"decay_steps\", 10, 150)\n",
    "#     decay_rate = trial.suggest_float(\"decay_rate\", 0.01, 0.99)\n",
    "#     lr_schedule = ExponentialDecay(initial_learning_rate=lr_initial,\n",
    "#                                   decay_steps=decay_steps,\n",
    "#                                   decay_rate=decay_rate)\n",
    "\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['Lion'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_schedule)\n",
    "\n",
    "    \n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='val_accuracy', patience=16, start_from_epoch=10, min_delta = 0.00005)\n",
    "\n",
    "#     # Trenowanie modelu\n",
    "#     model.fit(X_train_reduced, y_train_filters,\n",
    "#               epochs=trial.suggest_int('epochs', 40, 150),\n",
    "#               batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "#               validation_data=(X_val_reduced, y_val_filters),\n",
    "#               verbose=1,\n",
    "#               callbacks=[early_stopping])\n",
    "\n",
    "#     # Ewaluacja modelu\n",
    "#     _, accuracy = model.evaluate(X_val_reduced, y_val_filters, verbose=0)\n",
    "\n",
    "#     del model\n",
    "#     return accuracy\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "# #[I 2024-10-04 05:18:28,377] Trial 26 finished with value: 0.9810000061988831 and parameters: {'activation': 'elu', 'conv1d_filters_1': 347, 'kernel_size_1': 7,  'l1_1': 0.0002141584459916536, 'l2_1': 4.201061472934882e-06, 'pool_size_1': 6, 'conv1d_filters_2': 77, 'kernel_size_2': 14, 'l1_2': 3.203529032832527e-06, 'l2_2': 0.00015793896479891322, 'pool_size_2': 2, 'conv1d_filters_3': 258, 'kernel_size_3': 4, 'dropout_rate_3': 0.3297488074504043, 'dense_units_1': 271, 'dropout_rate_4': 0.5097723961373417, 'dense_units_2': 78, 'dropout_rate_5': 0.15473996425601394, 'lr_initial': 0.0004175922071628326, 'decay_steps': 33, 'decay_rate': 0.9780884395210858, 'optimizer': 'Lion', 'epochs': 123, 'batch_size': 264}. Best is trial 26 with value: 0.9810000061988831.\n",
    "# #[I 2024-10-04 14:58:14,236] Trial 61 finished with value: 0.9797499775886536 and parameters: {'activation': 'elu', 'conv1d_filters_1': 424, 'kernel_size_1': 13, 'l1_1': 0.0005501543495144062, 'l2_1': 0.02794377888054429,   'pool_size_1': 7, 'conv1d_filters_2': 79, 'kernel_size_2': 12, 'l1_2': 1.3699264627909325e-06, 'l2_2': 6.577814668979489e-06, 'pool_size_2': 3, 'conv1d_filters_3': 280, 'kernel_size_3': 3, 'dropout_rate_3': 0.21341586312675653, 'dense_units_1': 292, 'dropout_rate_4': 0.22415600475665626, 'dense_units_2': 106, 'dropout_rate_5': 0.09686035013603865, 'lr_initial': 0.0004430745906851504, 'decay_steps': 22, 'decay_rate': 0.9872365423551039, 'optimizer': 'Lion', 'epochs': 150, 'batch_size': 216}. Best is trial 26 with value: 0.9810000061988831.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drugi model na GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, LSTM , GRU , LayerNormalization, PReLU, ActivityRegularization, Dense, GlobalAveragePooling1D, Dropout\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# import optuna\n",
    "\n",
    "\n",
    "# X_train_reduced = np.load('X_train_reduced.npy')\n",
    "# X_val_reduced = np.load('X_val_reduced.npy')\n",
    "# y_train_filters = np.load('y_train_filters.npy')\n",
    "# y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "# X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "# X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "# def objective(trial):\n",
    "    \n",
    "#     Inp = Input(shape=(X_train_reduced.shape[1], 1))\n",
    "\n",
    "#     x = PReLU()(Inp)\n",
    "#     x = GRU(\n",
    "#         units=trial.suggest_int('units_1', 24, 600),\n",
    "#         activation='silu',\n",
    "#         return_sequences=True  # Dodano return_sequences=True\n",
    "#     )(x)\n",
    "#     x = LayerNormalization()(x)\n",
    "#     x = ActivityRegularization(\n",
    "#         l1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True),\n",
    "#         l2=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "#     )(x)\n",
    "\n",
    "#     x = PReLU()(x)\n",
    "#     x = GRU(\n",
    "#         units=trial.suggest_int('units_2', 24, 600),\n",
    "#         activation='silu'\n",
    "#     )(x)\n",
    "#     x = LayerNormalization()(x)\n",
    "#     x = ActivityRegularization(\n",
    "#         l1=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True),\n",
    "#         l2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "#     )(x)\n",
    "\n",
    "#     x = Dense(units=trial.suggest_int('dense_units', 24, 600), activation='silu')(x)\n",
    "#     x = Dropout(rate=trial.suggest_float('dropout_rate_last', 0.01, 0.75))(x)\n",
    "#     outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "#     model = Model(inputs=Inp, outputs=outputs)\n",
    "#     tf.keras.utils.plot_model(model, to_file='model.png')\n",
    "\n",
    "\n",
    "#     # ReduceLROnPlateau\n",
    "#     reduce_lr = ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.9, step=0.1),\n",
    "#         patience=trial.suggest_int(\"reduce_lr_patience\", 2, 12),\n",
    "#         min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "#     )\n",
    "\n",
    "#     # Optymalizator\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "#     # Początkową wartość learning rate\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     #model.summary()\n",
    "\n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='val_accuracy', patience=12, start_from_epoch=8, min_delta = 0.0001)\n",
    "    \n",
    "#     # Trenowanie modelu\n",
    "#     model.fit(X_train_reduced, y_train_filters,\n",
    "#               epochs=trial.suggest_int('epochs', 25, 80),\n",
    "#               batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "#               validation_data=(X_val_reduced, y_val_filters),\n",
    "#               verbose=0,\n",
    "#               callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "#     # Ewaluacja modelu\n",
    "#     _, accuracy = model.evaluate(X_val_reduced, y_val_filters, verbose=0)\n",
    "    \n",
    "    \n",
    "#     del model\n",
    "#     return accuracy\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model bazujący na dwóch potokach danych surowych przechodzących przez Conv2d, oraz LSTM dla danych z HOG i Gabor. Wykorzystanie mechanizmu atencji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GlobalAveragePooling1D, Permute, Flatten, Reshape, Conv2D, MaxPooling2D, Concatenate, Attention, Dense, LayerNormalization, ActivityRegularization, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import gc\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "X_train_reduced = np.load('X_train_reduced.npy')\n",
    "X_val_reduced = np.load('X_val_reduced.npy')\n",
    "y_train_filters = np.load('y_train_filters.npy')\n",
    "y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "X_train_image = X_train_image.astype('float32')\n",
    "X_val_image = X_val_image.astype('float32')\n",
    "X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "\n",
    "# # Sprawdzenie poprawności danych wejściowych\n",
    "# print(\"Kształt X_train_image:\", X_train_image.shape)\n",
    "# print(\"Kształt X_val_image:\", X_val_image.shape)\n",
    "# print(\"Kształt X_train_reduced:\", X_train_reduced.shape)\n",
    "# print(\"Kształt X_val_reduced:\", X_val_reduced.shape)\n",
    "\n",
    "# print(\"Kształt y_train_filters:\", y_train_filters.shape)\n",
    "# print(\"Kształt y_val_filters:\", y_val_filters.shape)\n",
    "# print(\"Kształt y_train_image:\", y_train_image.shape)\n",
    "# print(\"Kształt y_val_image:\", y_val_image.shape)\n",
    "\n",
    "# # Sprawdzenie typów danych\n",
    "# print(\"Typ danych X_train_image:\", X_train_image.dtype)\n",
    "# print(\"Typ danych X_val_image:\", X_val_image.dtype)\n",
    "# print(\"Typ danych X_train_reduced:\", X_train_reduced.dtype)\n",
    "# print(\"Typ danych X_val_reduced:\", X_val_reduced.dtype)\n",
    "# print(\"Typ danych y_train_filters:\", y_train_filters.dtype)\n",
    "# print(\"Typ danych y_val_filters:\", y_val_filters.dtype)\n",
    "# print(\"Typ danych y_train_image:\", y_train_image.dtype)\n",
    "# print(\"Typ danych y_val_image:\", y_val_image.dtype)\n",
    "\n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    # Dane wejściowe\n",
    "    image_input = Input(shape=X_train_image.shape[1:])\n",
    "    flattened_input = Input(shape=X_train_reduced.shape[1:])\n",
    "\n",
    "    ### Hiperparamatry dla LSTM\n",
    "    units1=trial.suggest_int('units_1', 12, 128)\n",
    "    units2=trial.suggest_int('units_2', 12, 128)\n",
    "    l1_1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True)\n",
    "    l2_1=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "    l1_2=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True)\n",
    "    l2_2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "    ### Hiperparamatry dla Conv2d\n",
    "    filters1=trial.suggest_int('conv2d_filters_1', 12, 128)\n",
    "    filters2=trial.suggest_int('conv2d_filters_2', 12, 128)\n",
    "    ###################\n",
    "    # Gałąź LSTM\n",
    "        ## Pierwsza warstwa\n",
    "    lstm1 = LSTM(units1,\n",
    "                return_sequences=True,\n",
    "                activation = 'silu'\n",
    "                )(flattened_input)\n",
    "    \n",
    "    lstm1 = LayerNormalization()(lstm1)\n",
    "    lstm1 = ActivityRegularization(\n",
    "                                    l1 = l1_1,\n",
    "                                    l2 = l2_1\n",
    "                                   )(lstm1)\n",
    "        ## Druga warstwa\n",
    "    lstm2 = LSTM(units2,\n",
    "                return_sequences=True,\n",
    "                activation = 'silu'\n",
    "                )(lstm1)\n",
    "    lstm2 = LayerNormalization()(lstm2)\n",
    "    lstm2 = ActivityRegularization(\n",
    "                                    l1 = l1_2,\n",
    "                                    l2 = l2_2 \n",
    "                                  )(lstm2)\n",
    "    \n",
    "    pool_lstm = GlobalAveragePooling1D()(lstm2) \n",
    "    flat_lstm = Flatten()(pool_lstm)\n",
    "    print(\"flat_lstm shape:\", flat_lstm)\n",
    "\n",
    "    # Gałąź Conv2D\n",
    "        ## Pierwsza warstwa\n",
    "    conv1 = Conv2D(filters1,\n",
    "                   kernel_size=(3, 3),\n",
    "                   activation='silu'\n",
    "                   )(image_input)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        ## Druga warstwa\n",
    "    conv2 = Conv2D(filters2,\n",
    "                   kernel_size=(3, 3),\n",
    "                   activation='silu'\n",
    "                   )(pool1)\n",
    "    pool2 = GlobalAveragePooling2D()(conv2)  \n",
    "    \n",
    "    flat_cnn = Flatten()(pool2)\n",
    "    print(\"flat_cnn shape:\", flat_cnn)\n",
    "    \n",
    "    # Połączenie gałęzi\n",
    "    merged = Concatenate()([flat_lstm, flat_cnn])\n",
    "    print(\"merged shape:\", merged)\n",
    "    merged = Reshape((merged.shape[1], 1))(merged)\n",
    "    print(\"Reshape shape:\", merged)\n",
    "    \n",
    "    # Attention Layer\n",
    "    attention_output = Attention()([merged, merged])\n",
    "    attention_output = Reshape((attention_output.shape[1],))(attention_output)\n",
    "    \n",
    "    # Dense\n",
    "    dense = Dense(units=trial.suggest_int('dense_units_1', 10, 120), activation='silu')(attention_output)\n",
    "    dense = Dropout(rate=trial.suggest_float('dropout_rate_4', 0.01, 0.75))(dense)\n",
    "\n",
    "    #Output\n",
    "    output = Dense(10, activation='softmax')(dense)\n",
    "    model = Model(inputs=[image_input, flattened_input], outputs=output)\n",
    "    \n",
    "    #tf.keras.utils.plot_model(model, to_file='model.png')\n",
    "    #model.summary()\n",
    "    \n",
    "    # ReduceLROnPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.9, step=0.1),\n",
    "        patience=trial.suggest_int(\"reduce_lr_patience\", 2, 12),\n",
    "        min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "    )\n",
    "\n",
    "    # Optymalizator\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Lion'])\n",
    "    optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "    # Początkową wartość learning rate\n",
    "    lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "    optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=8, start_from_epoch=8, min_delta=0.0001)\n",
    "\n",
    "    #tensorboard = TensorBoard(log_dir='./logs')\n",
    "    \n",
    "    # Trenowanie modelu\n",
    "    model.fit(\n",
    "        [X_train_image, X_train_reduced], \n",
    "        y_train_filters,\n",
    "        epochs=trial.suggest_int('epochs', 25, 80),\n",
    "        batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "        validation_data=([X_val_image, X_val_reduced], y_val_filters),\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping, reduce_lr] # , tensorboard\n",
    "    )\n",
    "\n",
    "    # Ewaluacja modelu\n",
    "    _, accuracy = model.evaluate([X_val_image, X_val_reduced], y_val_filters, verbose=0)\n",
    "    \n",
    "    del model\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n",
    "print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
