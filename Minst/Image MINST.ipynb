{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "import optuna as optuna\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import cv2 as cv2\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog, graycomatrix, graycoprops\n",
    "from skimage.filters import gabor_kernel, gabor\n",
    "from skimage import img_as_float32\n",
    "from skimage.transform import resize\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "\n",
    "try:\n",
    "    optuna_version = importlib.metadata.version('optuna')\n",
    "    print(f\"Optuna version: {optuna_version}\")\n",
    "except importlib.metadata.PackageNotFoundError:\n",
    "    print(\"Optuna is not installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 785 entries, label to 28x28\n",
      "dtypes: int64(785)\n",
      "memory usage: 359.3 MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('mnist_train.csv')\n",
    "test  = pd.read_csv('mnist_test.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_values: (60000, 28, 28)\n",
      "Shape of train_target_encoded: (60000, 10)\n",
      "Shape of test_values: (10000, 28, 28)\n",
      "Shape of test_target_encoded: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "\n",
    "train_target = train['label']\n",
    "test_target = test['label']\n",
    "\n",
    "train_values = train.drop('label', axis=1)\n",
    "test_values = test.drop('label', axis=1)\n",
    "\n",
    "train_values = train_values.values.reshape(-1, 28, 28) \n",
    "test_values = test_values.values.reshape(-1, 28, 28)\n",
    "\n",
    "train_target_encoded = encoder.fit_transform(train_target.values.reshape(-1, 1)).toarray()\n",
    "test_target_encoded = encoder.transform(test_target.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(\"Shape of train_values:\", train_values.shape)\n",
    "print(\"Shape of train_target_encoded:\", train_target_encoded.shape)\n",
    "\n",
    "print(\"Shape of test_values:\", test_values.shape)\n",
    "print(\"Shape of test_target_encoded:\", test_target_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_values\n",
    "X_train = train_values.astype(np.uint8)\n",
    "\n",
    "y_train_ENC = train_target_encoded\n",
    "y_train = np.argmax(y_train_ENC, axis=1)\n",
    "\n",
    "X_test = test_values\n",
    "X_test = test_values.astype(np.uint8)\n",
    "\n",
    "y_test_ENC = test_target_encoded\n",
    "y_test = np.argmax(y_test_ENC, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacja histogramem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sprawdzenie i konwersja typu danych\n",
    "# if X_train.dtype != np.uint8:\n",
    "#     X_train = X_train.astype(np.float32)\n",
    "# if X_test.dtype != np.uint8:\n",
    "#     X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Sprawdzenie i ewentualna korekta kształtu danych\n",
    "if X_train.shape[1:] != (28, 28):\n",
    "    X_train = X_train.reshape(-1, 28, 28)\n",
    "if X_test.shape[1:] != (28, 28):\n",
    "    X_test = X_test.reshape(-1, 28, 28)\n",
    "\n",
    "# Wyrównywanie histogramu\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = cv2.equalizeHist(X_train[i])\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = cv2.equalizeHist(X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powiększanie i wygładzenie obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlarge_and_smooth(image, new_size):\n",
    "    \"\"\"Powiększa obraz za pomocą interpolacji bicubic.\"\"\"\n",
    "    enlarged_image = cv2.resize(image, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "    return enlarged_image\n",
    "\n",
    "# # Sztuczne zwiększenie rozmiaru obrazu \n",
    "# sample_indices = np.random.choice(X_train.shape[0], 5, replace=False)\n",
    "\n",
    "# # Dla każdego wybranego obrazu\n",
    "# for idx in sample_indices:\n",
    "#     # Oryginalny obraz\n",
    "#     original_image = X_train[idx]\n",
    "\n",
    "#     # Powiększony i wygładzony obraz\n",
    "#     enlarged_smoothed_image = enlarge_and_smooth(original_image, new_size=(42, 42))  # Przykładowy nowy rozmiar\n",
    "\n",
    "#     # Wyświetl oryginalny i przekształcony obraz obok siebie\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     sns.heatmap(original_image, cmap=\"viridis\", ax=axes[0])\n",
    "#     axes[0].set_title(f'Oryginalny obraz ({idx})')\n",
    "\n",
    "#     sns.heatmap(enlarged_smoothed_image, cmap=\"viridis\", ax=axes[1])\n",
    "#     axes[1].set_title(f'Powiększony i wygładzony obraz (Lanczos4) ({idx})')\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie modelu wykorzystując SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Baseline accuracy without transformations\n",
    "# X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(X_train, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "# qda_base = QDA()\n",
    "# qda_base.fit(X_train_base.reshape(X_train_base.shape[0], -1), y_train_base)  # Flatten images for QDA\n",
    "# y_pred_qda_base = qda_base.predict(X_val_base.reshape(X_val_base.shape[0], -1))\n",
    "# accuracy_qda_base = accuracy_score(y_val_base, y_pred_qda_base)\n",
    "\n",
    "# svm_base = SVC()\n",
    "# svm_base.fit(X_train_base.reshape(X_train_base.shape[0], -1), y_train_base)  # Flatten images for SVM\n",
    "# y_pred_svm_base = svm_base.predict(X_val_base.reshape(X_val_base.shape[0], -1))\n",
    "# accuracy_svm_base = accuracy_score(y_val_base, y_pred_svm_base)\n",
    "\n",
    "# print(\"Baseline QDA accuracy:\", accuracy_qda_base)\n",
    "# print(\"Baseline SVM accuracy:\", accuracy_svm_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline QDA accuracy: 0.5525\n",
    "\n",
    "\n",
    "Baseline SVM accuracy: 0.9793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deklaracja funkcji dla ekstrakcji cech z obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_hog_parallel(images, orientations=None, pixels_per_cell=None, cells_per_block=None, n_jobs=-1):\n",
    "    \"\"\"Oblicza deskryptory HOG dla obrazu.\"\"\"\n",
    "    \n",
    "     # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if orientations is None:\n",
    "        orientations = 9\n",
    "    if pixels_per_cell is None:\n",
    "        pixels_per_cell = (8, 8)\n",
    "    if cells_per_block is None:\n",
    "        cells_per_block = (2, 2)\n",
    "    \n",
    "    def calculate_hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block):\n",
    "        \n",
    "        image = img_as_float32(image)\n",
    "\n",
    "        fd = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                cells_per_block=cells_per_block, visualize=False, channel_axis=None)\n",
    "\n",
    "        return fd\n",
    "    \n",
    "    return Parallel(n_jobs=n_jobs)(delayed(calculate_hog)(image, orientations, pixels_per_cell, cells_per_block) for image in images)\n",
    "\n",
    "\n",
    "def apply_gabor_filters_parallel(images, thetas=None, sigmas=None, frequencies=None, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Stosuje filtry Gabora do wielu obrazów równolegle.\n",
    "\n",
    "    Argumenty:\n",
    "        images: Lista obrazów wejściowych.\n",
    "        thetas: (Opcjonalnie) Lista kątów orientacji filtrów Gabora (w radianach). Jeśli None, używane są wartości domyślne.\n",
    "        sigmas: (Opcjonalnie) Lista odchyleń standardowych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        frequencies: (Opcjonalnie) Lista częstotliwości przestrzennych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        n_jobs: Liczba rdzeni procesora do wykorzystania podczas zrównoleglania. -1 oznacza użycie wszystkich dostępnych rdzeni.\n",
    "\n",
    "    Zwraca:\n",
    "        Listę tablic NumPy zawierających obrazy przefiltrowane filtrami Gabora, o typie danych float32.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if thetas is None:\n",
    "        thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    if sigmas is None:\n",
    "        sigmas = [1, 3]\n",
    "    if frequencies is None:\n",
    "        frequencies = [0.05, 0.25]\n",
    "\n",
    "    def apply_gabor_filters_single(image):\n",
    "        image = img_as_float32(image)\n",
    "        filtered_images = [gabor(image, frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)[0]\n",
    "                           for theta in thetas for sigma in sigmas for frequency in frequencies]\n",
    "        return np.array(filtered_images, dtype=np.float32)\n",
    "\n",
    "    return Parallel(n_jobs=n_jobs)(delayed(apply_gabor_filters_single)(image) for image in images)\n",
    "\n",
    "\n",
    "def extract_features_HOG_Gabor(X, hog_orientations=None, hog_pixels_per_cell=None, hog_cells_per_block=None, \n",
    "                              gabor_thetas=None, gabor_sigmas=None, gabor_frequencies=None):\n",
    "    \"\"\"Generator zwracający cechy HOG i Gabora dla obrazów.\"\"\"\n",
    "\n",
    "    hog_features_list = calculate_hog_parallel(X, orientations=hog_orientations, pixels_per_cell=hog_pixels_per_cell, cells_per_block=hog_cells_per_block)\n",
    "    gabor_features_list = apply_gabor_filters_parallel(X, thetas=gabor_thetas, sigmas=gabor_sigmas, frequencies=gabor_frequencies)\n",
    "\n",
    "    for hog_features, gabor_features in zip(hog_features_list, gabor_features_list):\n",
    "        \n",
    "        gabor_features_flattened = np.array([img.flatten() for img in gabor_features])\n",
    "\n",
    "        # Dopasowujemy kształt hog_features do gabor_features_flattened\n",
    "        hog_features_reshaped = np.repeat(hog_features.reshape(1, -1), gabor_features_flattened.shape[0], axis=0)\n",
    "\n",
    "        # Łączymy cechy HOG i Gabora w poziomie\n",
    "        concatenated_features = np.hstack((hog_features_reshaped, gabor_features_flattened))\n",
    "\n",
    "        concatenated_features = concatenated_features.flatten()\n",
    "\n",
    "        yield concatenated_features\n",
    "\n",
    "#################################################################################################################################################\n",
    "################################################  Directional filters, ORB    ###################################################################\n",
    "\n",
    "def apply_directional_filters(image, kernel_sizes=[3, 5, 7]):\n",
    "    \"\"\"Stosuje filtry kierunkowe do obrazu.\"\"\"\n",
    "    filtered_images = []\n",
    "    for kernel_size in kernel_sizes:\n",
    "        kernel_horizontal = cv2.getDerivKernels(1, 0, kernel_size, normalize=True)\n",
    "        kernel_vertical = cv2.getDerivKernels(0, 1, kernel_size, normalize=True)\n",
    "        filtered_horizontal = cv2.filter2D(image, cv2.CV_32F, kernel_horizontal[0])\n",
    "        filtered_vertical = cv2.filter2D(image, cv2.CV_32F, kernel_vertical[0])\n",
    "        filtered_images.append(filtered_horizontal.flatten())\n",
    "        filtered_images.append(filtered_vertical.flatten())\n",
    "        if kernel_size % 2 == 0 or kernel_size > 31:\n",
    "            raise ValueError(f\"Nieprawidłowy rozmiar jądra: {kernel_size}. Rozmiar jądra musi być nieparzysty i nie większy niż 31.\")\n",
    "    return np.array(filtered_images).squeeze()\n",
    "\n",
    "def extract_orb_features(image, nfeatures=500):\n",
    "    \"\"\"Wyodrębnia cechy ORB z obrazu.\"\"\"\n",
    "    orb = cv2.ORB_create(nfeatures=nfeatures)  \n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        return np.zeros(nfeatures * 32)  \n",
    "    return descriptors.flatten()\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################################################################################\n",
    "################################################  GLCM, Zernike    ##############################################################################\n",
    "def extract_glcm_features(image, distances=[1], angles=[0], properties=['contrast', 'energy', 'homogeneity', 'correlation']):\n",
    "    \"\"\"Wyodrębnia cechy GLCM z obrazu.\"\"\"\n",
    "    if len(image.shape) > 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    glcm = graycomatrix(image, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    features = []\n",
    "    for prop in properties:\n",
    "        features.extend(graycoprops(glcm, prop).flatten())\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testowanie modelu dla HOG i filtrów Gabora i porównanie wydajności przy zastosowaniu redukcji wymiarów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_evaluate_models(X_train, y_train, use_pca=True):\n",
    "#     \"\"\"\n",
    "#     Trenuje i ocenia modele QDA i SVC, opcjonalnie z redukcją wymiarów PCA.\n",
    "\n",
    "#     Argumenty:\n",
    "#         X_train: Dane treningowe.\n",
    "#         y_train: Etykiety treningowe.\n",
    "#         use_pca: Flaga wskazująca, czy używać PCA (domyślnie True).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Ekstrakcja cech HOG i Gabor za pomocą generatora\n",
    "#     X_train_features = np.array(list(extract_features_HOG_Gabor(X_train)))\n",
    "\n",
    "#     # Podział na zbiór treningowy i walidacyjny\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train_features, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "#     # Redukcja wymiarów (opcjonalnie)\n",
    "#     if use_pca:\n",
    "#         pca_dim = PCA(n_components=50)\n",
    "#         X_train = pca_dim.fit_transform(X_train)\n",
    "#         X_val = pca_dim.transform(X_val)\n",
    "\n",
    "#     # Tworzenie i trenowanie modelu QDA\n",
    "#     qda = QDA()\n",
    "#     qda.fit(X_train, y_train)\n",
    "\n",
    "#     # Tworzenie i trenowanie modelu SVC\n",
    "#     svc = SVC(kernel='rbf')\n",
    "#     svc.fit(X_train, y_train)\n",
    "\n",
    "#     # Ocena modelu\n",
    "#     y_pred_qda = qda.predict(X_val)\n",
    "#     accuracy_qda = accuracy_score(y_val, y_pred_qda)\n",
    "\n",
    "#     y_pred_svc = svc.predict(X_val)\n",
    "#     accuracy_svc = accuracy_score(y_val, y_pred_svc)\n",
    "\n",
    "#     print(\"Dokładność QDA HOG i Gabor\", \"po PCA:\" if use_pca else \":\", accuracy_qda)\n",
    "#     print(\"Dokładność SVC HOG i Gabor\", \"po PCA:\" if use_pca else \":\", accuracy_svc)\n",
    "\n",
    "# # Test z PCA\n",
    "# train_and_evaluate_models(X_train, y_train, use_pca=True)\n",
    "#train_and_evaluate_models(X_train, y_train, use_pca=False)\n",
    "#cProfile.run('train_and_evaluate_models(X_train, y_train, use_pca=True)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokładność QDA HOG i Gabor       : 0.5734\n",
    "\n",
    "\n",
    "Dokładność SVC HOG i Gabor       : 0.987\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dokładność QDA HOG i Gabor po PCA: 0.9701\n",
    "\n",
    "\n",
    "Dokładność SVC HOG i Gabor po PCA: 0.9869\n",
    "\n",
    "Czas obliczeń dla PCA 3min vs 30min bez PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTUNA dla HOG, Gabor, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", module=\"optuna\")\n",
    "\n",
    "\n",
    "# def objective(trial, X_train, y_train):\n",
    "#     \"\"\"\n",
    "#     Funkcja celu dla optymalizacji Optuna. \n",
    "#     Przyjmuje próbę (trial), dane treningowe (X_train) i etykiety (y_train).\n",
    "#     Zwraca dokładność QDA i SVM na danych walidacyjnych po zastosowaniu sekwencji transformacji.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Parametry HOG\n",
    "#     orientations      = trial.suggest_int(\"hog_orientations\", 2, 16)\n",
    "#     pixels_per_cell   = trial.suggest_categorical(\"hog_pixels_per_cell\", [str((2, 2)), str((2, 3)), str((3, 2)), str((3, 3)), str((4, 4)), str((5, 5)),str((6, 6)), str((7, 7))])\n",
    "#     cells_per_block   = trial.suggest_categorical(\"hog_cells_per_block\", [str((1, 1)), str((2, 2)), str((3, 3)), str((4, 4))])\n",
    "\n",
    "#     # Parametry filtrów Gabora\n",
    "#     gabor_thetas      = trial.suggest_categorical(\"gabor_thetas\", ([0],  [np.pi / 14],  [np.pi / 12], [0, np.pi / 14 ], [0, np.pi / 12 ] ))\n",
    "#     gabor_sigmas      = trial.suggest_categorical(\"gabor_sigmas\", ([0.33], [0.66], [1], [0.33 , 1.25], [0.66, 2.35], [1, 3]))\n",
    "#     gabor_frequencies = trial.suggest_categorical(\"gabor_frequencies\", ([0.025], [0.05], [0.125], [0.025, 0.125], [0.05, 0.175], [0.05, 0.25]))\n",
    "        \n",
    "#     #PCA\n",
    "#     PCA_n_components  =  trial.suggest_int(\"PCA_Components\", 4, 228)\n",
    "#     kernel = trial.suggest_categorical(\"kernel_pca_kernel\", ['linear', 'poly', 'rbf', 'sigmoid', 'cosine'])\n",
    "#     gamma = None\n",
    "#     if kernel in ['poly', 'rbf']:\n",
    "#         gamma = trial.suggest_loguniform(\"kernel_pca_gamma\", 0.001, 1)\n",
    "        \n",
    "#     # Powiększanie i wygładzanie\n",
    "#     # X_train_split = np.array([enlarge_and_smooth(img, new_size) for img in X_train])\n",
    "#     # X_val = np.array([enlarge_and_smooth(img, new_size) for img in X_val])\n",
    "    \n",
    "#     # Ekstrakcja cech HOG i Gabor\n",
    "#     X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=orientations, \n",
    "#         hog_pixels_per_cell=eval(pixels_per_cell), # Konwersja stringa na krotkę\n",
    "#         hog_cells_per_block=eval(cells_per_block),  # Konwersja stringa na krotkę\n",
    "#         gabor_thetas=gabor_thetas, \n",
    "#         gabor_sigmas=gabor_sigmas, \n",
    "#         gabor_frequencies=gabor_frequencies\n",
    "#     )))\n",
    "\n",
    "#     # Podział na zbiór treningowy i walidacyjny\n",
    "#     X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train, test_size=0.2, random_state=69)\n",
    "    \n",
    "    \n",
    "#     # Kernel PCA\n",
    "#     kpca = KernelPCA(n_components=PCA_n_components, kernel=kernel, gamma=gamma)\n",
    "#     X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "#     X_val_reduced = kpca.transform(X_val_filters)\n",
    "                \n",
    "#     # SVC\n",
    "#     svc = SVC(kernel='rbf')\n",
    "#     svc.fit(X_train_reduced, y_train_filters)\n",
    "#     y_pred_svm = svc.predict(X_val_reduced)\n",
    "#     accuracy_svm = accuracy_score(y_val_filters, y_pred_svm)\n",
    "\n",
    "#     return accuracy_svm\n",
    "\n",
    "# # Optymalizacji\n",
    "                            \n",
    "# study = optuna.create_study(directions=[\"maximize\"], sampler=optuna.samplers.CmaEsSampler() ) \n",
    "# study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, n_jobs=1)\n",
    "\n",
    "\n",
    "# print(\"Best parameters:\", study.best_params)\n",
    "# print(\"Best QDA accuracy:\", study.best_trial.values[0]) \n",
    "# print(\"Best SVM accuracy:\", study.best_trial.values[1]) \n",
    "\n",
    "# # [I 2024-09-17 05:43:37,964] Trial 13 finished with value: 0.99133 and parameters: {'hog_orientations': 16, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(3, 3)', 'gabor_thetas': [0.2243994752564138], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.025], 'PCA_Components': 209, 'kernel_pca_kernel': 'cosine'}. Best is trial 13 with value: 0.9913333333333333.\n",
    "# # [I 2024-09-17 08:45:58,345] Trial 19 finished with value: 0.99175 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(2, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.025], 'PCA_Components': 132, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 19 with value: 0.99175.\n",
    "# # [I 2024-09-17 10:45:29,605] Trial 23 finished with value: 0.99225 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(3, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 150, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:24:59,082] Trial 28 finished with value: 0.99108 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(1, 1)', 'gabor_thetas': [0, 0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 190, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:54:44,248] Trial 29 finished with value: 0.9915 and parameters: {'hog_orientations': 7, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05], 'PCA_Components': 164, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 18:02:03,444] Trial 37 finished with value: 0.99066 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05, 0.175], 'PCA_Components': 169, 'kernel_pca_kernel': 'rbf', 'kernel_pca_gamma': 0.0032186977490984547}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 19:02:58,946] Trial 39 finished with value: 0.9915 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0, 0.2243994752564138], 'gabor_sigmas': [1, 3], 'gabor_frequencies': [0.125], 'PCA_Components': 148, 'kernel_pca_kernel': 'linear'}. Best is trial 23 with value: 0.99225.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykorzystanie wcześniej otrzymanch hiperparamatrów dla HOG i filtrów Gabora po KPCA w celu stworzenia wektora cech z obrazów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=6, \n",
    "#         hog_pixels_per_cell=eval('(3, 3)'), \n",
    "#         hog_cells_per_block=eval('(2, 2)'),  \n",
    "#         gabor_thetas=[0], \n",
    "#         gabor_sigmas=[1], \n",
    "#         gabor_frequencies=[0.05, 0.25]\n",
    "#     )))\n",
    "\n",
    "\n",
    "# kpca = KernelPCA(n_components=150, kernel='cosine')\n",
    "\n",
    "# X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train_ENC, test_size=0.2, random_state=69)\n",
    "\n",
    "# X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "# X_val_reduced = kpca.transform(X_val_filters)\n",
    "\n",
    "\n",
    "# np.save('X_train_reduced.npy', X_train_reduced)\n",
    "# np.save('X_val_reduced.npy', X_val_reduced)\n",
    "# np.save('y_train_filters.npy', y_train_filters)\n",
    "# np.save('y_val_filters.npy', y_val_filters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pierwszy model na sieci konwolucyjnej dla bazy z HOG i Gabora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LayerNormalization , BatchNormalization, PReLU , Input, ActivityRegularization, GlobalAveragePooling1D, SeparableConv1D \n",
    "# from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "# from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Lion\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import gc\n",
    "\n",
    "\n",
    "\n",
    "# X_train_reduced = np.load('X_train_reduced.npy')\n",
    "# X_val_reduced = np.load('X_val_reduced.npy')\n",
    "# y_train_filters = np.load('y_train_filters.npy')\n",
    "# y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "# X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "# X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "# def calculate_max_pool_size(input_length, prev_pool_size, min_output_size=2):\n",
    "#     calculated_pool_size = input_length // prev_pool_size\n",
    "#     return max(min_output_size, calculated_pool_size)\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     \"\"\"Funkcja celu dla Optuny.\"\"\"\n",
    "#     #os.environ['TF_NUM_INTRAOP_THREADS'] = '1' ## Czy Kers działa na pojedycznym wątku ????\n",
    "#     gc.collect()\n",
    "#     tf.keras.backend.clear_session()\n",
    "#     gc.collect()\n",
    "\n",
    "#     activation_functions = ['relu', 'elu', 'leaky_relu', 'tanh']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "\n",
    "#     # Definiowanie modelu\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(X_train_reduced.shape[1], 1)))\n",
    "\n",
    "#     # Pierwsza warstwa konwolucyjna\n",
    "#     model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_1', 48, 450),\n",
    "#                      kernel_size=trial.suggest_int('kernel_size_1', 2, 16),\n",
    "#                      #activation=activation,\n",
    "#                      padding='same', \n",
    "#                      #dilation_rate=trial.suggest_int('dilation_rate_1', 1, 4)\n",
    "#                      )\n",
    "#               )\n",
    "#     model.add(PReLU())                #$$$$$$$ LAYER przed PRelu\n",
    "#     model.add(LayerNormalization())   ######\n",
    "#     model.add(ActivityRegularization(l1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True), \n",
    "#                                      l2=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "#                                      )\n",
    "#               )\n",
    "#     pool_size_1 = trial.suggest_int('pool_size_1', 2, 10)\n",
    "#     model.add(MaxPooling1D(pool_size=pool_size_1))\n",
    "\n",
    "\n",
    "\n",
    "#     # Druga warstwa konwolucyjna\n",
    "#     input_length_2 = X_train_reduced.shape[1] // pool_size_1\n",
    "#     model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_2', 24, 400),\n",
    "#                      kernel_size=trial.suggest_int('kernel_size_2', 2, 14),\n",
    "#                      padding='same',\n",
    "#                      #activation=activation,\n",
    "#                      #dilation_rate=trial.suggest_int('dilation_rate_2', 1, 4)\n",
    "#                      )\n",
    "#               )\n",
    "#     model.add(PReLU())\n",
    "#     model.add(LayerNormalization())\n",
    "#     model.add(ActivityRegularization(l1=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True), \n",
    "#                                      l2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "#                                      )\n",
    "#               )\n",
    "#     pool_size_2 = trial.suggest_int('pool_size_2', 2, input_length_2 // 2)\n",
    "#     #pool_size_2 = trial.suggest_int('pool_size_2', 2, 10)\n",
    "#     model.add(MaxPooling1D(pool_size=pool_size_2))\n",
    "\n",
    "\n",
    "\n",
    "#     # Trzecia warstwa konwolucyjna\n",
    "#     #input_length_3 = input_length_2 // pool_size_2\n",
    "#     model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_3', 32, 300),\n",
    "#                      kernel_size=trial.suggest_int('kernel_size_3', 2, 12),\n",
    "#                      padding='same',\n",
    "#                      )\n",
    "#               )\n",
    "#     model.add(PReLU())\n",
    "#     model.add(LayerNormalization())\n",
    "#     model.add(Dropout(rate=trial.suggest_float('dropout_rate_3', 0.01, 0.5)\n",
    "#                       )\n",
    "#               )\n",
    "#     #pool_size_3 = trial.suggest_int('pool_size_3', 2, max(2, input_length_3 // 2))\n",
    "#     #pool_size_3 = trial.suggest_int('pool_size_3', 2, 8)\n",
    "#     #model.add(MaxPooling1D(pool_size=pool_size_3))\n",
    "\n",
    "#     # Warstwy Dense\n",
    "#     model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "#     model.add(Dense(units=trial.suggest_int('dense_units_1', 40, 300), activation=activation))\n",
    "#     model.add(Dropout(rate=trial.suggest_float('dropout_rate_4', 0.01, 0.75)))\n",
    "    \n",
    "#     model.add(Dense(units=trial.suggest_int('dense_units_2', 20, 200), activation=activation))\n",
    "#     model.add(Dropout(rate=trial.suggest_float('dropout_rate_5', 0.01, 0.65)))\n",
    "    \n",
    "#     model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#     # Optymalizator\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 1e-5, 1e-1, log=True)\n",
    "#     decay_steps = trial.suggest_int(\"decay_steps\", 10, 150)\n",
    "#     decay_rate = trial.suggest_float(\"decay_rate\", 0.01, 0.99)\n",
    "#     lr_schedule = ExponentialDecay(initial_learning_rate=lr_initial,\n",
    "#                                   decay_steps=decay_steps,\n",
    "#                                   decay_rate=decay_rate)\n",
    "\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['Lion'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_schedule)\n",
    "\n",
    "    \n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='val_accuracy', patience=16, start_from_epoch=10, min_delta = 0.00005)\n",
    "\n",
    "#     # Trenowanie modelu\n",
    "#     model.fit(X_train_reduced, y_train_filters,\n",
    "#               epochs=trial.suggest_int('epochs', 40, 150),\n",
    "#               batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "#               validation_data=(X_val_reduced, y_val_filters),\n",
    "#               verbose=1,\n",
    "#               callbacks=[early_stopping])\n",
    "\n",
    "#     # Ewaluacja modelu\n",
    "#     _, accuracy = model.evaluate(X_val_reduced, y_val_filters, verbose=0)\n",
    "\n",
    "#     del model\n",
    "#     return accuracy\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "# #[I 2024-10-04 05:18:28,377] Trial 26 finished with value: 0.9810000061988831 and parameters: {'activation': 'elu', 'conv1d_filters_1': 347, 'kernel_size_1': 7,  'l1_1': 0.0002141584459916536, 'l2_1': 4.201061472934882e-06, 'pool_size_1': 6, 'conv1d_filters_2': 77, 'kernel_size_2': 14, 'l1_2': 3.203529032832527e-06, 'l2_2': 0.00015793896479891322, 'pool_size_2': 2, 'conv1d_filters_3': 258, 'kernel_size_3': 4, 'dropout_rate_3': 0.3297488074504043, 'dense_units_1': 271, 'dropout_rate_4': 0.5097723961373417, 'dense_units_2': 78, 'dropout_rate_5': 0.15473996425601394, 'lr_initial': 0.0004175922071628326, 'decay_steps': 33, 'decay_rate': 0.9780884395210858, 'optimizer': 'Lion', 'epochs': 123, 'batch_size': 264}. Best is trial 26 with value: 0.9810000061988831.\n",
    "# #[I 2024-10-04 14:58:14,236] Trial 61 finished with value: 0.9797499775886536 and parameters: {'activation': 'elu', 'conv1d_filters_1': 424, 'kernel_size_1': 13, 'l1_1': 0.0005501543495144062, 'l2_1': 0.02794377888054429,   'pool_size_1': 7, 'conv1d_filters_2': 79, 'kernel_size_2': 12, 'l1_2': 1.3699264627909325e-06, 'l2_2': 6.577814668979489e-06, 'pool_size_2': 3, 'conv1d_filters_3': 280, 'kernel_size_3': 3, 'dropout_rate_3': 0.21341586312675653, 'dense_units_1': 292, 'dropout_rate_4': 0.22415600475665626, 'dense_units_2': 106, 'dropout_rate_5': 0.09686035013603865, 'lr_initial': 0.0004430745906851504, 'decay_steps': 22, 'decay_rate': 0.9872365423551039, 'optimizer': 'Lion', 'epochs': 150, 'batch_size': 216}. Best is trial 26 with value: 0.9810000061988831.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sieć CONV2D z zamrożonym PCA dla danych pierwotnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# import tensorflow_probability as tfp\n",
    "# import optuna\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "# X_train_image = X_train_image.astype('float32')\n",
    "# X_val_image = X_val_image.astype('float32')\n",
    "# X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "# X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "# def objective(trial):\n",
    "#     # Przestrzeń hiperparametrów\n",
    "#     num_filters1 = trial.suggest_int(\"num_filters1\", 32, 256)    # Pierwsza warstwa Conv2d\n",
    "#     num_filters2 = trial.suggest_int(\"num_filters2\", 64, 256)    # Druga warstwa Conv2d\n",
    "#     num_filters3 = trial.suggest_int(\"num_filters3\", 128, 256)   # Trzecia warstwa Conv2d\n",
    "#     n_components = trial.suggest_int(\"PCA_components\", 350, 500) # PCA\n",
    "\n",
    "#     l1_1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True)\n",
    "#     l2_1=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     l1_2=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True)\n",
    "#     l2_2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "\n",
    "#     l1_3=trial.suggest_float('l1_3', 1e-6, 1e-1, log=True)\n",
    "#     l2_3=trial.suggest_float('l2_3', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     activation_functions = ['relu', 'gelu', 'leaky_relu', 'silu', 'mish']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "    \n",
    "    \n",
    "#     # Warstwy konwolucyjne 2D\n",
    "#     conv_layers = [\n",
    "#         # Pierwsza warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters1,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_1),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_1),\n",
    "#                                 padding='same',\n",
    "#                                 input_shape=(X_train_image.shape[1:])\n",
    "#                                 ),\n",
    "        \n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2,strides=1,\n",
    "#                                  ),\n",
    "#         # Druga warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters2,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_2),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_2),                                \n",
    "#                                 padding='same',\n",
    "#                                 ),\n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "#         # Trzecia warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters3,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_3),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_3),                                \n",
    "#                                 padding='same'\n",
    "#                                 ),\n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        \n",
    "#         tf.keras.layers.Flatten(),\n",
    "#                 ]\n",
    "\n",
    "#     ### Obliczenia dla zamrożonego PCA na podstawie pierwszej inicjalizacji modelu\n",
    "#     conv_model = keras.models.Sequential(conv_layers)  \n",
    "#     X_train_PCA = conv_model.predict(X_train_image)\n",
    "    \n",
    "#     X_train_PCA = tf.reshape(X_train_PCA, [X_train_PCA.shape[0], -1])\n",
    "\n",
    "#     # Macierz kowariancji\n",
    "#     covariance_matrix = tfp.stats.covariance(X_train_PCA, sample_axis=0, event_axis=-1)\n",
    "#     # Wektory i wartości własne\n",
    "#     eigenvalues, eigenvectors = tf.linalg.eigh(covariance_matrix)\n",
    "#     sorted_indices = tf.argsort(eigenvalues, direction='DESCENDING')\n",
    "#     eigenvalues = tf.gather(eigenvalues, sorted_indices)\n",
    "#     eigenvectors = tf.gather(eigenvectors, sorted_indices, axis=1)\n",
    "    \n",
    "#     #### Opcja druga na SVD, ale wymaga jeszcze zabawy\n",
    "#     # s, u, v = tf.linalg.svd(covariance_matrix)\n",
    "#     # eigenvectors = v\n",
    "#     # eigenvalues = tf.square(s) \n",
    "    \n",
    "#     # Warstwa PCA\n",
    "#     model = keras.models.Sequential(conv_layers + [\n",
    "#         tf.keras.layers.Lambda(lambda x, eigenvectors: tf.matmul(x, eigenvectors), arguments={'eigenvectors': eigenvectors[:, :n_components]})\n",
    "#     ])\n",
    "\n",
    "#     # Output (klasyfikacja)\n",
    "#     model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "#     #model.summary()\n",
    "#     tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    \n",
    "\n",
    "#     # Optymalizator\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "#     # Początkową wartość learning rate\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "#     ## Kompilacja modelu\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=8, start_from_epoch=8, min_delta=0.0001)\n",
    "    \n",
    "#     # ReduceLROnPlateau\n",
    "#     reduce_lr = ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.9, step=0.1),\n",
    "#         patience=trial.suggest_int(\"reduce_lr_patience\", 2, 12),\n",
    "#         min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "#     )\n",
    "    \n",
    "#     # Trenowanie i ocena modelu\n",
    "#     history = model.fit(X_train_image, y_train_image,\n",
    "#                         epochs=trial.suggest_int('epochs', 15, 80),\n",
    "#                         batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "#                         validation_data=(X_val_image, y_val_image),\n",
    "#                         callbacks=[early_stopping, reduce_lr]\n",
    "#                         )\n",
    "    \n",
    "#     accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) \n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "# ### dwa tygodnie więc lepiej aby te wartości stanowiły dobrą podstawę...\n",
    "# #Trial 76 finished with value:  0.9906666874885559 and parameters: {'num_filters1': 194, 'num_filters2': 167, 'num_filters3': 221, 'PCA_components': 383, 'l1_1': 1.4917571854023011e-06, 'l2_1': 0.0003844462705458024,  'l1_2': 0.0011921068803372,     'l2_2': 0.00021948428596782946, 'l1_3': 0.05677951548917238,    'l2_3': 7.714986268695782e-05,  'activation': 'relu', 'optimizer': 'AdamW', 'lr_initial': 0.006869840110235172, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.028747844465778e-06,  'epochs': 51, 'batch_size': 479}.\n",
    "# #Trial 99 finished with value:  0.9911666512489319 and parameters: {'num_filters1': 222, 'num_filters2': 220, 'num_filters3': 202, 'PCA_components': 367, 'l1_1': 4.875844948451946e-06,  'l2_1': 0.00014963179483695857, 'l1_2': 2.3622464527905993e-06, 'l2_2': 0.0007506481920246097,  'l1_3': 0.030678868126237845,   'l2_3': 0.00016514043221099142, 'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.00653453290497133,  'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.8941869280860333e-06, 'epochs': 54, 'batch_size': 331}.\n",
    "# #Trial 108 finished with value: 0.9915000200271606 and parameters: {'num_filters1': 205, 'num_filters2': 206, 'num_filters3': 220, 'PCA_components': 370, 'l1_1': 9.95600828812184e-06,   'l2_1': 4.680137051239973e-05,  'l1_2': 4.9076000996726795e-06, 'l2_2': 8.638060092445147e-05,  'l1_3': 0.0361709248993415,     'l2_3': 0.0007397174884371887,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006269166650314777, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 5.203896150606987e-06,  'epochs': 62, 'batch_size': 239}.\n",
    "# #Trial 121 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 189, 'num_filters2': 200, 'num_filters3': 220, 'PCA_components': 387, 'l1_1': 8.32443244291399e-05,   'l2_1': 5.963850113018811e-05,  'l1_2': 4.8017415472934674e-06, 'l2_2': 3.9423405128005956e-05, 'l1_3': 0.00047971791859799064, 'l2_3': 0.0014900662405900767,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006533479568732247, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 6, 'reduce_lr_min_lr': 7.315164005922789e-06,  'epochs': 58, 'batch_size': 249}.\n",
    "# #Trial 134 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 180, 'num_filters2': 196, 'num_filters3': 234, 'PCA_components': 390, 'l1_1': 4.148854564296224e-05,  'l2_1': 2.3209137723337733e-05, 'l1_2': 0.0009209306936723504,  'l2_2': 1.9816087702628798e-05, 'l1_3': 0.0008993453039409338,  'l2_3': 0.002218879345590759,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007404316715937198, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 3.862546224445297e-07,  'epochs': 61, 'batch_size': 368}.\n",
    "# #Trial 151 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 201, 'num_filters2': 189, 'num_filters3': 229, 'PCA_components': 403, 'l1_1': 6.15892270118e-05,      'l2_1': 1.6056284141459273e-05, 'l1_2': 0.0009167391639733724,  'l2_2': 1.4409315946656477e-05, 'l1_3': 0.0008899963895277467,  'l2_3': 0.005080838638011425,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007378956521980373, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 1.9759552872681682e-07, 'epochs': 63, 'batch_size': 416}.\n",
    "# #Trial 153 finished with value: 0.9905833601951599 and parameters: {'num_filters1': 158, 'num_filters2': 190, 'num_filters3': 229, 'PCA_components': 405, 'l1_1': 3.317865892202326e-05,  'l2_1': 1.715760099034611e-05,  'l1_2': 0.00028475050803045533, 'l2_2': 7.144046413508896e-06,  'l1_3': 0.0010057548651594868,  'l2_3': 0.001544820248650564,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007667044780464368, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 1.6284942102179344e-07, 'epochs': 64, 'batch_size': 420}.\n",
    "# #Trial 189 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 187, 'num_filters2': 194, 'num_filters3': 191, 'PCA_components': 381, 'l1_1': 0.0001222508587314991,  'l2_1': 5.2900939483419044e-05, 'l1_2': 4.050810455502403e-05,  'l2_2': 0.00017281194315162931, 'l1_3': 0.0014341620209873955,  'l2_3': 0.0025511470455798512,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006996548564114207, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 6, 'reduce_lr_min_lr': 9.663271049317225e-07,  'epochs': 60, 'batch_size': 219}.\n",
    "# #Trial 205 finished with value: 0.9904999732971191 and parameters: {'num_filters1': 210, 'num_filters2': 189, 'num_filters3': 198, 'PCA_components': 483, 'l1_1': 8.253178154361272e-05,  'l2_1': 1.7866032331171518e-05, 'l1_2': 0.0008019764970918962,  'l2_2': 1.0316537375038554e-05, 'l1_3': 0.000838618983241391,   'l2_3': 0.008249547665798275,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006570979651986537, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.6114120319881016e-07, 'epochs': 63, 'batch_size': 560}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V2 zmina na dynamic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# import tensorflow_probability as tfp\n",
    "# import optuna\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau \n",
    "\n",
    "# X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "# X_train_image = X_train_image.astype('float32')\n",
    "# X_val_image = X_val_image.astype('float32')\n",
    "# X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "# X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "# def objective(trial):\n",
    "#     # Przestrzeń hiperparametrów\n",
    "#     num_filters1 = trial.suggest_int(\"num_filters1\", 180, 230)    # Pierwsza warstwa Conv2d\n",
    "#     num_filters2 = trial.suggest_int(\"num_filters2\", 180, 230)    # Druga warstwa Conv2d\n",
    "#     num_filters3 = trial.suggest_int(\"num_filters3\", 180, 260)    # Trzecia warstwa Conv2d\n",
    "#     n_components = trial.suggest_int(\"PCA_components\", 350, 420)  # PCA\n",
    "    \n",
    "#     l1_1=trial.suggest_float('l1_1', 1e-6, 5e-3, log=True)\n",
    "#     l2_1=trial.suggest_float('l2_1', 5e-5, 5e-3, log=True)\n",
    "    \n",
    "#     l1_2=trial.suggest_float('l1_2', 1e-6, 1e-3, log=True)\n",
    "#     l2_2=trial.suggest_float('l2_2', 1e-6, 1e-5, log=True)\n",
    "\n",
    "#     l1_3=trial.suggest_float('l1_3', 1e-6, 1e-1, log=True)\n",
    "#     l2_3=trial.suggest_float('l2_3', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     activation_functions = ['gelu', 'leaky_relu', 'silu', 'mish']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "    \n",
    "    \n",
    "#     class DynamicPCA(keras.layers.Layer):\n",
    "#         def __init__(self, n_components, layer, update_freq=2, threshold=0.5, **kwargs):  \n",
    "#             super(DynamicPCA, self).__init__(**kwargs)\n",
    "#             self.n_components = n_components\n",
    "#             self.threshold = threshold\n",
    "#             self.layer = layer  \n",
    "#             self.update_freq = update_freq\n",
    "#             self.val_loss = None\n",
    "#             self.eigenvectors = None\n",
    "#             self.epoch_count = 0\n",
    "\n",
    "#         def build(self, input_shape):\n",
    "#             self.eigenvectors = self.add_weight(\n",
    "#                 shape=(input_shape[-1], self.n_components),\n",
    "#                 initializer=\"zeros\",\n",
    "#                 trainable=True,\n",
    "#             )\n",
    "\n",
    "#         def call(self, inputs):\n",
    "#             self.epoch_count += 1\n",
    "#             if (self.val_loss is not None and \n",
    "#                 self.val_loss < self.threshold and \n",
    "#                 self.epoch_count % self.update_freq == 0):\n",
    "#                 print(\"DynamicPCA: Aktualizacja wektorów własnych!\")\n",
    "#                 print(f\"val_loss: {self.val_loss}, threshold: {self.threshold}, epoch_count: {self.epoch_count}, update_freq: {self.update_freq}\")\n",
    "                \n",
    "#                 # Pobierz wagi bezpośrednio z warstwy\n",
    "#                 weights = self.layer.get_weights()  \n",
    "#                 flat_weights = tf.concat([tf.reshape(w, [-1]) for w in weights], axis=0)\n",
    "#                 # Normalizacja wag\n",
    "#                 flat_weights = tf.math.l2_normalize(flat_weights)\n",
    "#                 # Obliczanie nowych wektorów własnych\n",
    "#                 new_eigenvectors = self.calculate_pca(flat_weights)\n",
    "#                 # Aktualizacja wag (wektorów własnych)\n",
    "#                 self.eigenvectors.assign(new_eigenvectors)\n",
    "\n",
    "#             # Mnożenie przez wektory własne\n",
    "#             output = tf.matmul(inputs, self.eigenvectors)\n",
    "#             return output\n",
    "\n",
    "#         def calculate_pca(self, inputs):\n",
    "#             print(flat_weights.shape)\n",
    "#             # Obliczanie macierzy kowariancji\n",
    "#             covariance_matrix = tf.linalg.cov(inputs)  \n",
    "#             # Wektory i wartości własne\n",
    "#             eigenvalues, eigenvectors = tf.linalg.eigh(covariance_matrix)\n",
    "#             sorted_indices = tf.argsort(eigenvalues, direction='DESCENDING')\n",
    "#             eigenvectors = tf.gather(eigenvectors, sorted_indices, axis=1)\n",
    "#             return eigenvectors[:, :self.n_components]\n",
    "\n",
    "#     class MyCallback(keras.callbacks.Callback):\n",
    "#         def on_epoch_end(self, epoch, logs=None):\n",
    "#             pca_layer = self.model.get_layer('dynamic_pca')\n",
    "#             pca_layer.val_loss = logs.get('val_loss')\n",
    "#             print(logs.get('val_loss'))\n",
    "\n",
    "#     # Definiowanie wejścia\n",
    "#     image_input = keras.Input(shape=X_train_image.shape[1:])  \n",
    "\n",
    "#     # Warstwy konwolucyjne\n",
    "#     conv1 = keras.layers.Conv2D(num_filters1, kernel_size=(2, 2), activation=activation)(image_input)\n",
    "#     pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "#     conv2 = keras.layers.Conv2D(num_filters2, kernel_size=(2, 2), activation=activation)(pool1)\n",
    "#     pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "#     conv3 = keras.layers.Conv2D(num_filters3, kernel_size=(2, 2), activation=activation)(pool2)\n",
    "#     pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "#     flat_cnn = keras.layers.Flatten()(pool3)\n",
    "\n",
    "#     # Dense 1\n",
    "#     dense = keras.layers.Dense(units=trial.suggest_int('dense_units_1', 60, 240), activation=activation)(flat_cnn) \n",
    "#     dense = keras.layers.Dropout(rate=trial.suggest_float('dropout_rate_1', 0.01, 0.75))(dense)\n",
    "\n",
    "#     # Warstwa DynamicPCA \n",
    "#     pca_output = DynamicPCA(n_components=n_components, layer=conv3, name='dynamic_pca', threshold=1.25)(flat_cnn)  \n",
    "    \n",
    "#     merged = keras.layers.Concatenate()([dense, pca_output])\n",
    "\n",
    "#     dense2 = keras.layers.Dense(units=trial.suggest_int('dense_units_2', 60, 240), activation=activation)(merged)  \n",
    "#     dense2 = keras.layers.Dropout(rate=trial.suggest_float('dropout_rate_2', 0.01, 0.75))(dense2)\n",
    "\n",
    "#     # Output (klasyfikacja)\n",
    "#     outputs = keras.layers.Dense(10, activation='softmax')(dense2)\n",
    "\n",
    "#     # Utworzenie modelu\n",
    "#     model = keras.Model(inputs=image_input, outputs=outputs)\n",
    "\n",
    "#     # Utworzenie modelu\n",
    "#     model = keras.Model(inputs=image_input, outputs=outputs)\n",
    "#     #model.summary()\n",
    "#     tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    \n",
    "\n",
    "#     # Optymalizator\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "#     # Początkową wartość learning rate\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "#     ## Kompilacja modelu\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=10, start_from_epoch=8, min_delta=0.0001)\n",
    "    \n",
    "#     # ReduceLROnPlateau\n",
    "#     reduce_lr = ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.5, step=0.1),\n",
    "#         patience=trial.suggest_int(\"reduce_lr_patience\", 4, 8),\n",
    "#         min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-5, log=True)\n",
    "#     )\n",
    "    \n",
    "#     # Trenowanie i ocena modelu\n",
    "#     history = model.fit(X_train_image, y_train_image,\n",
    "#                         epochs=trial.suggest_int('epochs', 20, 80),\n",
    "#                         batch_size=trial.suggest_int('batch_size', 164, 768),\n",
    "#                         validation_data=(X_val_image, y_val_image),\n",
    "#                         validation_freq=1,\n",
    "#                         callbacks=[early_stopping, reduce_lr, MyCallback()]\n",
    "#                         )\n",
    "    \n",
    "#     accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) \n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model bazujący na dwóch potokach danych. Surowych przechodzących przez Conv2d, oraz LSTM dla danych z HOG i Gabor. Wykorzystanie mechanizmu atencji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n"
     ]
    }
   ],
   "source": [
    "import keras as tf\n",
    "print(tf.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-26 13:39:28,025] A new study created in memory with name: no-name-755f8ea2-3658-4c11-a064-b512d8f6f628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_1']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n",
      "[I 2024-10-26 13:49:02,780] Trial 0 finished with value: 0.14399999380111694 and parameters: {'activation': 'leaky_relu', 'Sep-Conv2d filters1': 85, 'Sep-Conv2d filters2': 126, 'Sep-Conv2d filters3': 105, 'l1_1_1D': 5.336789111525558e-06, 'l2_1_1D': 1.0039425147584368e-06, 'l1_2_1D': 0.00023053069325812393, 'l2_2_1D': 2.6022544136051107e-05, 'l1_3_1D': 0.0017991628224367847, 'l2_3_1D': 0.00011848460407226111, 'Conv2d filters1': 70, 'Conv2d filters2': 127, 'Conv2d filters3': 154, 'l1_1_2D': 6.131835042095534e-06, 'l2_1_2D': 0.0025453068989822196, 'l1_2_2D': 0.002552482170564757, 'l2_2_2D': 0.001029856056201003, 'l1_3_2D': 0.022545426343153818, 'l2_3_2D': 0.0015190165371143708, 'dense SeparableConv1D': 152, 'dropout_rate_1': 0.7124794388003659, 'dense Conv2D': 116, 'dropout_rate_2': 0.27959629839654043, 'dense przed Output': 148, 'dropout przed Output': 0.07386328428094646, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 14, 'reduce_lr_min_lr': 2.8911995954743493e-05, 'optimizer': 'Lion', 'lr_initial': 0.05332245112057965, 'epochs': 47, 'batch_size': 372}. Best is trial 0 with value: 0.14399999380111694.\n",
      "[I 2024-10-26 13:54:58,783] Trial 1 finished with value: 0.1067499965429306 and parameters: {'activation': 'silu', 'Sep-Conv2d filters1': 61, 'Sep-Conv2d filters2': 102, 'Sep-Conv2d filters3': 146, 'l1_1_1D': 0.00027818185440464514, 'l2_1_1D': 1.4292855945071587e-06, 'l1_2_1D': 0.0002058453395630073, 'l2_2_1D': 0.0007334723000004829, 'l1_3_1D': 0.03539887321146669, 'l2_3_1D': 0.00023664685070403583, 'Conv2d filters1': 78, 'Conv2d filters2': 84, 'Conv2d filters3': 76, 'l1_1_2D': 5.359171921927817e-06, 'l2_1_2D': 0.0620818021839478, 'l1_2_2D': 0.0337052460393501, 'l2_2_2D': 1.78809107867914e-05, 'l1_3_2D': 6.5259712943597265e-06, 'l2_3_2D': 0.00019618524571012977, 'dense SeparableConv1D': 203, 'dropout_rate_1': 0.14266899235820032, 'dense Conv2D': 105, 'dropout_rate_2': 0.6622551174098077, 'dense przed Output': 181, 'dropout przed Output': 0.2569543962594121, 'reduce_lr_factor': 0.6, 'reduce_lr_patience': 11, 'reduce_lr_min_lr': 1.230320384808469e-05, 'optimizer': 'Lion', 'lr_initial': 0.049795303845732164, 'epochs': 86, 'batch_size': 659}. Best is trial 0 with value: 0.14399999380111694.\n",
      "[I 2024-10-26 14:25:02,537] Trial 2 finished with value: 0.1003333330154419 and parameters: {'activation': 'gelu', 'Sep-Conv2d filters1': 75, 'Sep-Conv2d filters2': 131, 'Sep-Conv2d filters3': 178, 'l1_1_1D': 9.973446503565196e-06, 'l2_1_1D': 3.0667424536878907e-06, 'l1_2_1D': 0.0020039765574626418, 'l2_2_1D': 1.2505598085069439e-05, 'l1_3_1D': 1.0126241047163868e-06, 'l2_3_1D': 0.002741106691305197, 'Conv2d filters1': 108, 'Conv2d filters2': 55, 'Conv2d filters3': 81, 'l1_1_2D': 0.0008346755916641932, 'l2_1_2D': 3.5787249885194285e-05, 'l1_2_2D': 0.00027328402808725533, 'l2_2_2D': 0.03989902967518663, 'l1_3_2D': 0.003331999219336138, 'l2_3_2D': 0.0001925026567019862, 'dense SeparableConv1D': 182, 'dropout_rate_1': 0.09701086238882939, 'dense Conv2D': 189, 'dropout_rate_2': 0.4014169043467464, 'dense przed Output': 140, 'dropout przed Output': 0.6193069572636433, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 13, 'reduce_lr_min_lr': 1.0865603258632137e-06, 'optimizer': 'AdamW', 'lr_initial': 0.05506348180072123, 'epochs': 58, 'batch_size': 490}. Best is trial 0 with value: 0.14399999380111694.\n",
      "[I 2024-10-26 14:32:01,168] Trial 3 finished with value: 0.2561666667461395 and parameters: {'activation': 'gelu', 'Sep-Conv2d filters1': 90, 'Sep-Conv2d filters2': 54, 'Sep-Conv2d filters3': 97, 'l1_1_1D': 6.2492347687514564e-06, 'l2_1_1D': 0.04507872718816157, 'l1_2_1D': 0.0017724918278183785, 'l2_2_1D': 0.004898517948512782, 'l1_3_1D': 0.00019187426811373935, 'l2_3_1D': 0.015893817025900373, 'Conv2d filters1': 73, 'Conv2d filters2': 52, 'Conv2d filters3': 139, 'l1_1_2D': 0.0007702087742993694, 'l2_1_2D': 4.577018774907882e-05, 'l1_2_2D': 0.0011088463806123976, 'l2_2_2D': 0.0013986332374122349, 'l1_3_2D': 1.483789867952185e-06, 'l2_3_2D': 9.89737767900214e-05, 'dense SeparableConv1D': 205, 'dropout_rate_1': 0.38701007005959276, 'dense Conv2D': 167, 'dropout_rate_2': 0.2881856270440126, 'dense przed Output': 116, 'dropout przed Output': 0.22788610741451787, 'reduce_lr_factor': 0.30000000000000004, 'reduce_lr_patience': 3, 'reduce_lr_min_lr': 6.197681331662438e-05, 'optimizer': 'AdamW', 'lr_initial': 0.02274693603917496, 'epochs': 61, 'batch_size': 218}. Best is trial 3 with value: 0.2561666667461395.\n",
      "[I 2024-10-26 14:53:20,178] Trial 4 finished with value: 0.3370000123977661 and parameters: {'activation': 'gelu', 'Sep-Conv2d filters1': 128, 'Sep-Conv2d filters2': 147, 'Sep-Conv2d filters3': 148, 'l1_1_1D': 0.020984377342334184, 'l2_1_1D': 0.06079714949290571, 'l1_2_1D': 1.3508055221110504e-06, 'l2_2_1D': 8.557466189437341e-05, 'l1_3_1D': 0.0072045491982117025, 'l2_3_1D': 0.00574771780695762, 'Conv2d filters1': 94, 'Conv2d filters2': 147, 'Conv2d filters3': 126, 'l1_1_2D': 3.925496646358777e-05, 'l2_1_2D': 0.00024329473351781175, 'l1_2_2D': 1.484012921797658e-06, 'l2_2_2D': 0.021012518604781837, 'l1_3_2D': 2.2584238334889673e-06, 'l2_3_2D': 0.00015836509337163962, 'dense SeparableConv1D': 138, 'dropout_rate_1': 0.49255313543674695, 'dense Conv2D': 194, 'dropout_rate_2': 0.3373233807229527, 'dense przed Output': 87, 'dropout przed Output': 0.6285785098202348, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 3, 'reduce_lr_min_lr': 6.036943636308863e-06, 'optimizer': 'AdamW', 'lr_initial': 0.0371261734048358, 'epochs': 37, 'batch_size': 554}. Best is trial 4 with value: 0.3370000123977661.\n",
      "[W 2024-10-26 14:54:38,287] Trial 5 failed with parameters: {'activation': 'silu', 'Sep-Conv2d filters1': 62, 'Sep-Conv2d filters2': 121, 'Sep-Conv2d filters3': 175, 'l1_1_1D': 1.973038196305863e-06, 'l2_1_1D': 0.00028290673663447274, 'l1_2_1D': 0.0040707611201993765, 'l2_2_1D': 0.021517051830391272, 'l1_3_1D': 8.292381435465813e-06, 'l2_3_1D': 3.2664778582116802e-06, 'Conv2d filters1': 109, 'Conv2d filters2': 76, 'Conv2d filters3': 152, 'l1_1_2D': 0.00016558764863757125, 'l2_1_2D': 1.038650088020111e-06, 'l1_2_2D': 0.018816088062752093, 'l2_2_2D': 2.231285030474048e-06, 'l1_3_2D': 0.07461393244435371, 'l2_3_2D': 0.0001402130369208994, 'dense SeparableConv1D': 163, 'dropout_rate_1': 0.2994923140013266, 'dense Conv2D': 213, 'dropout_rate_2': 0.21303384086157284, 'dense przed Output': 146, 'dropout przed Output': 0.703967584155221, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 10, 'reduce_lr_min_lr': 6.13082858124615e-07, 'optimizer': 'AdamW', 'lr_initial': 0.010211294291571887, 'epochs': 79, 'batch_size': 602} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_6932\\1687020451.py\", line 202, in objective\n",
      "    history = model.fit(\n",
      "              ^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n",
      "    logs = self.train_function(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 833, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 878, in _call\n",
      "    results = tracing_compilation.call_function(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py\", line 139, in call_function\n",
      "    return function._call_flat(  # pylint: disable=protected-access\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py\", line 1322, in _call_flat\n",
      "    return self._inference_function.call_preflattened(args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 216, in call_preflattened\n",
      "    flat_outputs = self.call_flat(*args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 251, in call_flat\n",
      "    outputs = self._bound_context.call_function(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 1683, in call_function\n",
      "    outputs = execute.execute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 53, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-26 14:54:38,291] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 222\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_val_accuracy\n\u001b[0;32m    221\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler())\n\u001b[1;32m--> 222\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Najlepsze parametry i wynik\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNajlepsze parametry:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[17], line 202\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    197\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, start_from_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m#log_dir = \"logs/\"  # Katalog, w którym będą zapisywane dane TensorBoard\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m#tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1 )\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Trenowanie modelu\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_reduced\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m164\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_val_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_reduced\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_filters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# , tensorboard\u001b[39;49;00m\n\u001b[0;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m best_val_accuracy \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# Ewaluacja modelu na danych testowych\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m#loss, accuracy = model.evaluate([X_test, X_test], y_test_ENC, verbose=1)\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m#print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\domin\\miniforge3\\envs\\ML1\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "\n",
    "from tensorflow.keras.layers import ( Input, LSTM, GlobalAveragePooling1D, Permute, PReLU, Flatten, Reshape, Conv2D, MaxPooling2D, Concatenate, Attention, Dense,\n",
    "                                     LayerNormalization, ActivityRegularization, Dropout, GlobalAveragePooling2D, AveragePooling2D, AveragePooling1D, GlobalAveragePooling1D, \n",
    "                                     GlobalMaxPool1D, GlobalMaxPool2D, MaxPool2D, SeparableConv1D, RandomRotation, BatchNormalization\n",
    "                                    )\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import gc\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "X_train_reduced = np.load('X_train_reduced.npy')\n",
    "X_val_reduced = np.load('X_val_reduced.npy')\n",
    "y_train_filters = np.load('y_train_filters.npy')\n",
    "y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "X_train_image = X_train_image.astype('float32')\n",
    "X_val_image = X_val_image.astype('float32')\n",
    "X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    "\n",
    "class SpectralNorm(Constraint):\n",
    "    \"\"\"\n",
    "    Spectral Normalization constraint.\n",
    "\n",
    "    Args:\n",
    "        iteration: Number of iterations for power iteration.\n",
    "    \"\"\"\n",
    "    def __init__(self, iteration=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.iteration = iteration\n",
    "        self.u = None\n",
    "\n",
    "    def __call__(self, w):\n",
    "        w_shape = w.shape.as_list()\n",
    "        w = tf.reshape(w, [-1, w_shape[-1]])\n",
    "\n",
    "        if self.u is None:\n",
    "            self.u = tf.Variable(\n",
    "                initial_value=tf.keras.initializers.RandomNormal()(shape=[1, w_shape[-1]]),\n",
    "                trainable=False\n",
    "            )\n",
    "\n",
    "        if self.iteration == 1:\n",
    "            v_hat = tf.matmul(self.u, w, transpose_b=True)\n",
    "            v_hat = v_hat / tf.norm(v_hat, ord=2, axis=-1, keepdims=True)\n",
    "            u_hat = tf.matmul(v_hat, w)\n",
    "            u_hat = u_hat / tf.norm(u_hat, ord=2, axis=-1, keepdims=True)\n",
    "            self.u.assign(u_hat)  # Aktualizacja self.u\n",
    "        else:\n",
    "            def body(i, u_hat):\n",
    "                v_hat = tf.matmul(u_hat, w, transpose_b=True)\n",
    "                v_hat = v_hat / tf.norm(v_hat, ord=2, axis=-1, keepdims=True)\n",
    "                u_hat = tf.matmul(v_hat, w)\n",
    "                u_hat = u_hat / tf.norm(u_hat, ord=2, axis=-1, keepdims=True)\n",
    "                self.u.assign(u_hat)  # Aktualizacja self.u\n",
    "                return i + 1, u_hat\n",
    "\n",
    "            _, u_hat = tf.while_loop(\n",
    "                lambda i, _: i < self.iteration,\n",
    "                body,\n",
    "                loop_vars=[0, self.u]\n",
    "            )\n",
    "\n",
    "        w_norm = w / tf.norm(w, ord=2, axis=0, keepdims=True)  # Normalizacja wag\n",
    "        w_norm = tf.reshape(w_norm, w_shape)\n",
    "\n",
    "        return w_norm\n",
    "    \n",
    "def create_sepconv1d_layer(units, activation, l1, l2):\n",
    "    return SeparableConv1D(\n",
    "        units,\n",
    "        kernel_size=1,\n",
    "        padding='same',\n",
    "        activation=activation,\n",
    "        depthwise_constraint=SpectralNorm,\n",
    "        bias_regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    )\n",
    "\n",
    "def create_conv2d_layer(filters, activation, l1, l2):\n",
    "    return Conv2D(\n",
    "        filters,\n",
    "        kernel_size=(2, 2),\n",
    "        activation=activation,\n",
    "        kernel_constraint=SpectralNorm,\n",
    "        bias_regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    )    \n",
    "    \n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    # Dane wejściowe\n",
    "    image_input = Input(shape=X_train_image.shape[1:])\n",
    "    flattened_input = Input(shape=X_train_reduced.shape[1:])\n",
    "\n",
    "    ### Hiperparamatry \n",
    "    activation_functions = ['gelu', 'leaky_relu', 'silu', 'mish']\n",
    "    activation = trial.suggest_categorical('activation', activation_functions)\n",
    "        \n",
    "    filters1_1D=trial.suggest_int('Sep-Conv2d filters1', 32, 128)  # Pierwsza warstwa SeparableConv1D\n",
    "    filters2_1D=trial.suggest_int('Sep-Conv2d filters2', 48, 148)  # Druga warstwa SeparableConv1D\n",
    "    filters3_1D=trial.suggest_int('Sep-Conv2d filters3', 64, 184) # Trzecia warstwa SeparableConv1D\n",
    "    \n",
    "    l1_1_1D=trial.suggest_float('l1_1_1D', 1e-6, 1e-1, log=True)\n",
    "    l2_1_1D=trial.suggest_float('l2_1_1D', 1e-6, 1e-1, log=True)\n",
    "    l1_2_1D=trial.suggest_float('l1_2_1D', 1e-6, 1e-1, log=True)\n",
    "    l2_2_1D=trial.suggest_float('l2_2_1D', 1e-6, 1e-1, log=True)\n",
    "    l1_3_1D=trial.suggest_float('l1_3_1D', 1e-6, 1e-1, log=True)\n",
    "    l2_3_1D=trial.suggest_float('l2_3_1D', 1e-6, 1e-1, log=True)\n",
    "       \n",
    "    filters1_2D = trial.suggest_int(\"Conv2d filters1\", 32, 128)    # Pierwsza warstwa Conv2d\n",
    "    filters2_2D = trial.suggest_int(\"Conv2d filters2\", 48, 148)    # Druga warstwa Conv2d\n",
    "    filters3_2D = trial.suggest_int(\"Conv2d filters3\", 64, 184)   # Trzecia warstwa Conv2d\n",
    "\n",
    "    l1_1_2D=trial.suggest_float('l1_1_2D', 1e-6, 1e-1, log=True)\n",
    "    l2_1_2D=trial.suggest_float('l2_1_2D', 1e-6, 1e-1, log=True)\n",
    "    l1_2_2D=trial.suggest_float('l1_2_2D', 1e-6, 1e-1, log=True)\n",
    "    l2_2_2D=trial.suggest_float('l2_2_2D', 1e-6, 1e-1, log=True)\n",
    "    l1_3_2D=trial.suggest_float('l1_3_2D', 1e-6, 1e-1, log=True)\n",
    "    l2_3_2D=trial.suggest_float('l2_3_2D', 1e-6, 1e-1, log=True)\n",
    "\n",
    "    # Gałąź SeparableConv1D\n",
    "    Sep_Conv1D_1 = create_sepconv1d_layer(filters1_1D, activation, l1_1_1D, l2_1_1D)(flattened_input)\n",
    "    Sep_Conv1D_2 = create_sepconv1d_layer(filters2_1D, activation, l1_2_1D, l2_2_1D)(Sep_Conv1D_1)\n",
    "    Sep_Conv1D_3 = create_sepconv1d_layer(filters3_1D, activation, l1_3_1D, l2_3_1D)(Sep_Conv1D_2)\n",
    "    # Pooling \n",
    "    Sep_pool_Conv1D = GlobalAveragePooling1D()(Sep_Conv1D_3)    \n",
    "\n",
    "    # Gałąź Conv2D\n",
    "        ## Pierwsza warstwa\n",
    "    conv1 = create_conv2d_layer(filters1_2D, activation, l1_1_2D, l2_1_2D)(image_input)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        ## Druga warstwa\n",
    "    conv2 = create_conv2d_layer(filters2_2D, activation, l1_2_2D, l2_2_2D)(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        ## Trzecia warstwa\n",
    "    conv3 = create_conv2d_layer(filters3_2D, activation, l1_3_2D, l2_3_2D)(pool2)\n",
    "    pool3 = MaxPool2D(pool_size=(3, 3))(conv3)  \n",
    "    \n",
    "    flat_cnn = Flatten()(pool3)\n",
    "    \n",
    "    #Dense gałąź SeparableConv1D\n",
    "    dense1 = Dense(units=trial.suggest_int('dense SeparableConv1D', 64, 225), activation=activation)(Sep_pool_Conv1D)\n",
    "    dense1 = Dropout(rate=trial.suggest_float('dropout_rate_1', 0.0, 0.75))(dense1)  \n",
    "\n",
    "    #Dense gałąź Conv2D    \n",
    "    dense2 = Dense(units=trial.suggest_int('dense Conv2D', 64, 225), activation=activation)(flat_cnn)\n",
    "    dense2 = Dropout(rate=trial.suggest_float('dropout_rate_2', 0.0, 0.75))(dense2)\n",
    "    \n",
    "    # Połączenie gałęzi\n",
    "    merged = Concatenate()([dense1, dense2])\n",
    "\n",
    "    merged = Reshape((merged.shape[1], 1))(merged)\n",
    "\n",
    "    # Attention Layer\n",
    "    attention_output = Attention()([merged, merged])\n",
    "    attention_output = Reshape((attention_output.shape[1],))(attention_output)\n",
    "    \n",
    "    #Dense\n",
    "    dense3 = Dense(units=trial.suggest_int('dense przed Output', 36, 256), activation=activation)(attention_output)\n",
    "    dense3 = Dropout(rate=trial.suggest_float('dropout przed Output', 0.0, 0.75))(dense3)\n",
    "\n",
    "    #Output\n",
    "    output = Dense(10, activation='softmax')(dense3)\n",
    "    model = Model(inputs=[image_input, flattened_input], outputs=output)\n",
    "    \n",
    "    #tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    #model.summary()\n",
    "    \n",
    "    # ReduceLROnPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.6, step=0.1),\n",
    "        patience=trial.suggest_int(\"reduce_lr_patience\", 2, 14),\n",
    "        min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "    )\n",
    "\n",
    "    # Optymalizator\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['AdamW', 'Lion'])\n",
    "    optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "    # Początkową wartość learning rate\n",
    "    lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "    optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=8, min_delta=0.0001)\n",
    "    #log_dir = \"logs/\"  # Katalog, w którym będą zapisywane dane TensorBoard\n",
    "    #tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1 )\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    history = model.fit(\n",
    "        [X_train_image, X_train_reduced], \n",
    "        y_train_filters,\n",
    "        epochs=trial.suggest_int('epochs', 20, 90),\n",
    "        batch_size=trial.suggest_int('batch_size', 164, 768),\n",
    "        validation_data=([X_val_image, X_val_reduced], y_val_filters),\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping, reduce_lr],# , tensorboard\n",
    "    )\n",
    "\n",
    "    best_val_accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "    # Ewaluacja modelu na danych testowych\n",
    "    #loss, accuracy = model.evaluate([X_test, X_test], y_test_ENC, verbose=1)\n",
    "    #print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    del model\n",
    "    return best_val_accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=200, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n",
    "print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "#[I 2024-10-25 02:17:14,226] Trial 55 finished with value: 0.9590833187103271 and parameters: {'activation': 'leaky_relu', 'Sep-Conv2d filters1': 23, 'Sep-Conv2d filters2': 58, 'Sep-Conv2d filters3': 85, 'l1_1_1D': 0.0002263330199021373, 'l2_1_1D': 4.2547366301065874e-06, 'l1_2_1D': 1.8831463765106328e-05, 'l2_2_1D': 2.6849906489521574e-06, 'l1_3_1D': 5.376750360962702e-06, 'l2_3_1D': 4.5507289814394546e-05, 'Conv2d filters1': 27, 'Conv2d filters2': 75, 'Conv2d filters3': 66, 'l1_1_2D': 0.03570924628671723, 'l2_1_2D': 0.00013702268815391088, 'l1_2_2D': 0.004180188847206343, 'l2_2_2D': 0.015697748113667306, 'l1_3_2D': 1.1093102758419453e-06, 'l2_3_2D': 2.3039556898400452e-05, 'dense SeparableConv1D': 113, 'dropout_rate_1': 0.25988069757258775, 'dense Conv2D': 55, 'dropout_rate_2': 0.1729409318039701, 'dense przed Output': 213, 'dropout przed Output': 0.692278722477647, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 8, 'reduce_lr_min_lr': 1.2382721420683838e-05, 'optimizer': 'AdamW', 'lr_initial': 0.007665780845145814, 'epochs': 81, 'batch_size': 173}. Best is trial 55 with value: 0.9590833187103271.\n",
    "#[I 2024-10-25 05:23:25,094] Trial 82 finished with value: 0.9622499942779541 and parameters: {'activation': 'leaky_relu', 'Sep-Conv2d filters1': 22, 'Sep-Conv2d filters2': 28, 'Sep-Conv2d filters3': 100, 'l1_1_1D': 6.186293277794836e-05, 'l2_1_1D': 0.0003913398305461908, 'l1_2_1D': 1.5927923474696962e-06, 'l2_2_1D': 7.009282960520817e-06, 'l1_3_1D': 7.676137877709806e-06, 'l2_3_1D': 0.00022663346915615475, 'Conv2d filters1': 24, 'Conv2d filters2': 74, 'Conv2d filters3': 92, 'l1_1_2D': 0.0993169614612255, 'l2_1_2D': 0.0006883072766765725, 'l1_2_2D': 0.0005616491418975441, 'l2_2_2D': 1.0689411586574192e-05, 'l1_3_2D': 4.1278686535916746e-05, 'l2_3_2D': 4.837146281888261e-06, 'dense SeparableConv1D': 155, 'dropout_rate_1': 0.0509776183623675, 'dense Conv2D': 29, 'dropout_rate_2': 0.024717745882008837, 'dense przed Output': 178, 'dropout przed Output': 0.7445169215013963, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 4, 'reduce_lr_min_lr': 6.725637729646196e-06, 'optimizer': 'AdamW', 'lr_initial': 0.01728017154953551, 'epochs': 85, 'batch_size': 191}. Best is trial 82 with value: 0.9622499942779541.\n",
    "#[I 2024-10-25 15:22:24,767] Trial 142 finished with value: 0.9635000228881836 and parameters: {'activation': 'leaky_relu', 'Sep-Conv2d filters1': 22, 'Sep-Conv2d filters2': 28, 'Sep-Conv2d filters3': 107, 'l1_1_1D': 6.120269265952697e-05, 'l2_1_1D': 7.956576920829512e-06, 'l1_2_1D': 1.2494724596667829e-06, 'l2_2_1D': 7.6581362167544e-06, 'l1_3_1D': 0.0039961697402018225, 'l2_3_1D': 0.00021250335195961356, 'Conv2d filters1': 28, 'Conv2d filters2': 72, 'Conv2d filters3': 99, 'l1_1_2D': 0.07544375319709569, 'l2_1_2D': 0.000936229620640654, 'l1_2_2D': 0.001250431002984649, 'l2_2_2D': 0.01324640444949233, 'l1_3_2D': 0.0012955495310992596, 'l2_3_2D': 6.618279886291623e-06, 'dense SeparableConv1D': 151, 'dropout_rate_1': 0.08650725184529029, 'dense Conv2D': 29, 'dropout_rate_2': 0.0500476697674726, 'dense przed Output': 205, 'dropout przed Output': 0.7360088478595872, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 9.207463977521836e-07, 'optimizer': 'AdamW', 'lr_initial': 0.01209682198565378, 'epochs': 88, 'batch_size': 228}. Best is trial 142 with value: 0.9635000228881836.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
