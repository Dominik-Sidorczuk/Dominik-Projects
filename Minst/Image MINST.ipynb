{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cProfile\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import cv2 as cv2\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog, graycomatrix, graycoprops\n",
    "from skimage.filters import gabor_kernel, gabor\n",
    "from skimage import img_as_float32\n",
    "from skimage.transform import resize\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 785 entries, label to 28x28\n",
      "dtypes: int64(785)\n",
      "memory usage: 359.3 MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('mnist_train.csv')\n",
    "test  = pd.read_csv('mnist_test.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_values: (60000, 28, 28)\n",
      "Shape of train_target_encoded: (60000, 10)\n",
      "Shape of test_values: (10000, 28, 28)\n",
      "Shape of test_target_encoded: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "\n",
    "train_target = train['label']\n",
    "test_target = test['label']\n",
    "\n",
    "train_values = train.drop('label', axis=1)\n",
    "test_values = test.drop('label', axis=1)\n",
    "\n",
    "train_values = train_values.values.reshape(-1, 28, 28) \n",
    "test_values = test_values.values.reshape(-1, 28, 28)\n",
    "\n",
    "train_target_encoded = encoder.fit_transform(train_target.values.reshape(-1, 1)).toarray()\n",
    "test_target_encoded = encoder.transform(test_target.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(\"Shape of train_values:\", train_values.shape)\n",
    "print(\"Shape of train_target_encoded:\", train_target_encoded.shape)\n",
    "\n",
    "print(\"Shape of test_values:\", test_values.shape)\n",
    "print(\"Shape of test_target_encoded:\", test_target_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_values\n",
    "X_train = train_values.astype(np.uint8)\n",
    "\n",
    "y_train_ENC = train_target_encoded\n",
    "y_train = np.argmax(y_train_ENC, axis=1)\n",
    "\n",
    "X_test = test_values\n",
    "X_test = test_values.astype(np.uint8)\n",
    "\n",
    "y_test_ENC = test_target_encoded\n",
    "y_test = np.argmax(y_test_ENC, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacja histogramem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sprawdzenie i konwersja typu danych\n",
    "# if X_train.dtype != np.uint8:\n",
    "#     X_train = X_train.astype(np.float32)\n",
    "# if X_test.dtype != np.uint8:\n",
    "#     X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Sprawdzenie i ewentualna korekta kształtu danych\n",
    "if X_train.shape[1:] != (28, 28):\n",
    "    X_train = X_train.reshape(-1, 28, 28)\n",
    "if X_test.shape[1:] != (28, 28):\n",
    "    X_test = X_test.reshape(-1, 28, 28)\n",
    "\n",
    "# Wyrównywanie histogramu\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = cv2.equalizeHist(X_train[i])\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = cv2.equalizeHist(X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powiększanie i wygładzenie obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlarge_and_smooth(image, new_size):\n",
    "    \"\"\"Powiększa obraz za pomocą interpolacji bicubic.\"\"\"\n",
    "    enlarged_image = cv2.resize(image, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "    return enlarged_image\n",
    "\n",
    "# # Sztuczne zwiększenie rozmiaru obrazu \n",
    "# sample_indices = np.random.choice(X_train.shape[0], 5, replace=False)\n",
    "\n",
    "# # Dla każdego wybranego obrazu\n",
    "# for idx in sample_indices:\n",
    "#     # Oryginalny obraz\n",
    "#     original_image = X_train[idx]\n",
    "\n",
    "#     # Powiększony i wygładzony obraz\n",
    "#     enlarged_smoothed_image = enlarge_and_smooth(original_image, new_size=(42, 42))  # Przykładowy nowy rozmiar\n",
    "\n",
    "#     # Wyświetl oryginalny i przekształcony obraz obok siebie\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     sns.heatmap(original_image, cmap=\"viridis\", ax=axes[0])\n",
    "#     axes[0].set_title(f'Oryginalny obraz ({idx})')\n",
    "\n",
    "#     sns.heatmap(enlarged_smoothed_image, cmap=\"viridis\", ax=axes[1])\n",
    "#     axes[1].set_title(f'Powiększony i wygładzony obraz (Lanczos4) ({idx})')\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie modelu wykorzystując SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Baseline accuracy without transformations\n",
    "# X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(X_train, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "# qda_base = QDA()\n",
    "# qda_base.fit(X_train_base.reshape(X_train_base.shape[0], -1), y_train_base)  # Flatten images for QDA\n",
    "# y_pred_qda_base = qda_base.predict(X_val_base.reshape(X_val_base.shape[0], -1))\n",
    "# accuracy_qda_base = accuracy_score(y_val_base, y_pred_qda_base)\n",
    "\n",
    "# svm_base = SVC()\n",
    "# svm_base.fit(X_train_base.reshape(X_train_base.shape[0], -1), y_train_base)  # Flatten images for SVM\n",
    "# y_pred_svm_base = svm_base.predict(X_val_base.reshape(X_val_base.shape[0], -1))\n",
    "# accuracy_svm_base = accuracy_score(y_val_base, y_pred_svm_base)\n",
    "\n",
    "# print(\"Baseline QDA accuracy:\", accuracy_qda_base)\n",
    "# print(\"Baseline SVM accuracy:\", accuracy_svm_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline QDA accuracy: 0.5525\n",
    "\n",
    "\n",
    "Baseline SVM accuracy: 0.9793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deklaracja funkcji dla ekstrakcji cech z obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_hog_parallel(images, orientations=None, pixels_per_cell=None, cells_per_block=None, n_jobs=-1):\n",
    "    \"\"\"Oblicza deskryptory HOG dla obrazu.\"\"\"\n",
    "    \n",
    "     # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if orientations is None:\n",
    "        orientations = 9\n",
    "    if pixels_per_cell is None:\n",
    "        pixels_per_cell = (8, 8)\n",
    "    if cells_per_block is None:\n",
    "        cells_per_block = (2, 2)\n",
    "    \n",
    "    def calculate_hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block):\n",
    "        \n",
    "        image = img_as_float32(image)\n",
    "\n",
    "        fd = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                cells_per_block=cells_per_block, visualize=False, channel_axis=None)\n",
    "\n",
    "        return fd\n",
    "    \n",
    "    return Parallel(n_jobs=n_jobs)(delayed(calculate_hog)(image, orientations, pixels_per_cell, cells_per_block) for image in images)\n",
    "\n",
    "\n",
    "def apply_gabor_filters_parallel(images, thetas=None, sigmas=None, frequencies=None, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Stosuje filtry Gabora do wielu obrazów równolegle.\n",
    "\n",
    "    Argumenty:\n",
    "        images: Lista obrazów wejściowych.\n",
    "        thetas: (Opcjonalnie) Lista kątów orientacji filtrów Gabora (w radianach). Jeśli None, używane są wartości domyślne.\n",
    "        sigmas: (Opcjonalnie) Lista odchyleń standardowych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        frequencies: (Opcjonalnie) Lista częstotliwości przestrzennych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        n_jobs: Liczba rdzeni procesora do wykorzystania podczas zrównoleglania. -1 oznacza użycie wszystkich dostępnych rdzeni.\n",
    "\n",
    "    Zwraca:\n",
    "        Listę tablic NumPy zawierających obrazy przefiltrowane filtrami Gabora, o typie danych float32.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if thetas is None:\n",
    "        thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    if sigmas is None:\n",
    "        sigmas = [1, 3]\n",
    "    if frequencies is None:\n",
    "        frequencies = [0.05, 0.25]\n",
    "\n",
    "    def apply_gabor_filters_single(image):\n",
    "        image = img_as_float32(image)\n",
    "        filtered_images = [gabor(image, frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)[0]\n",
    "                           for theta in thetas for sigma in sigmas for frequency in frequencies]\n",
    "        return np.array(filtered_images, dtype=np.float32)\n",
    "\n",
    "    return Parallel(n_jobs=n_jobs)(delayed(apply_gabor_filters_single)(image) for image in images)\n",
    "\n",
    "\n",
    "def extract_features_HOG_Gabor(X, hog_orientations=None, hog_pixels_per_cell=None, hog_cells_per_block=None, \n",
    "                              gabor_thetas=None, gabor_sigmas=None, gabor_frequencies=None):\n",
    "    \"\"\"Generator zwracający cechy HOG i Gabora dla obrazów.\"\"\"\n",
    "\n",
    "    hog_features_list = calculate_hog_parallel(X, orientations=hog_orientations, pixels_per_cell=hog_pixels_per_cell, cells_per_block=hog_cells_per_block)\n",
    "    gabor_features_list = apply_gabor_filters_parallel(X, thetas=gabor_thetas, sigmas=gabor_sigmas, frequencies=gabor_frequencies)\n",
    "\n",
    "    for hog_features, gabor_features in zip(hog_features_list, gabor_features_list):\n",
    "        \n",
    "        gabor_features_flattened = np.array([img.flatten() for img in gabor_features])\n",
    "\n",
    "        # Dopasowujemy kształt hog_features do gabor_features_flattened\n",
    "        hog_features_reshaped = np.repeat(hog_features.reshape(1, -1), gabor_features_flattened.shape[0], axis=0)\n",
    "\n",
    "        # Łączymy cechy HOG i Gabora w poziomie\n",
    "        concatenated_features = np.hstack((hog_features_reshaped, gabor_features_flattened))\n",
    "\n",
    "        concatenated_features = concatenated_features.flatten()\n",
    "\n",
    "        yield concatenated_features\n",
    "\n",
    "#################################################################################################################################################\n",
    "################################################  Directional filters, ORB    ###################################################################\n",
    "\n",
    "def apply_directional_filters(image, kernel_sizes=[3, 5, 7]):\n",
    "    \"\"\"Stosuje filtry kierunkowe do obrazu.\"\"\"\n",
    "    filtered_images = []\n",
    "    for kernel_size in kernel_sizes:\n",
    "        kernel_horizontal = cv2.getDerivKernels(1, 0, kernel_size, normalize=True)\n",
    "        kernel_vertical = cv2.getDerivKernels(0, 1, kernel_size, normalize=True)\n",
    "        filtered_horizontal = cv2.filter2D(image, cv2.CV_32F, kernel_horizontal[0])\n",
    "        filtered_vertical = cv2.filter2D(image, cv2.CV_32F, kernel_vertical[0])\n",
    "        filtered_images.append(filtered_horizontal.flatten())\n",
    "        filtered_images.append(filtered_vertical.flatten())\n",
    "        if kernel_size % 2 == 0 or kernel_size > 31:\n",
    "            raise ValueError(f\"Nieprawidłowy rozmiar jądra: {kernel_size}. Rozmiar jądra musi być nieparzysty i nie większy niż 31.\")\n",
    "    return np.array(filtered_images).squeeze()\n",
    "\n",
    "def extract_orb_features(image, nfeatures=500):\n",
    "    \"\"\"Wyodrębnia cechy ORB z obrazu.\"\"\"\n",
    "    orb = cv2.ORB_create(nfeatures=nfeatures)  \n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        return np.zeros(nfeatures * 32)  \n",
    "    return descriptors.flatten()\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################################################################################\n",
    "################################################  GLCM, Zernike    ##############################################################################\n",
    "def extract_glcm_features(image, distances=[1], angles=[0], properties=['contrast', 'energy', 'homogeneity', 'correlation']):\n",
    "    \"\"\"Wyodrębnia cechy GLCM z obrazu.\"\"\"\n",
    "    if len(image.shape) > 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    glcm = graycomatrix(image, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    features = []\n",
    "    for prop in properties:\n",
    "        features.extend(graycoprops(glcm, prop).flatten())\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testowanie modelu dla HOG i filtrów Gabora i porównanie wydajności przy zastosowaniu redukcji wymiarów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_evaluate_models(X_train, y_train, use_pca=True):\n",
    "#     \"\"\"\n",
    "#     Trenuje i ocenia modele QDA i SVC, opcjonalnie z redukcją wymiarów PCA.\n",
    "\n",
    "#     Argumenty:\n",
    "#         X_train: Dane treningowe.\n",
    "#         y_train: Etykiety treningowe.\n",
    "#         use_pca: Flaga wskazująca, czy używać PCA (domyślnie True).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Ekstrakcja cech HOG i Gabor za pomocą generatora\n",
    "#     X_train_features = np.array(list(extract_features_HOG_Gabor(X_train)))\n",
    "\n",
    "#     # Podział na zbiór treningowy i walidacyjny\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train_features, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "#     # Redukcja wymiarów (opcjonalnie)\n",
    "#     if use_pca:\n",
    "#         pca_dim = PCA(n_components=50)\n",
    "#         X_train = pca_dim.fit_transform(X_train)\n",
    "#         X_val = pca_dim.transform(X_val)\n",
    "\n",
    "#     # Tworzenie i trenowanie modelu QDA\n",
    "#     qda = QDA()\n",
    "#     qda.fit(X_train, y_train)\n",
    "\n",
    "#     # Tworzenie i trenowanie modelu SVC\n",
    "#     svc = SVC(kernel='rbf')\n",
    "#     svc.fit(X_train, y_train)\n",
    "\n",
    "#     # Ocena modelu\n",
    "#     y_pred_qda = qda.predict(X_val)\n",
    "#     accuracy_qda = accuracy_score(y_val, y_pred_qda)\n",
    "\n",
    "#     y_pred_svc = svc.predict(X_val)\n",
    "#     accuracy_svc = accuracy_score(y_val, y_pred_svc)\n",
    "\n",
    "#     print(\"Dokładność QDA HOG i Gabor\", \"po PCA:\" if use_pca else \":\", accuracy_qda)\n",
    "#     print(\"Dokładność SVC HOG i Gabor\", \"po PCA:\" if use_pca else \":\", accuracy_svc)\n",
    "\n",
    "# # Test z PCA\n",
    "# train_and_evaluate_models(X_train, y_train, use_pca=True)\n",
    "#train_and_evaluate_models(X_train, y_train, use_pca=False)\n",
    "#cProfile.run('train_and_evaluate_models(X_train, y_train, use_pca=True)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokładność QDA HOG i Gabor       : 0.5734\n",
    "\n",
    "\n",
    "Dokładność SVC HOG i Gabor       : 0.987\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dokładność QDA HOG i Gabor po PCA: 0.9701\n",
    "\n",
    "\n",
    "Dokładność SVC HOG i Gabor po PCA: 0.9869\n",
    "\n",
    "Czas obliczeń dla PCA 3min vs 30min bez PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTUNA dla HOG, Gabor, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", module=\"optuna\")\n",
    "\n",
    "\n",
    "# def objective(trial, X_train, y_train):\n",
    "#     \"\"\"\n",
    "#     Funkcja celu dla optymalizacji Optuna. \n",
    "#     Przyjmuje próbę (trial), dane treningowe (X_train) i etykiety (y_train).\n",
    "#     Zwraca dokładność QDA i SVM na danych walidacyjnych po zastosowaniu sekwencji transformacji.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Parametry HOG\n",
    "#     orientations      = trial.suggest_int(\"hog_orientations\", 2, 16)\n",
    "#     pixels_per_cell   = trial.suggest_categorical(\"hog_pixels_per_cell\", [str((2, 2)), str((2, 3)), str((3, 2)), str((3, 3)), str((4, 4)), str((5, 5)),str((6, 6)), str((7, 7))])\n",
    "#     cells_per_block   = trial.suggest_categorical(\"hog_cells_per_block\", [str((1, 1)), str((2, 2)), str((3, 3)), str((4, 4))])\n",
    "\n",
    "#     # Parametry filtrów Gabora\n",
    "#     gabor_thetas      = trial.suggest_categorical(\"gabor_thetas\", ([0],  [np.pi / 14],  [np.pi / 12], [0, np.pi / 14 ], [0, np.pi / 12 ] ))\n",
    "#     gabor_sigmas      = trial.suggest_categorical(\"gabor_sigmas\", ([0.33], [0.66], [1], [0.33 , 1.25], [0.66, 2.35], [1, 3]))\n",
    "#     gabor_frequencies = trial.suggest_categorical(\"gabor_frequencies\", ([0.025], [0.05], [0.125], [0.025, 0.125], [0.05, 0.175], [0.05, 0.25]))\n",
    "        \n",
    "#     #PCA\n",
    "#     PCA_n_components  =  trial.suggest_int(\"PCA_Components\", 4, 228)\n",
    "#     kernel = trial.suggest_categorical(\"kernel_pca_kernel\", ['linear', 'poly', 'rbf', 'sigmoid', 'cosine'])\n",
    "#     gamma = None\n",
    "#     if kernel in ['poly', 'rbf']:\n",
    "#         gamma = trial.suggest_loguniform(\"kernel_pca_gamma\", 0.001, 1)\n",
    "        \n",
    "#     # Powiększanie i wygładzanie\n",
    "#     # X_train_split = np.array([enlarge_and_smooth(img, new_size) for img in X_train])\n",
    "#     # X_val = np.array([enlarge_and_smooth(img, new_size) for img in X_val])\n",
    "    \n",
    "#     # Ekstrakcja cech HOG i Gabor\n",
    "#     X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=orientations, \n",
    "#         hog_pixels_per_cell=eval(pixels_per_cell), # Konwersja stringa na krotkę\n",
    "#         hog_cells_per_block=eval(cells_per_block),  # Konwersja stringa na krotkę\n",
    "#         gabor_thetas=gabor_thetas, \n",
    "#         gabor_sigmas=gabor_sigmas, \n",
    "#         gabor_frequencies=gabor_frequencies\n",
    "#     )))\n",
    "\n",
    "#     # Podział na zbiór treningowy i walidacyjny\n",
    "#     X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train, test_size=0.2, random_state=69)\n",
    "    \n",
    "    \n",
    "#     # Kernel PCA\n",
    "#     kpca = KernelPCA(n_components=PCA_n_components, kernel=kernel, gamma=gamma)\n",
    "#     X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "#     X_val_reduced = kpca.transform(X_val_filters)\n",
    "                \n",
    "#     # SVC\n",
    "#     svc = SVC(kernel='rbf')\n",
    "#     svc.fit(X_train_reduced, y_train_filters)\n",
    "#     y_pred_svm = svc.predict(X_val_reduced)\n",
    "#     accuracy_svm = accuracy_score(y_val_filters, y_pred_svm)\n",
    "\n",
    "#     return accuracy_svm\n",
    "\n",
    "# # Optymalizacji\n",
    "                            \n",
    "# study = optuna.create_study(directions=[\"maximize\"], sampler=optuna.samplers.CmaEsSampler() ) \n",
    "# study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, n_jobs=1)\n",
    "\n",
    "\n",
    "# print(\"Best parameters:\", study.best_params)\n",
    "# print(\"Best QDA accuracy:\", study.best_trial.values[0]) \n",
    "# print(\"Best SVM accuracy:\", study.best_trial.values[1]) \n",
    "\n",
    "# # [I 2024-09-17 05:43:37,964] Trial 13 finished with value: 0.99133 and parameters: {'hog_orientations': 16, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(3, 3)', 'gabor_thetas': [0.2243994752564138], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.025], 'PCA_Components': 209, 'kernel_pca_kernel': 'cosine'}. Best is trial 13 with value: 0.9913333333333333.\n",
    "# # [I 2024-09-17 08:45:58,345] Trial 19 finished with value: 0.99175 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(2, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.025], 'PCA_Components': 132, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 19 with value: 0.99175.\n",
    "# # [I 2024-09-17 10:45:29,605] Trial 23 finished with value: 0.99225 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(3, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 150, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:24:59,082] Trial 28 finished with value: 0.99108 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(1, 1)', 'gabor_thetas': [0, 0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 190, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:54:44,248] Trial 29 finished with value: 0.9915 and parameters: {'hog_orientations': 7, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05], 'PCA_Components': 164, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 18:02:03,444] Trial 37 finished with value: 0.99066 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05, 0.175], 'PCA_Components': 169, 'kernel_pca_kernel': 'rbf', 'kernel_pca_gamma': 0.0032186977490984547}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 19:02:58,946] Trial 39 finished with value: 0.9915 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0, 0.2243994752564138], 'gabor_sigmas': [1, 3], 'gabor_frequencies': [0.125], 'PCA_Components': 148, 'kernel_pca_kernel': 'linear'}. Best is trial 23 with value: 0.99225.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykorzystanie wcześniej otrzymanch hiperparamatrów dla HOG i filtrów Gabora po KPCA w celu stworzenia wektora cech z obrazów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=6, \n",
    "#         hog_pixels_per_cell=eval('(3, 3)'), \n",
    "#         hog_cells_per_block=eval('(2, 2)'),  \n",
    "#         gabor_thetas=[0], \n",
    "#         gabor_sigmas=[1], \n",
    "#         gabor_frequencies=[0.05, 0.25]\n",
    "#     )))\n",
    "\n",
    "\n",
    "# kpca = KernelPCA(n_components=150, kernel='cosine')\n",
    "\n",
    "# X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train_ENC, test_size=0.2, random_state=69)\n",
    "\n",
    "# X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "# X_val_reduced = kpca.transform(X_val_filters)\n",
    "\n",
    "\n",
    "# np.save('X_train_reduced.npy', X_train_reduced)\n",
    "# np.save('X_val_reduced.npy', X_val_reduced)\n",
    "# np.save('y_train_filters.npy', y_train_filters)\n",
    "# np.save('y_val_filters.npy', y_val_filters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pierwszy model na sieci konwolucyjnej dla bazy z HOG i Gabora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LayerNormalization , BatchNormalization, PReLU , Input, ActivityRegularization, GlobalAveragePooling1D, SeparableConv1D \n",
    "# from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "# from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Lion\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import gc\n",
    "\n",
    "\n",
    "\n",
    "# X_train_reduced = np.load('X_train_reduced.npy')\n",
    "# X_val_reduced = np.load('X_val_reduced.npy')\n",
    "# y_train_filters = np.load('y_train_filters.npy')\n",
    "# y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "# X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "# X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "# def calculate_max_pool_size(input_length, prev_pool_size, min_output_size=2):\n",
    "#     calculated_pool_size = input_length // prev_pool_size\n",
    "#     return max(min_output_size, calculated_pool_size)\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     \"\"\"Funkcja celu dla Optuny.\"\"\"\n",
    "#     #os.environ['TF_NUM_INTRAOP_THREADS'] = '1' ## Czy Kers działa na pojedycznym wątku ????\n",
    "#     gc.collect()\n",
    "#     tf.keras.backend.clear_session()\n",
    "#     gc.collect()\n",
    "\n",
    "#     activation_functions = ['relu', 'elu', 'leaky_relu', 'tanh']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "\n",
    "#     # Definiowanie modelu\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(X_train_reduced.shape[1], 1)))\n",
    "\n",
    "#     # Pierwsza warstwa konwolucyjna\n",
    "#     model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_1', 48, 450),\n",
    "#                      kernel_size=trial.suggest_int('kernel_size_1', 2, 16),\n",
    "#                      #activation=activation,\n",
    "#                      padding='same', \n",
    "#                      #dilation_rate=trial.suggest_int('dilation_rate_1', 1, 4)\n",
    "#                      )\n",
    "#               )\n",
    "#     model.add(PReLU())                #$$$$$$$ LAYER przed PRelu\n",
    "#     model.add(LayerNormalization())   ######\n",
    "#     model.add(ActivityRegularization(l1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True), \n",
    "#                                      l2=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "#                                      )\n",
    "#               )\n",
    "#     pool_size_1 = trial.suggest_int('pool_size_1', 2, 10)\n",
    "#     model.add(MaxPooling1D(pool_size=pool_size_1))\n",
    "\n",
    "\n",
    "\n",
    "#     # Druga warstwa konwolucyjna\n",
    "#     input_length_2 = X_train_reduced.shape[1] // pool_size_1\n",
    "#     model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_2', 24, 400),\n",
    "#                      kernel_size=trial.suggest_int('kernel_size_2', 2, 14),\n",
    "#                      padding='same',\n",
    "#                      #activation=activation,\n",
    "#                      #dilation_rate=trial.suggest_int('dilation_rate_2', 1, 4)\n",
    "#                      )\n",
    "#               )\n",
    "#     model.add(PReLU())\n",
    "#     model.add(LayerNormalization())\n",
    "#     model.add(ActivityRegularization(l1=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True), \n",
    "#                                      l2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "#                                      )\n",
    "#               )\n",
    "#     pool_size_2 = trial.suggest_int('pool_size_2', 2, input_length_2 // 2)\n",
    "#     #pool_size_2 = trial.suggest_int('pool_size_2', 2, 10)\n",
    "#     model.add(MaxPooling1D(pool_size=pool_size_2))\n",
    "\n",
    "\n",
    "\n",
    "#     # Trzecia warstwa konwolucyjna\n",
    "#     #input_length_3 = input_length_2 // pool_size_2\n",
    "#     model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_3', 32, 300),\n",
    "#                      kernel_size=trial.suggest_int('kernel_size_3', 2, 12),\n",
    "#                      padding='same',\n",
    "#                      )\n",
    "#               )\n",
    "#     model.add(PReLU())\n",
    "#     model.add(LayerNormalization())\n",
    "#     model.add(Dropout(rate=trial.suggest_float('dropout_rate_3', 0.01, 0.5)\n",
    "#                       )\n",
    "#               )\n",
    "#     #pool_size_3 = trial.suggest_int('pool_size_3', 2, max(2, input_length_3 // 2))\n",
    "#     #pool_size_3 = trial.suggest_int('pool_size_3', 2, 8)\n",
    "#     #model.add(MaxPooling1D(pool_size=pool_size_3))\n",
    "\n",
    "#     # Warstwy Dense\n",
    "#     model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "#     model.add(Dense(units=trial.suggest_int('dense_units_1', 40, 300), activation=activation))\n",
    "#     model.add(Dropout(rate=trial.suggest_float('dropout_rate_4', 0.01, 0.75)))\n",
    "    \n",
    "#     model.add(Dense(units=trial.suggest_int('dense_units_2', 20, 200), activation=activation))\n",
    "#     model.add(Dropout(rate=trial.suggest_float('dropout_rate_5', 0.01, 0.65)))\n",
    "    \n",
    "#     model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#     # Optymalizator\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 1e-5, 1e-1, log=True)\n",
    "#     decay_steps = trial.suggest_int(\"decay_steps\", 10, 150)\n",
    "#     decay_rate = trial.suggest_float(\"decay_rate\", 0.01, 0.99)\n",
    "#     lr_schedule = ExponentialDecay(initial_learning_rate=lr_initial,\n",
    "#                                   decay_steps=decay_steps,\n",
    "#                                   decay_rate=decay_rate)\n",
    "\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['Lion'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_schedule)\n",
    "\n",
    "    \n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='val_accuracy', patience=16, start_from_epoch=10, min_delta = 0.00005)\n",
    "\n",
    "#     # Trenowanie modelu\n",
    "#     model.fit(X_train_reduced, y_train_filters,\n",
    "#               epochs=trial.suggest_int('epochs', 40, 150),\n",
    "#               batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "#               validation_data=(X_val_reduced, y_val_filters),\n",
    "#               verbose=1,\n",
    "#               callbacks=[early_stopping])\n",
    "\n",
    "#     # Ewaluacja modelu\n",
    "#     _, accuracy = model.evaluate(X_val_reduced, y_val_filters, verbose=0)\n",
    "\n",
    "#     del model\n",
    "#     return accuracy\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "# #[I 2024-10-04 05:18:28,377] Trial 26 finished with value: 0.9810000061988831 and parameters: {'activation': 'elu', 'conv1d_filters_1': 347, 'kernel_size_1': 7,  'l1_1': 0.0002141584459916536, 'l2_1': 4.201061472934882e-06, 'pool_size_1': 6, 'conv1d_filters_2': 77, 'kernel_size_2': 14, 'l1_2': 3.203529032832527e-06, 'l2_2': 0.00015793896479891322, 'pool_size_2': 2, 'conv1d_filters_3': 258, 'kernel_size_3': 4, 'dropout_rate_3': 0.3297488074504043, 'dense_units_1': 271, 'dropout_rate_4': 0.5097723961373417, 'dense_units_2': 78, 'dropout_rate_5': 0.15473996425601394, 'lr_initial': 0.0004175922071628326, 'decay_steps': 33, 'decay_rate': 0.9780884395210858, 'optimizer': 'Lion', 'epochs': 123, 'batch_size': 264}. Best is trial 26 with value: 0.9810000061988831.\n",
    "# #[I 2024-10-04 14:58:14,236] Trial 61 finished with value: 0.9797499775886536 and parameters: {'activation': 'elu', 'conv1d_filters_1': 424, 'kernel_size_1': 13, 'l1_1': 0.0005501543495144062, 'l2_1': 0.02794377888054429,   'pool_size_1': 7, 'conv1d_filters_2': 79, 'kernel_size_2': 12, 'l1_2': 1.3699264627909325e-06, 'l2_2': 6.577814668979489e-06, 'pool_size_2': 3, 'conv1d_filters_3': 280, 'kernel_size_3': 3, 'dropout_rate_3': 0.21341586312675653, 'dense_units_1': 292, 'dropout_rate_4': 0.22415600475665626, 'dense_units_2': 106, 'dropout_rate_5': 0.09686035013603865, 'lr_initial': 0.0004430745906851504, 'decay_steps': 22, 'decay_rate': 0.9872365423551039, 'optimizer': 'Lion', 'epochs': 150, 'batch_size': 216}. Best is trial 26 with value: 0.9810000061988831.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sieć CONV2D z zamrożonym PCA dla danych pierwotnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# import tensorflow_probability as tfp\n",
    "# import optuna\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "# X_train_image = X_train_image.astype('float32')\n",
    "# X_val_image = X_val_image.astype('float32')\n",
    "# X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "# X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "# def objective(trial):\n",
    "#     # Przestrzeń hiperparametrów\n",
    "#     num_filters1 = trial.suggest_int(\"num_filters1\", 32, 256)    # Pierwsza warstwa Conv2d\n",
    "#     num_filters2 = trial.suggest_int(\"num_filters2\", 64, 256)    # Druga warstwa Conv2d\n",
    "#     num_filters3 = trial.suggest_int(\"num_filters3\", 128, 256)   # Trzecia warstwa Conv2d\n",
    "#     n_components = trial.suggest_int(\"PCA_components\", 350, 500) # PCA\n",
    "\n",
    "#     l1_1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True)\n",
    "#     l2_1=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     l1_2=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True)\n",
    "#     l2_2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "\n",
    "#     l1_3=trial.suggest_float('l1_3', 1e-6, 1e-1, log=True)\n",
    "#     l2_3=trial.suggest_float('l2_3', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     activation_functions = ['relu', 'gelu', 'leaky_relu', 'silu', 'mish']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "    \n",
    "    \n",
    "#     # Warstwy konwolucyjne 2D\n",
    "#     conv_layers = [\n",
    "#         # Pierwsza warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters1,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_1),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_1),\n",
    "#                                 padding='same',\n",
    "#                                 input_shape=(X_train_image.shape[1:])\n",
    "#                                 ),\n",
    "        \n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2,strides=1,\n",
    "#                                  ),\n",
    "#         # Druga warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters2,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_2),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_2),                                \n",
    "#                                 padding='same',\n",
    "#                                 ),\n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "#         # Trzecia warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters3,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_3),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_3),                                \n",
    "#                                 padding='same'\n",
    "#                                 ),\n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        \n",
    "#         tf.keras.layers.Flatten(),\n",
    "#                 ]\n",
    "\n",
    "#     ### Obliczenia dla zamrożonego PCA na podstawie pierwszej inicjalizacji modelu\n",
    "#     conv_model = keras.models.Sequential(conv_layers)  \n",
    "#     X_train_PCA = conv_model.predict(X_train_image)\n",
    "    \n",
    "#     X_train_PCA = tf.reshape(X_train_PCA, [X_train_PCA.shape[0], -1])\n",
    "\n",
    "#     # Macierz kowariancji\n",
    "#     covariance_matrix = tfp.stats.covariance(X_train_PCA, sample_axis=0, event_axis=-1)\n",
    "#     # Wektory i wartości własne\n",
    "#     eigenvalues, eigenvectors = tf.linalg.eigh(covariance_matrix)\n",
    "#     sorted_indices = tf.argsort(eigenvalues, direction='DESCENDING')\n",
    "#     eigenvalues = tf.gather(eigenvalues, sorted_indices)\n",
    "#     eigenvectors = tf.gather(eigenvectors, sorted_indices, axis=1)\n",
    "    \n",
    "#     #### Opcja druga na SVD, ale wymaga jeszcze zabawy\n",
    "#     # s, u, v = tf.linalg.svd(covariance_matrix)\n",
    "#     # eigenvectors = v\n",
    "#     # eigenvalues = tf.square(s) \n",
    "    \n",
    "#     # Warstwa PCA\n",
    "#     model = keras.models.Sequential(conv_layers + [\n",
    "#         tf.keras.layers.Lambda(lambda x, eigenvectors: tf.matmul(x, eigenvectors), arguments={'eigenvectors': eigenvectors[:, :n_components]})\n",
    "#     ])\n",
    "\n",
    "#     # Output (klasyfikacja)\n",
    "#     model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "#     #model.summary()\n",
    "#     tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    \n",
    "\n",
    "#     # Optymalizator\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "#     # Początkową wartość learning rate\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "#     ## Kompilacja modelu\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=8, start_from_epoch=8, min_delta=0.0001)\n",
    "    \n",
    "#     # ReduceLROnPlateau\n",
    "#     reduce_lr = ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.9, step=0.1),\n",
    "#         patience=trial.suggest_int(\"reduce_lr_patience\", 2, 12),\n",
    "#         min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "#     )\n",
    "    \n",
    "#     # Trenowanie i ocena modelu\n",
    "#     history = model.fit(X_train_image, y_train_image,\n",
    "#                         epochs=trial.suggest_int('epochs', 15, 80),\n",
    "#                         batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "#                         validation_data=(X_val_image, y_val_image),\n",
    "#                         callbacks=[early_stopping, reduce_lr]\n",
    "#                         )\n",
    "    \n",
    "#     accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) \n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "# ### dwa tygodnie więc lepiej aby te wartości stanowiły dobrą podstawę...\n",
    "# #Trial 76 finished with value:  0.9906666874885559 and parameters: {'num_filters1': 194, 'num_filters2': 167, 'num_filters3': 221, 'PCA_components': 383, 'l1_1': 1.4917571854023011e-06, 'l2_1': 0.0003844462705458024,  'l1_2': 0.0011921068803372,     'l2_2': 0.00021948428596782946, 'l1_3': 0.05677951548917238,    'l2_3': 7.714986268695782e-05,  'activation': 'relu', 'optimizer': 'AdamW', 'lr_initial': 0.006869840110235172, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.028747844465778e-06,  'epochs': 51, 'batch_size': 479}.\n",
    "# #Trial 99 finished with value:  0.9911666512489319 and parameters: {'num_filters1': 222, 'num_filters2': 220, 'num_filters3': 202, 'PCA_components': 367, 'l1_1': 4.875844948451946e-06,  'l2_1': 0.00014963179483695857, 'l1_2': 2.3622464527905993e-06, 'l2_2': 0.0007506481920246097,  'l1_3': 0.030678868126237845,   'l2_3': 0.00016514043221099142, 'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.00653453290497133,  'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.8941869280860333e-06, 'epochs': 54, 'batch_size': 331}.\n",
    "# #Trial 108 finished with value: 0.9915000200271606 and parameters: {'num_filters1': 205, 'num_filters2': 206, 'num_filters3': 220, 'PCA_components': 370, 'l1_1': 9.95600828812184e-06,   'l2_1': 4.680137051239973e-05,  'l1_2': 4.9076000996726795e-06, 'l2_2': 8.638060092445147e-05,  'l1_3': 0.0361709248993415,     'l2_3': 0.0007397174884371887,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006269166650314777, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 5.203896150606987e-06,  'epochs': 62, 'batch_size': 239}.\n",
    "# #Trial 121 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 189, 'num_filters2': 200, 'num_filters3': 220, 'PCA_components': 387, 'l1_1': 8.32443244291399e-05,   'l2_1': 5.963850113018811e-05,  'l1_2': 4.8017415472934674e-06, 'l2_2': 3.9423405128005956e-05, 'l1_3': 0.00047971791859799064, 'l2_3': 0.0014900662405900767,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006533479568732247, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 6, 'reduce_lr_min_lr': 7.315164005922789e-06,  'epochs': 58, 'batch_size': 249}.\n",
    "# #Trial 134 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 180, 'num_filters2': 196, 'num_filters3': 234, 'PCA_components': 390, 'l1_1': 4.148854564296224e-05,  'l2_1': 2.3209137723337733e-05, 'l1_2': 0.0009209306936723504,  'l2_2': 1.9816087702628798e-05, 'l1_3': 0.0008993453039409338,  'l2_3': 0.002218879345590759,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007404316715937198, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 3.862546224445297e-07,  'epochs': 61, 'batch_size': 368}.\n",
    "# #Trial 151 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 201, 'num_filters2': 189, 'num_filters3': 229, 'PCA_components': 403, 'l1_1': 6.15892270118e-05,      'l2_1': 1.6056284141459273e-05, 'l1_2': 0.0009167391639733724,  'l2_2': 1.4409315946656477e-05, 'l1_3': 0.0008899963895277467,  'l2_3': 0.005080838638011425,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007378956521980373, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 1.9759552872681682e-07, 'epochs': 63, 'batch_size': 416}.\n",
    "# #Trial 153 finished with value: 0.9905833601951599 and parameters: {'num_filters1': 158, 'num_filters2': 190, 'num_filters3': 229, 'PCA_components': 405, 'l1_1': 3.317865892202326e-05,  'l2_1': 1.715760099034611e-05,  'l1_2': 0.00028475050803045533, 'l2_2': 7.144046413508896e-06,  'l1_3': 0.0010057548651594868,  'l2_3': 0.001544820248650564,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007667044780464368, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 1.6284942102179344e-07, 'epochs': 64, 'batch_size': 420}.\n",
    "# #Trial 189 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 187, 'num_filters2': 194, 'num_filters3': 191, 'PCA_components': 381, 'l1_1': 0.0001222508587314991,  'l2_1': 5.2900939483419044e-05, 'l1_2': 4.050810455502403e-05,  'l2_2': 0.00017281194315162931, 'l1_3': 0.0014341620209873955,  'l2_3': 0.0025511470455798512,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006996548564114207, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 6, 'reduce_lr_min_lr': 9.663271049317225e-07,  'epochs': 60, 'batch_size': 219}.\n",
    "# #Trial 205 finished with value: 0.9904999732971191 and parameters: {'num_filters1': 210, 'num_filters2': 189, 'num_filters3': 198, 'PCA_components': 483, 'l1_1': 8.253178154361272e-05,  'l2_1': 1.7866032331171518e-05, 'l1_2': 0.0008019764970918962,  'l2_2': 1.0316537375038554e-05, 'l1_3': 0.000838618983241391,   'l2_3': 0.008249547665798275,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006570979651986537, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.6114120319881016e-07, 'epochs': 63, 'batch_size': 560}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V2 zmina na dynamic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau \n",
    "\n",
    "X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "X_train_image = X_train_image.astype('float32')\n",
    "X_val_image = X_val_image.astype('float32')\n",
    "X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "def objective(trial):\n",
    "    # Przestrzeń hiperparametrów\n",
    "    num_filters1 = trial.suggest_int(\"num_filters1\", 180, 230)    # Pierwsza warstwa Conv2d\n",
    "    num_filters2 = trial.suggest_int(\"num_filters2\", 180, 230)    # Druga warstwa Conv2d\n",
    "    num_filters3 = trial.suggest_int(\"num_filters3\", 180, 260)    # Trzecia warstwa Conv2d\n",
    "    n_components = trial.suggest_int(\"PCA_components\", 350, 420)  # PCA\n",
    "    \n",
    "    l1_1=trial.suggest_float('l1_1', 1e-6, 5e-3, log=True)\n",
    "    l2_1=trial.suggest_float('l2_1', 5e-5, 5e-3, log=True)\n",
    "    \n",
    "    l1_2=trial.suggest_float('l1_2', 1e-6, 1e-3, log=True)\n",
    "    l2_2=trial.suggest_float('l2_2', 1e-6, 1e-5, log=True)\n",
    "\n",
    "    l1_3=trial.suggest_float('l1_3', 1e-6, 1e-1, log=True)\n",
    "    l2_3=trial.suggest_float('l2_3', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "    activation_functions = ['gelu', 'leaky_relu', 'silu', 'mish']\n",
    "    activation = trial.suggest_categorical('activation', activation_functions)\n",
    "    \n",
    "    \n",
    "    class DynamicPCA(keras.layers.Layer):\n",
    "        def __init__(self, n_components, layer, update_freq=2, threshold=0.5, **kwargs):  \n",
    "            super(DynamicPCA, self).__init__(**kwargs)\n",
    "            self.n_components = n_components\n",
    "            self.threshold = threshold\n",
    "            self.layer = layer  \n",
    "            self.update_freq = update_freq\n",
    "            self.val_loss = None\n",
    "            self.eigenvectors = None\n",
    "            self.epoch_count = 0\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.eigenvectors = self.add_weight(\n",
    "                shape=(input_shape[-1], self.n_components),\n",
    "                initializer=\"zeros\",\n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "        def call(self, inputs):\n",
    "            self.epoch_count += 1\n",
    "            if (self.val_loss is not None and \n",
    "                self.val_loss < self.threshold and \n",
    "                self.epoch_count % self.update_freq == 0):\n",
    "                print(\"DynamicPCA: Aktualizacja wektorów własnych!\")\n",
    "                print(f\"val_loss: {self.val_loss}, threshold: {self.threshold}, epoch_count: {self.epoch_count}, update_freq: {self.update_freq}\")\n",
    "                \n",
    "                # Pobierz wagi bezpośrednio z warstwy\n",
    "                weights = self.layer.get_weights()  \n",
    "                flat_weights = tf.concat([tf.reshape(w, [-1]) for w in weights], axis=0)\n",
    "                # Normalizacja wag\n",
    "                flat_weights = tf.math.l2_normalize(flat_weights)\n",
    "                # Obliczanie nowych wektorów własnych\n",
    "                new_eigenvectors = self.calculate_pca(flat_weights)\n",
    "                # Aktualizacja wag (wektorów własnych)\n",
    "                self.eigenvectors.assign(new_eigenvectors)\n",
    "\n",
    "            # Mnożenie przez wektory własne\n",
    "            output = tf.matmul(inputs, self.eigenvectors)\n",
    "            return output\n",
    "\n",
    "        def calculate_pca(self, inputs):\n",
    "            print(flat_weights.shape)\n",
    "            # Obliczanie macierzy kowariancji\n",
    "            covariance_matrix = tf.linalg.cov(inputs)  \n",
    "            # Wektory i wartości własne\n",
    "            eigenvalues, eigenvectors = tf.linalg.eigh(covariance_matrix)\n",
    "            sorted_indices = tf.argsort(eigenvalues, direction='DESCENDING')\n",
    "            eigenvectors = tf.gather(eigenvectors, sorted_indices, axis=1)\n",
    "            return eigenvectors[:, :self.n_components]\n",
    "\n",
    "    class MyCallback(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            pca_layer = self.model.get_layer('dynamic_pca')\n",
    "            pca_layer.val_loss = logs.get('val_loss')\n",
    "            print(logs.get('val_loss'))\n",
    "\n",
    "    # Definiowanie wejścia\n",
    "    image_input = keras.Input(shape=X_train_image.shape[1:])  \n",
    "\n",
    "    # Warstwy konwolucyjne\n",
    "    conv1 = keras.layers.Conv2D(num_filters1, kernel_size=(2, 2), activation=activation)(image_input)\n",
    "    pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = keras.layers.Conv2D(num_filters2, kernel_size=(2, 2), activation=activation)(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = keras.layers.Conv2D(num_filters3, kernel_size=(2, 2), activation=activation)(pool2)\n",
    "    pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    flat_cnn = keras.layers.Flatten()(pool3)\n",
    "\n",
    "    # Dense 1\n",
    "    dense = keras.layers.Dense(units=trial.suggest_int('dense_units_1', 60, 240), activation=activation)(flat_cnn) \n",
    "    dense = keras.layers.Dropout(rate=trial.suggest_float('dropout_rate_1', 0.01, 0.75))(dense)\n",
    "\n",
    "    # Warstwa DynamicPCA \n",
    "    pca_output = DynamicPCA(n_components=n_components, layer=conv3, name='dynamic_pca', threshold=1.25)(flat_cnn)  \n",
    "    \n",
    "    merged = keras.layers.Concatenate()([dense, pca_output])\n",
    "\n",
    "    dense2 = keras.layers.Dense(units=trial.suggest_int('dense_units_2', 60, 240), activation=activation)(merged)  \n",
    "    dense2 = keras.layers.Dropout(rate=trial.suggest_float('dropout_rate_2', 0.01, 0.75))(dense2)\n",
    "\n",
    "    # Output (klasyfikacja)\n",
    "    outputs = keras.layers.Dense(10, activation='softmax')(dense2)\n",
    "\n",
    "    # Utworzenie modelu\n",
    "    model = keras.Model(inputs=image_input, outputs=outputs)\n",
    "\n",
    "    # Utworzenie modelu\n",
    "    model = keras.Model(inputs=image_input, outputs=outputs)\n",
    "    #model.summary()\n",
    "    tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    \n",
    "\n",
    "    # Optymalizator\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "    optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "    # Początkową wartość learning rate\n",
    "    lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "    optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "    ## Kompilacja modelu\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=10, start_from_epoch=8, min_delta=0.0001)\n",
    "    \n",
    "    # ReduceLROnPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.5, step=0.1),\n",
    "        patience=trial.suggest_int(\"reduce_lr_patience\", 4, 8),\n",
    "        min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-5, log=True)\n",
    "    )\n",
    "    \n",
    "    # Trenowanie i ocena modelu\n",
    "    history = model.fit(X_train_image, y_train_image,\n",
    "                        epochs=trial.suggest_int('epochs', 20, 80),\n",
    "                        batch_size=trial.suggest_int('batch_size', 164, 768),\n",
    "                        validation_data=(X_val_image, y_val_image),\n",
    "                        validation_freq=1,\n",
    "                        callbacks=[early_stopping, reduce_lr, MyCallback()]\n",
    "                        )\n",
    "    \n",
    "    accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) \n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n",
    "print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model bazujący na dwóch potokach danych. Surowych przechodzących przez Conv2d, oraz LSTM dla danych z HOG i Gabor. Wykorzystanie mechanizmu atencji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-17 19:24:20,369] A new study created in memory with name: no-name-0e4e5524-c635-47f6-b8b0-910b3a76366a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged shape: <KerasTensor shape=(None, 512), dtype=float32, sparse=False, name=keras_tensor_16>\n",
      "Reshape shape: <KerasTensor shape=(None, 512), dtype=float32, sparse=False, name=keras_tensor_16>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">13,920</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">115</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activity_regulariz… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ActivityRegulariz…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">67</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,768</span> │ activity_regular… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,973</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activity_regulariz… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">67</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ActivityRegulariz…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">331,776</span> │ activity_regular… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">165,120</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activity_regulariz… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ActivityRegulariz…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activity_regular… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">113</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">57,969</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">113</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,140</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m58\u001b[0m)   │     \u001b[38;5;34m13,920\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m,    │        \u001b[38;5;34m115\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m23\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activity_regulariz… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m58\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mActivityRegulariz…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m23\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m67\u001b[0m)   │     \u001b[38;5;34m33,768\u001b[0m │ activity_regular… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │     \u001b[38;5;34m14,973\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m161\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activity_regulariz… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m67\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mActivityRegulariz…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m161\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m331,776\u001b[0m │ activity_regular… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m165,120\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activity_regulariz… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mActivityRegulariz…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ activity_regular… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m113\u001b[0m)       │     \u001b[38;5;34m57,969\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m113\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │      \u001b[38;5;34m1,140\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">618,781</span> (2.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m618,781\u001b[0m (2.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">618,781</span> (2.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m618,781\u001b[0m (2.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/57\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - categorical_accuracy: 0.2584 - loss: 22.8660 - val_categorical_accuracy: 0.8390 - val_loss: 0.7060 - learning_rate: 0.0171\n",
      "Epoch 2/57\n",
      "\u001b[1m14/58\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 2s/step - categorical_accuracy: 0.8356 - loss: 0.6978"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GlobalAveragePooling1D, Permute, Flatten, Reshape, Conv2D, MaxPooling2D, Concatenate, Attention, Dense, LayerNormalization, ActivityRegularization, Dropout, GlobalAveragePooling2D, AveragePooling2D, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPool1D, GlobalMaxPool2D, MaxPool2D \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import gc\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "X_train_reduced = np.load('X_train_reduced.npy')\n",
    "X_val_reduced = np.load('X_val_reduced.npy')\n",
    "y_train_filters = np.load('y_train_filters.npy')\n",
    "y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "X_train_image = X_train_image.astype('float32')\n",
    "X_val_image = X_val_image.astype('float32')\n",
    "X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "\n",
    "# # Sprawdzenie poprawności danych wejściowych\n",
    "# print(\"Kształt X_train_image:\", X_train_image.shape)\n",
    "# print(\"Kształt X_val_image:\", X_val_image.shape)\n",
    "# print(\"Kształt X_train_reduced:\", X_train_reduced.shape)\n",
    "# print(\"Kształt X_val_reduced:\", X_val_reduced.shape)\n",
    "\n",
    "# print(\"Kształt y_train_filters:\", y_train_filters.shape)\n",
    "# print(\"Kształt y_val_filters:\", y_val_filters.shape)\n",
    "# print(\"Kształt y_train_image:\", y_train_image.shape)\n",
    "# print(\"Kształt y_val_image:\", y_val_image.shape)\n",
    "\n",
    "# # Sprawdzenie typów danych\n",
    "# print(\"Typ danych X_train_image:\", X_train_image.dtype)\n",
    "# print(\"Typ danych X_val_image:\", X_val_image.dtype)\n",
    "# print(\"Typ danych X_train_reduced:\", X_train_reduced.dtype)\n",
    "# print(\"Typ danych X_val_reduced:\", X_val_reduced.dtype)\n",
    "# print(\"Typ danych y_train_filters:\", y_train_filters.dtype)\n",
    "# print(\"Typ danych y_val_filters:\", y_val_filters.dtype)\n",
    "# print(\"Typ danych y_train_image:\", y_train_image.dtype)\n",
    "# print(\"Typ danych y_val_image:\", y_val_image.dtype)\n",
    "\n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    # Dane wejściowe\n",
    "    image_input = Input(shape=X_train_image.shape[1:])\n",
    "    flattened_input = Input(shape=X_train_reduced.shape[1:])\n",
    "\n",
    "    ### Hiperparamatry dla LSTM\n",
    "    units1=trial.suggest_int('units_1', 12, 92)\n",
    "    units2=trial.suggest_int('units_2', 24, 164)\n",
    "    units3=trial.suggest_int('units_3', 48, 256)\n",
    "\n",
    "    l1_1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True)\n",
    "    l2_1=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "    l1_2=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True)\n",
    "    l2_2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "\n",
    "    l1_3=trial.suggest_float('l1_3', 1e-6, 1e-1, log=True)\n",
    "    l2_3=trial.suggest_float('l2_3', 1e-6, 1e-1, log=True)\n",
    "    ### Hiperparamatry dla Conv2d\n",
    "    filters1=trial.suggest_int('conv2d_filters_1', 12, 92)\n",
    "    filters2=trial.suggest_int('conv2d_filters_2', 24, 164)\n",
    "    filters3=trial.suggest_int('conv2d_filters_3', 48, 256)\n",
    "\n",
    "    ###################\n",
    "    # Gałąź LSTM\n",
    "        ## Pierwsza warstwa\n",
    "    lstm1 = LSTM(units1,\n",
    "                return_sequences=True,\n",
    "                activation = 'silu'\n",
    "                )(flattened_input)\n",
    "    \n",
    "    #lstm1 = LayerNormalization()(lstm1)\n",
    "    lstm1 = ActivityRegularization(\n",
    "                                    l1 = l1_1,\n",
    "                                    l2 = l2_1\n",
    "                                   )(lstm1)\n",
    "        ## Druga warstwa\n",
    "    lstm2 = LSTM(units2,\n",
    "                return_sequences=True,\n",
    "                activation = 'silu'\n",
    "                )(lstm1)\n",
    "    #lstm2 = LayerNormalization()(lstm2)\n",
    "    lstm2 = ActivityRegularization(\n",
    "                                    l1 = l1_2,\n",
    "                                    l2 = l2_2 \n",
    "                                  )(lstm2)\n",
    "        ## Trzecia warstwa\n",
    "    lstm3 = LSTM(256,\n",
    "                return_sequences=True,\n",
    "                activation = 'silu'\n",
    "                )(lstm2)\n",
    "    #lstm3 = LayerNormalization()(lstm3)\n",
    "    lstm3 = ActivityRegularization(\n",
    "                                    l1 = l1_3,\n",
    "                                    l2 = l2_3 \n",
    "                                  )(lstm3)\n",
    "    ## Pooling \n",
    "    pool_lstm = GlobalAveragePooling1D(\n",
    "                                       )(lstm3) \n",
    "    #flat_lstm = Flatten()(pool_lstm)\n",
    "    #print(\"flat_lstm shape:\", flat_lstm)\n",
    "\n",
    "    # Gałąź Conv2D\n",
    "        ## Pierwsza warstwa\n",
    "    conv1 = Conv2D(filters1,\n",
    "                   kernel_size=(2, 2),\n",
    "                   activation='silu',\n",
    "                   \n",
    "                   )(image_input)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        ## Druga warstwa\n",
    "    conv2 = Conv2D(filters2,\n",
    "                   kernel_size=(2, 2),\n",
    "                   activation='silu'\n",
    "                   )(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        ## Trzecia warstwa\n",
    "    conv3 = Conv2D(256,\n",
    "                kernel_size=(2, 2),\n",
    "                activation='silu'\n",
    "                )(pool2)\n",
    "    \n",
    "    pool2 = MaxPool2D(pool_size=(3, 3)\n",
    "                        )(conv3)  \n",
    "    \n",
    "    flat_cnn = Flatten()(pool2)\n",
    "    #print(\"flat_cnn shape:\", flat_cnn)\n",
    "    \n",
    "    # Połączenie gałęzi\n",
    "    merged = Concatenate()([pool_lstm, flat_cnn])\n",
    "    print(\"merged shape:\", merged)\n",
    "    #merged = Reshape((merged.shape[1], 1))(merged)\n",
    "    print(\"Reshape shape:\", merged)\n",
    "    \n",
    "    # Attention Layer\n",
    "    #attention_output = Attention()([merged, merged])\n",
    "    #attention_output = Reshape((attention_output.shape[1],))(attention_output)\n",
    "    \n",
    "    # Dense\n",
    "    dense = Dense(units=trial.suggest_int('dense_units_1', 60, 120), activation='silu')(merged)\n",
    "    dense = Dropout(rate=trial.suggest_float('dropout_rate_4', 0.01, 0.75))(dense)\n",
    "\n",
    "    #Output\n",
    "    output = Dense(10, activation='softmax')(dense)\n",
    "    model = Model(inputs=[image_input, flattened_input], outputs=output)\n",
    "    \n",
    "    tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    model.summary()\n",
    "    \n",
    "    # ReduceLROnPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.9, step=0.1),\n",
    "        patience=trial.suggest_int(\"reduce_lr_patience\", 2, 12),\n",
    "        min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "    )\n",
    "\n",
    "    # Optymalizator\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "    optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "    # Początkową wartość learning rate\n",
    "    lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "    optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=8, start_from_epoch=8, min_delta=0.0001)\n",
    "\n",
    "    #tensorboard = TensorBoard(log_dir='./logs')\n",
    "    \n",
    "    # Trenowanie modelu\n",
    "    model.fit(\n",
    "        [X_train_image, X_train_reduced], \n",
    "        y_train_filters,\n",
    "        epochs=trial.suggest_int('epochs', 25, 80),\n",
    "        batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "        validation_data=([X_val_image, X_val_reduced], y_val_filters),\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, reduce_lr] # , tensorboard\n",
    "    )\n",
    "\n",
    "    # Ewaluacja modelu\n",
    "    _, accuracy = model.evaluate([X_val_image, X_val_reduced], y_val_filters, verbose=0)\n",
    "    \n",
    "    del model\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n",
    "print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "#[I 2024-10-07 12:16:47,663] Trial 12 finished with value: 0.9774166941642761 and parameters: {'units_1': 90, 'units_2': 75, 'units_3': 105, 'l1_1': 0.0007371006413692947, 'l2_1': 1.1174857882131448e-06, 'l1_2': 0.011311214197055712, 'l2_2': 1.2735301285541541e-05, 'l1_3': 4.1983950029163696e-05, 'l2_3': 0.0007493927477204213, 'conv2d_filters_1': 72, 'conv2d_filters_2': 24, 'conv2d_filters_3': 177, 'dense_units_1': 107, 'dropout_rate_4': 0.20974716998604293, 'reduce_lr_factor': 0.8, 'reduce_lr_patience': 9, 'reduce_lr_min_lr': 1.739344697796337e-05, 'optimizer': 'AdamW', 'lr_initial': 0.005715433495262424, 'epochs': 61, 'batch_size': 1838}. Best is trial 12 with value: 0.9774166941642761.\n",
    "#[I 2024-10-07 13:19:36,166] Trial 13 finished with value: 0.9700000286102295 and parameters: {'units_1': 92, 'units_2': 161, 'units_3': 117, 'l1_1': 0.0008839107419295125, 'l2_1': 1.154687911591668e-06, 'l1_2': 0.006313924135568636, 'l2_2': 6.780100126072228e-06, 'l1_3': 0.003353325847224238, 'l2_3': 5.4329911159199266e-05, 'conv2d_filters_1': 71, 'conv2d_filters_2': 56, 'conv2d_filters_3': 239, 'dense_units_1': 118, 'dropout_rate_4': 0.24924760076872401, 'reduce_lr_factor': 0.9, 'reduce_lr_patience': 2, 'reduce_lr_min_lr': 1.2116979296707788e-05, 'optimizer': 'AdamW', 'lr_initial': 0.005150547498928084, 'epochs': 62, 'batch_size': 1918}. Best is trial 12 with value: 0.9774166941642761.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
