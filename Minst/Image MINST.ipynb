{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 785 entries, label to 28x28\n",
      "dtypes: int64(785)\n",
      "memory usage: 359.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 785 entries, label to 28x28\n",
      "dtypes: int64(785)\n",
      "memory usage: 59.9 MB\n",
      "Shape of X_train: (60000, 28, 28)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of X_test: (10000, 28, 28)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# --- Przetwarzanie danych ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Wizualizacja ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Preprocessing ---\n",
    "from scipy.signal import correlate2d\n",
    "from scipy.stats import bartlett, levene, fligner\n",
    "from skimage.metrics import structural_similarity, variation_of_information, normalized_root_mse\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    silhouette_samples,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    fowlkes_mallows_score,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "import umap\n",
    "\n",
    "# --- Modelowanie ---\n",
    "import optuna\n",
    "import optunahub\n",
    "from sklearn.cluster import FeatureAgglomeration, KMeans\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# --- Ewaluacja ---\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# --- Przetwarzanie obrazów ---\n",
    "import cv2 as cv2\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog, graycomatrix, graycoprops\n",
    "from skimage.filters import gabor_kernel, gabor\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import img_as_float32\n",
    "from skimage.transform import resize\n",
    "\n",
    "# --- Zrównoleglanie ---\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# --- Inne ---\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import sqlite3\n",
    "\n",
    "# --- Sklearn ---\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# --- Skimage ---\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from skimage.filters import gaussian, unsharp_mask\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "# --- Dodatkowe importy ---\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_csv('mnist_train.csv')\n",
    "test  = pd.read_csv('mnist_test.csv')\n",
    "train.info()\n",
    "test.info()\n",
    "\n",
    "# Podział na cechy i etykiety\n",
    "X_train = train.drop('label', axis=1).values.reshape(-1, 28, 28)\n",
    "y_train = train['label'].values\n",
    "\n",
    "X_test = test.drop('label', axis=1).values.reshape(-1, 28, 28)\n",
    "y_test = test['label'].values\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('mnist_train.csv')\n",
    "test  = pd.read_csv('mnist_test.csv')\n",
    "train.info()\n",
    "test.info()\n",
    "\n",
    "# Podział na cechy i etykiety\n",
    "X_train = train.drop('label', axis=1).values.reshape(-1, 28, 28)\n",
    "y_train = train['label'].values\n",
    "\n",
    "X_test = test.drop('label', axis=1).values.reshape(-1, 28, 28)\n",
    "y_test = test['label'].values\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Wizualizacja podstawowych cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do wizualizacji statystyk\n",
    "def visualize_statistics(X, label):\n",
    "    \"\"\"Oblicza i wizualizuje statystyki dla danej klasy.\"\"\"\n",
    "\n",
    "    # Obliczenie statystyk\n",
    "    mean_pixels = np.mean(X, axis=0)\n",
    "    std_pixels = np.std(X, axis=0)\n",
    "    median_pixels = np.median(X, axis=0)\n",
    "    q25_pixels = np.quantile(X, 0.25, axis=0)\n",
    "    q75_pixels = np.quantile(X, 0.75, axis=0)\n",
    "    min_pixels = np.mean(np.percentile(X, [0, 1.5], axis=0), axis=0)\n",
    "    max_pixels = np.mean(np.percentile(X, [98.5, 100], axis=0), axis=0)\n",
    "\n",
    "    # Wizualizacja wszystkich statystyk na jednym wykresie\n",
    "    _, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "\n",
    "    sns.heatmap(mean_pixels, cmap='gray', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f\"Średnia\")\n",
    "\n",
    "    sns.heatmap(std_pixels, cmap='viridis', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(f\"Odchylenie std.\")\n",
    "\n",
    "    axes[0, 2].imshow(median_pixels, cmap='gray')\n",
    "    axes[0, 2].set_title(\"Mediana\")\n",
    "    axes[0, 2].axis('off')\n",
    "\n",
    "    axes[0, 3].imshow(q25_pixels, cmap='gray')\n",
    "    axes[0, 3].set_title(\"Kwartyl 25%\")\n",
    "    axes[0, 3].axis('off')\n",
    "\n",
    "    axes[1, 0].imshow(q75_pixels, cmap='gray')\n",
    "    axes[1, 0].set_title(\"Kwartyl 75%\")\n",
    "    axes[1, 0].axis('off')\n",
    "\n",
    "    axes[1, 1].imshow(min_pixels, cmap='gray')\n",
    "    axes[1, 1].set_title(\"1.5% minimum\")\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    axes[1, 2].imshow(max_pixels, cmap='gray')\n",
    "    axes[1, 2].set_title(\"1.5% maksimum\")\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "    # Histogram wartości pikseli\n",
    "    axes[1, 3].hist(X.flatten(), bins=50)\n",
    "    axes[1, 3].set_title(f\"Histogram\\nwartości pikseli\")\n",
    "    axes[1, 3].set_xlabel('Wartość piksela')\n",
    "    axes[1, 3].set_ylabel('Liczba wystąpień')\n",
    "\n",
    "    plt.suptitle(f\"Etykieta: {label}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Grupowanie po etykietach w zbiorze treningowym\n",
    "for label in range(10):\n",
    "    X_train_group = X_train[y_train == label]\n",
    "    X_test_group = X_test[y_test == label]\n",
    "\n",
    "    # Wizualizacja statystyk dla zbioru treningowego\n",
    "    print(\"Zbiór treningowy:\")\n",
    "    visualize_statistics(X_train_group, label)\n",
    "\n",
    "    # Wizualizacja statystyk dla zbioru testowego\n",
    "    print(\"\\nZbiór testowy:\")\n",
    "    visualize_statistics(X_test_group, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porownaj_srednie_obrazow(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Porównuje średnie obrazy dla każdej klasy w zbiorze treningowym i testowym \n",
    "    za pomocą trzech metryk: SSIM, NRMSE i Variation of Information.\n",
    "\n",
    "    Args:\n",
    "        X_train: Zbiór treningowy obrazów (numpy array).\n",
    "        y_train: Etykiety dla zbioru treningowego (numpy array).\n",
    "        X_test: Zbiór testowy obrazów (numpy array).\n",
    "        y_test: Etykiety dla zbioru testowego (numpy array).\n",
    "\n",
    "    Returns:\n",
    "        Słownik zawierający wyniki dla każdej metryki i klasy.\n",
    "    \"\"\"\n",
    "\n",
    "    wyniki = {}\n",
    "    for label in range(10):\n",
    "        X_train_group = X_train[y_train == label]\n",
    "        X_test_group = X_test[y_test == label]\n",
    "\n",
    "        # Obliczenie średnich obrazów\n",
    "        sredni_obraz_train = np.mean(X_train_group, axis=0)\n",
    "        sredni_obraz_test = np.mean(X_test_group, axis=0)\n",
    "\n",
    "        # Obliczenie metryk\n",
    "        ssim = structural_similarity(sredni_obraz_train, sredni_obraz_test, data_range=sredni_obraz_train.max() - sredni_obraz_train.min())\n",
    "        nrmse = normalized_root_mse(sredni_obraz_train, sredni_obraz_test)\n",
    "\n",
    "        # Variation of Information wymaga obrazów typu int\n",
    "        vi = variation_of_information(sredni_obraz_train.astype(int), sredni_obraz_test.astype(int)) \n",
    "\n",
    "        wyniki[label] = {\n",
    "            'ssim': ssim,\n",
    "            'nrmse': nrmse,\n",
    "            'vi': vi,\n",
    "        }\n",
    "\n",
    "    return wyniki\n",
    "\n",
    "\n",
    "wyniki = porownaj_srednie_obrazow(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# SSIM\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x=list(wyniki.keys()), y=[wyniki[label]['ssim'] for label in wyniki])\n",
    "plt.title('SSIM')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Wartość SSIM')\n",
    "\n",
    "# NRMSE\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x=list(wyniki.keys()), y=[wyniki[label]['nrmse'] for label in wyniki])\n",
    "plt.title('NRMSE')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Wartość NRMSE')\n",
    "\n",
    "# VI\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x=list(wyniki.keys()), y=[wyniki[label]['vi'][0] for label in wyniki])\n",
    "plt.title('Variation of Information')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Wartość VI')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja wartości i wektrów własnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_eigen(X, label):\n",
    "    \"\"\"Analizuje i wizualizuje wartości i wektory własne dla danej klasy.\"\"\"\n",
    "\n",
    "    # Spłaszczenie danych\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    # Obliczenie macierzy kowariancji\n",
    "    cov_matrix = np.cov(X_flat, rowvar=False)\n",
    "\n",
    "    # Obliczenie wartości i wektorów własnych\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # Sortowanie wartości własnych\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # Utworzenie siatki dla wykresów\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    gs = fig.add_gridspec(2, 5, height_ratios=[3, 1])  \n",
    "\n",
    "    # Wykres wartości własnych z podwójnymi osiami Y\n",
    "    ax1 = fig.add_subplot(gs[0, :])  \n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Numer wartości własnej')\n",
    "    ax1.set_ylabel('Wartość własna', color=color)\n",
    "    ax1.plot(eigenvalues, marker='o', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True)\n",
    "    ax1.set_title(f\"Wektory własne i wartości własne dla etykiety {label}\")\n",
    "\n",
    "    # Utworzenie drugiej osi Y (prawa oś Y)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Wykres wartości własnych od 200\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Wartość własna (powiększenie)', color=color)\n",
    "    ax2.plot(range(200, len(eigenvalues)), eigenvalues[200:], marker='o', color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Wizualizacja wektorów własnych w drugim wierszu\n",
    "    num_eigenvectors_to_display = 5\n",
    "    for i in range(num_eigenvectors_to_display):\n",
    "        ax = fig.add_subplot(gs[1, i])\n",
    "        ax.imshow(eigenvectors[:, i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(f\"Wektor własny {i+1}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iteracja po etykietach (zakładając, że masz X_train i y_train)\n",
    "for label in range(10):\n",
    "    X_train_group = X_train[y_train == label]\n",
    "    analyze_eigen(X_train_group, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metody składowych głównych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(X_train, y_train, method_name):\n",
    "    \"\"\"\n",
    "    Redukuje wymiarowość danych MNIST za pomocą określonej metody i ocenia jej jakość.\n",
    "\n",
    "    Args:\n",
    "      X_train: Dane treningowe MNIST.\n",
    "      y_train: Etykiety danych treningowych.\n",
    "      method_name: Nazwa metody redukcji wymiarów.\n",
    "\n",
    "    Returns:\n",
    "      Krotka (nazwa metody, dane po redukcji wymiarów, czas obliczeń).\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method_name == \"PCA\":\n",
    "        method = PCA(n_components=2)\n",
    "        X_transformed = method.fit_transform(X_train_flat)\n",
    "        \n",
    "    elif method_name == \"KernelPCA\":\n",
    "        method = KernelPCA(n_components=2, kernel=\"cosine\")\n",
    "        X_transformed = method.fit_transform(X_train_flat)\n",
    "        \n",
    "    elif method_name == \"t-SNE\":\n",
    "        method = TSNE(\n",
    "            n_components=2, random_state=42, perplexity=30)   # 3>method='exact',\n",
    "        X_transformed = method.fit_transform(X_train_flat)\n",
    "\n",
    "    elif method_name == \"UMAP\": \n",
    "        method = umap.UMAP(n_components=2, random_state=42)\n",
    "        X_transformed = method.fit_transform(X_train_flat)\n",
    "        \n",
    "    elif method_name == \"LLE\":\n",
    "        method = LocallyLinearEmbedding(n_components=2)\n",
    "        X_transformed = method.fit_transform(X_train_flat)\n",
    "        \n",
    "    elif method_name == \"LDA\":\n",
    "        method = LinearDiscriminantAnalysis(n_components=2)\n",
    "        X_transformed = method.fit_transform(X_train_flat, y_train)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Nieznana metoda: {method_name}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return method_name, X_transformed, time_taken\n",
    "\n",
    "\n",
    "def evaluate_and_visualize(X_train, y_train, method_name, X_transformed, time_taken):\n",
    "    \"\"\"\n",
    "    Ocenia jakość redukcji wymiarów i wizualizuje wyniki dla pojedynczej metody.\n",
    "\n",
    "    Args:\n",
    "      X_train: Dane treningowe MNIST.\n",
    "      y_train: Etykiety danych treningowych.\n",
    "      method_name: Nazwa metody redukcji wymiarów.\n",
    "      X_transformed: Dane po redukcji wymiarów.\n",
    "      time_taken: Czas obliczeń.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Wizualizacja ---\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(\n",
    "        X_transformed[:, 0],\n",
    "        X_transformed[:, 1],\n",
    "        c=y_train,\n",
    "        cmap=\"jet\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    plt.title(f\"{method_name} MNIST\")\n",
    "    plt.xlabel(f\"{method_name} wymiar 1\")\n",
    "    plt.ylabel(f\"{method_name} wymiar 2\")\n",
    "    legend = plt.legend(\n",
    "        *scatter.legend_elements(), loc=\"lower left\", title=\"Cyfry\"\n",
    "    )\n",
    "    plt.gca().add_artist(legend)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Ocena jakości redukcji wymiarów ---\n",
    "    if method_name != \"LDA\":\n",
    "        silhouette_score_value = silhouette_score(X_transformed, y_train ,sample_size=10000)\n",
    "        print(f\"{method_name}: silhouette score = {silhouette_score_value:.3f}\")\n",
    "        gc.collect()\n",
    "    print(f\"{method_name}: czas obliczeń = {time_taken:.2f} s\")\n",
    "    \n",
    "    # homogeneity and completeness and V-Measure\n",
    "    # Klasteryzacja \n",
    "    kmeans = KMeans(n_clusters=10, random_state=42) \n",
    "    y_pred = kmeans.fit_predict(X_transformed)\n",
    "\n",
    "    # homogeneity_completeness_v_measure\n",
    "    homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(\n",
    "        y_train, y_pred\n",
    "    )\n",
    "\n",
    "    print(f\"{method_name}: homogeneity = {homogeneity:.3f}\")\n",
    "    print(f\"{method_name}: completeness = {completeness:.3f}\")\n",
    "    print(f\"{method_name}: v_measure = {v_measure:.3f}\")\n",
    "\n",
    "    print(f\"{method_name}: czas obliczeń = {time_taken:.2f} s\")\n",
    "\n",
    "# Uruchomienie metod redukcji wymiarów sekwencyjnie\n",
    "method_names = [\"PCA\", \"KernelPCA\", \"LLE\", \"LDA\", \"t-SNE\", \"UMAP\",]\n",
    "\n",
    "for method_name in method_names:\n",
    "    method_name, X_transformed, time_taken = reduce_dimensionality(\n",
    "        X_train, y_train, method_name\n",
    "    )\n",
    "    evaluate_and_visualize(\n",
    "        X_train, y_train, method_name, X_transformed, time_taken\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyznacznik macierzy kowariancji, oraz korelacji przestrzennej dla średnich obrazów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_covariance(X):\n",
    "    \"\"\"\n",
    "    Analizuje macierz kowariancji danych, obliczając wariancję, wyznacznik i liczbę warunkową.\n",
    "    \"\"\"\n",
    "\n",
    "    variance = np.var(X, axis=0)  # Wariancja dla każdej cechy\n",
    "    total_variance = np.sum(variance)  # Całkowita wariancja\n",
    "\n",
    "    cov_matrix = np.cov(X, rowvar=False)\n",
    "    determinant = np.linalg.det(cov_matrix)\n",
    "    eigenvalues = np.linalg.eigvalsh(cov_matrix)\n",
    "\n",
    "    # Usunięcie wartości własnych mniejszych niż 1e-8\n",
    "    eigenvalues = eigenvalues[eigenvalues > 1e-8]\n",
    "\n",
    "    condition_number = np.max(eigenvalues) / np.min(eigenvalues)\n",
    "\n",
    "    return total_variance, determinant, condition_number\n",
    "\n",
    "# Obliczenie średniego obrazu dla każdej klasy\n",
    "mean_images = []\n",
    "for label in range(10):\n",
    "    mean_images.append(np.mean(X_train[y_train == label], axis=0))\n",
    "\n",
    "# Obliczenie macierzy korelacji między średnimi obrazami\n",
    "corr_matrix = np.corrcoef(np.array(mean_images).reshape(10, -1))\n",
    "\n",
    "# Zastosowanie UMAP do całego zbioru danych\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_train_reduced = reducer.fit_transform(X_train_flat)\n",
    "\n",
    "# Wizualizacja macierzy korelacji i wyników analizy kowariancji\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Macierz korelacji\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", ax=axes[0])\n",
    "axes[0].set_title('Macierz korelacji między średnimi obrazami dla danych nie przerobionych')\n",
    "axes[0].set_xticks(np.arange(10) + 0.5)\n",
    "axes[0].set_xticklabels(range(10))\n",
    "axes[0].set_yticks(np.arange(10) + 0.5)\n",
    "axes[0].set_yticklabels(range(10))\n",
    "\n",
    "# Wyniki analizy kowariancji\n",
    "determinants = []\n",
    "condition_numbers = []\n",
    "variances = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for label in range(10):\n",
    "    X_train_group_reduced = X_train_reduced[y_train == label]\n",
    "    variance, determinant, condition_number = analyze_covariance(X_train_group_reduced)\n",
    "    determinants.append(determinant)\n",
    "    condition_numbers.append(condition_number)\n",
    "    variances.append(variance)\n",
    "\n",
    "    # Obliczenie silhouette score dla każdej klasy\n",
    "    silhouette_avg = np.mean(silhouette_samples(X_train_reduced, y_train))\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# --- KMeans clustering ---\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_train_reduced)\n",
    "\n",
    "# --- Metrics ---\n",
    "y_true = y_train \n",
    "\n",
    "# Adjusted Rand Score\n",
    "ars = adjusted_rand_score(y_true, y_pred)\n",
    "\n",
    "# Adjusted Mutual Information Score\n",
    "amis = adjusted_mutual_info_score(y_true, y_pred)\n",
    "\n",
    "# Fowlkes-Mallows Score\n",
    "fms = fowlkes_mallows_score(y_true, y_pred)\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "dbi = davies_bouldin_score(X_train_reduced, y_pred)\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "chi = calinski_harabasz_score(X_train_reduced, y_pred)\n",
    "\n",
    "print(\"UMAP: silhouette score = {:.3f}\".format(silhouette_score(X_train_reduced, y_train)))\n",
    "print(\"UMAP: adjusted_rand_score = {:.3f}\".format(ars))\n",
    "print(\"UMAP: adjusted_mutual_info_score = {:.3f}\".format(amis))\n",
    "print(\"UMAP: fowlkes_mallows_score = {:.3f}\".format(fms))\n",
    "print(\"UMAP: Davies-Bouldin index = {:.3f}\".format(dbi))\n",
    "print(\"UMAP: Calinski-Harabasz index = {:.3f}\".format(chi))\n",
    "print(\"UMAP: czas obliczeń = {:.2f} s\".format(36.71))\n",
    "\n",
    "# Wykresy\n",
    "axes[1].bar(range(10), determinants, label='Wyznacznik')\n",
    "axes[1].set_xticks(np.arange(10))\n",
    "axes[1].set_ylabel('Wyznacznik')\n",
    "axes[1].legend(loc='upper left')\n",
    "\n",
    "ax2 = axes[1].twinx()\n",
    "ax2.plot(range(10), condition_numbers, color='red', marker='o', label='Liczba warunkowa')\n",
    "ax2.set_ylabel('Liczba warunkowa')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "axes[1].set_title('Wyznacznik i liczba warunkowa macierzy kowariancji po UMAP')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testy statystyczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_covariance(X, method='bartlett'):\n",
    "    \"\"\"\n",
    "    Analizuje kowariancję w zbiorze danych X za pomocą wybranego testu.\n",
    "\n",
    "    Dostępne testy:\n",
    "    - 'bartlett': Test Bartletta \n",
    "    - 'levene': Test Levene'a\n",
    "    - 'fligner': Test Flignera-Killeena\n",
    "\n",
    "    Args:\n",
    "        X: Zbiór danych (numpy array).\n",
    "        method: Nazwa testu do wykonania.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Statystyka testu i p-wartość.\n",
    "    \"\"\"\n",
    "\n",
    "    if method == 'bartlett':\n",
    "        statistic, p_value = bartlett(*X.T)\n",
    "    elif method == 'levene':\n",
    "        statistic, p_value = levene(*X.T)\n",
    "    elif method == 'fligner':\n",
    "        statistic, p_value = fligner(*X.T)\n",
    "    else:\n",
    "        raise ValueError(\"Nieznana metoda. Dostępne metody: 'bartlett', 'levene', 'fligner'\")\n",
    "\n",
    "    return statistic, p_value\n",
    "\n",
    "# Zastosowanie UMAP do całego zbioru danych\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_train_reduced = reducer.fit_transform(X_train_flat)  \n",
    "\n",
    "grouped = pd.DataFrame(X_train_reduced).groupby(y_train)\n",
    "\n",
    "results = [] \n",
    "\n",
    "# Iteracja po grupach i wykonanie testów\n",
    "for label, group in grouped:\n",
    "    for method in ['bartlett', 'levene', 'fligner']:\n",
    "        statistic, p_value = analyze_covariance(group.values, method=method)\n",
    "\n",
    "        # Dodanie wyników do listy jako słownik\n",
    "        results.append({\n",
    "            'label': label,\n",
    "            'method': method,\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "pivot_table = results_df.pivot(index='label', columns='method', values=['statistic', 'p_value'])\n",
    "\n",
    "display(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (60000, 28, 28)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of y_train_ENC: (60000, 10)\n",
      "Shape of X_test: (10000, 28, 28)\n",
      "Shape of y_test: (10000,)\n",
      "Shape of y_test_ENC: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reshape \n",
    "X_train = X_train.reshape(-1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 28, 28)\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "y_train_ENC = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_ENC = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_train_ENC:\", y_train_ENC.shape)\n",
    "\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"Shape of y_test_ENC:\", y_test_ENC.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacja histogramem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cv2.normalize(X_train, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_64F)\n",
    "X_train = X_train.astype(np.uint8)\n",
    "# Wyrównywanie histogramu\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = cv2.equalizeHist(X_train[i])\n",
    "\n",
    "X_test = cv2.normalize(X_test, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_64F)\n",
    "X_test = X_test.astype(np.uint8)\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = cv2.equalizeHist(X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powiększanie i wygładzenie obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlarge_and_smooth(image, new_size):\n",
    "    \"\"\"Powiększa obraz za pomocą interpolacji bicubic.\"\"\"\n",
    "    enlarged_image = cv2.resize(image, new_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "    return enlarged_image\n",
    "\n",
    "# # Sztuczne zwiększenie rozmiaru obrazu \n",
    "# sample_indices = np.random.choice(X_train.shape[0], 5, replace=False)\n",
    "\n",
    "# # Dla każdego wybranego obrazu\n",
    "# for idx in sample_indices:\n",
    "#     # Oryginalny obraz\n",
    "#     original_image = X_train[idx]\n",
    "\n",
    "#     # Powiększony i wygładzony obraz\n",
    "#     enlarged_smoothed_image = enlarge_and_smooth(original_image, new_size=(42, 42))\n",
    "\n",
    "#     # Wyświetl oryginalny i przekształcony obraz obok siebie\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     print(\"Shape of original_image:\", original_image.shape)\n",
    "#     print(\"Shape of enlarged_smoothed_image:\", enlarged_smoothed_image.shape)\n",
    "\n",
    "#     sns.heatmap(original_image, cmap=\"viridis\", ax=axes[0]) \n",
    "#     axes[0].set_title(f'Oryginalny obraz ({idx})')\n",
    "\n",
    "#     sns.heatmap(enlarged_smoothed_image, cmap=\"viridis\", ax=axes[1]) \n",
    "#     axes[1].set_title(f'Powiększony i wygładzony obraz (Lanczos4) ({idx})')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train --- Test split na danych treningowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar X_train_base: (48000, 28, 28)\n",
      "Rozmiar X_val_base: (12000, 28, 28)\n",
      "Rozmiar y_train_base: (48000,)\n",
      "Rozmiar y_val_base: (12000,)\n",
      "Rozmiar X_train_Upsc: (48000, 42, 42)\n",
      "Rozmiar X_val_Upsc: (12000, 42, 42)\n",
      "Rozmiar y_train_Upsc: (48000,)\n",
      "Rozmiar y_val_Upsc: (12000,)\n"
     ]
    }
   ],
   "source": [
    "# Baseline accuracy without transformations\n",
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=69)\n",
    "print(\"Rozmiar X_train_base:\", X_train_base.shape)\n",
    "print(\"Rozmiar X_val_base:\", X_val_base.shape)\n",
    "print(\"Rozmiar y_train_base:\", y_train_base.shape)\n",
    "print(\"Rozmiar y_val_base:\", y_val_base.shape)\n",
    "\n",
    "\n",
    "smoothed_X_train = np.array([enlarge_and_smooth(image, new_size=(42, 42)) for image in X_train])\n",
    "# Baseline accuracy upscaling\n",
    "X_train_Upsc, X_val_Upsc, y_train_Upsc, y_val_Upsc = train_test_split(\n",
    "    smoothed_X_train, y_train, test_size=0.2, random_state=69)\n",
    "print(\"Rozmiar X_train_Upsc:\", X_train_Upsc.shape)\n",
    "print(\"Rozmiar X_val_Upsc:\", X_val_Upsc.shape)\n",
    "print(\"Rozmiar y_train_Upsc:\", y_train_Upsc.shape)\n",
    "print(\"Rozmiar y_val_Upsc:\", y_val_Upsc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa Ewaulacji modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    return X.reshape(X.shape[0], -1)\n",
    "\n",
    "flatten_transformer = FunctionTransformer(flatten)\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Klasa do ewaluacji modeli klasyfikacji. Umożliwia\n",
    "    przekazanie pipeline'u z dowolnymi operacjami\n",
    "    przetwarzania danych. Oblicza i wyświetla wybrane\n",
    "    metryki, macierz pomyłek oraz raport klasyfikacji.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metrics=None, display_labels=None):\n",
    "        \"\"\"\n",
    "        Inicjalizuje ModelEvaluator z domyślnymi lub\n",
    "        zdefiniowanymi metrykami i etykietami.\n",
    "\n",
    "        Args:\n",
    "            metrics: Lista metryk do obliczenia.\n",
    "                    Domyślnie: [\"accuracy\", \"precision\", \"recall\", \"f1\"].\n",
    "                    Dostępne metryki: \"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\".\n",
    "            display_labels: Etykiety klas do wyświetlenia w macierzy pomyłek.\n",
    "                            Domyślnie None (używane unikalne etykiety z y_true).\n",
    "        \"\"\"\n",
    "        self.metrics = metrics if metrics is not None else [\n",
    "            \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
    "        ]\n",
    "        self.display_labels = display_labels\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        calculate_proba=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ewaluuje pipeline na podanych danych. Pipeline\n",
    "        może zawierać dowolne operacje przetwarzania\n",
    "        danych.\n",
    "\n",
    "        Args:\n",
    "            pipeline: Pipeline do ewaluacji.\n",
    "            X_train: Dane treningowe.\n",
    "            y_train: Etykiety treningowe.\n",
    "            X_val: Dane walidacyjne.\n",
    "            y_val: Etykiety walidacyjne.\n",
    "            calculate_proba: Flaga wskazująca, czy obliczyć\n",
    "                            prawdopodobieństwa (wymagane dla\n",
    "                            roc_auc, domyślnie False).\n",
    "            verbose: Flaga wskazująca, czy wyświetlać\n",
    "                     szczegółowe wyniki (macierz pomyłek,\n",
    "                     raport klasyfikacji, krzywa ROC,\n",
    "                     domyślnie True).\n",
    "\n",
    "        Returns:\n",
    "            dict: Słownik z wynikami ewaluacji.\n",
    "        \"\"\"\n",
    "        # Sprawdzenie, czy w pipeline jest redukcja wymiarów\n",
    "        for i, step in enumerate(pipeline.steps):\n",
    "            try:\n",
    "                X_temp = step[1].fit_transform(X_train)  \n",
    "                if X_temp.shape[1] < X_train.shape[1]:  \n",
    "                    pipeline.steps.insert(i, (\"flatten\", flatten_transformer)) \n",
    "                    break\n",
    "            except:\n",
    "                pass  \n",
    "\n",
    "        # Jeśli nie znaleziono redukcji wymiarów,  spłaszczenie na początku\n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                (\"flatten\", flatten_transformer),\n",
    "                *pipeline.steps\n",
    "            ])\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "\n",
    "        y_prob = None\n",
    "        if calculate_proba:\n",
    "            try:\n",
    "                y_prob = pipeline.predict_proba(X_val)[:, 1]\n",
    "            except AttributeError:\n",
    "                print(\n",
    "                    \"Model nie posiada metody predict_proba. \"\n",
    "                    \"Metryka AUC nie zostanie obliczona.\"\n",
    "                )\n",
    "\n",
    "        eval_results = self._calculate_metrics(y_val, y_pred, y_prob, verbose, calculate_proba)\n",
    "        self._display_results(pipeline, y_val, y_pred, eval_results, verbose, calculate_proba)  \n",
    "        return eval_results\n",
    "\n",
    "    def _calculate_metrics(self, y_true, y_pred, y_prob=None, verbose=True, calculate_proba=False):  \n",
    "        \"\"\"Oblicza wybrane metryki.\"\"\"\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        if not calculate_proba:  # Obliczanie tylko podstawowych metryk, gdy calculate_proba=False\n",
    "            results[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "            results[\"precision\"] = precision_score(y_true, y_pred, average=\"macro\")\n",
    "            results[\"recall\"] = recall_score(y_true, y_pred, average=\"macro\")\n",
    "            results[\"f1\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        else:  # Obliczanie wszystkich metryk, gdy calculate_proba=True\n",
    "            for metric in self.metrics:\n",
    "                #print(metric)\n",
    "                if metric == \"accuracy\":\n",
    "                    results[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "                elif metric == \"precision\":\n",
    "                    results[\"precision\"] = precision_score(y_true, y_pred, average=\"macro\")\n",
    "                elif metric == \"recall\":\n",
    "                    results[\"recall\"] = recall_score(y_true, y_pred, average=\"macro\")\n",
    "                elif metric == \"f1\":\n",
    "                    results[\"f1\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
    "                elif metric == \"roc_auc\" and y_prob is not None:\n",
    "                    print('Obliczam roc_auc')\n",
    "                    results[\"roc_auc\"] = self._calculate_roc_auc(y_true, y_prob)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _calculate_roc_auc(self, y_true, y_prob):\n",
    "        \"\"\"Oblicza AUC i rysuje krzywą ROC.\"\"\"\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob, pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        return roc_auc, fpr, tpr\n",
    "\n",
    "    def _display_results(self, model, y_true, y_pred, results, verbose=True, calculate_proba=False):\n",
    "        \"\"\"Wyświetla metryki, macierz pomyłek i raport klasyfikacji.\"\"\"\n",
    "        if isinstance(model, Pipeline):\n",
    "            model_name = model.steps[-1][1].__class__.__name__\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "\n",
    "        # Tworzenie DataFrame'a z wynikami\n",
    "        metrics_df = pd.DataFrame([results])\n",
    "\n",
    "        if verbose:\n",
    "            display(metrics_df)\n",
    "            display(f\"\\nWyniki dla {model_name}:\")\n",
    "\n",
    "            # Wyświetlanie macierzy pomyłek i raportu klasyfikacji tylko gdy calculate_proba=True\n",
    "            if calculate_proba:  \n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                disp = ConfusionMatrixDisplay(\n",
    "                    confusion_matrix=confusion_matrix(y_true, y_pred),\n",
    "                    display_labels=self.display_labels\n",
    "                    if self.display_labels\n",
    "                    else np.unique(y_true),\n",
    "                )\n",
    "                disp.plot(ax=ax, cmap=plt.cm.Blues)\n",
    "                plt.title(\"Macierz pomyłek\")\n",
    "                plt.show()\n",
    "\n",
    "                report = classification_report(y_true, y_pred, output_dict=True)\n",
    "                report_df = pd.DataFrame(report).transpose()\n",
    "                display(report_df)\n",
    "\n",
    "            if \"roc_auc\" in results:\n",
    "                roc_auc, fpr, tpr = results[\"roc_auc\"]\n",
    "                self._plot_roc_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "        return metrics_df\n",
    "\n",
    "    def _plot_roc_curve(self, fpr, tpr, roc_auc):\n",
    "        \"\"\"Rysuje krzywą ROC.\"\"\"\n",
    "        plt.figure()\n",
    "        plt.plot(\n",
    "            fpr,\n",
    "            tpr,\n",
    "            color=\"darkorange\",\n",
    "            lw=2,\n",
    "            label=f\"ROC curve (area = {roc_auc:.2f})\",\n",
    "        )\n",
    "        plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie modelu wykorzystując VotingClassifier dla danych orginalnaych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie estymatorów dla VotingClassifier\n",
    "qda = QDA(reg_param=0.1)\n",
    "hgb = HistGradientBoostingClassifier()\n",
    "et = ExtraTreesClassifier()\n",
    "\n",
    "# Utworzenie VotingClassifier\n",
    "VotClass = VotingClassifier(\n",
    "    estimators=[('qda', qda), ('hgb', hgb), ('et', et)],\n",
    "    voting='soft' \n",
    ")\n",
    "# Lista modeli do testowania\n",
    "models = [VotClass]\n",
    "\n",
    "# Tworzenie instancji klasy ModelEvaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Trenowanie i ewaluacja modeli\n",
    "all_metrics = {}\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # Utworzenie pipeline'u (możesz dodać kroki przetwarzania danych)\n",
    "    pipeline = Pipeline([\n",
    "        (\"classifier\", model)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Trenowanie i ewaluacja pipeline'u\n",
    "    eval_results = evaluator.evaluate(\n",
    "        \n",
    "        pipeline, X_train_base, y_train_base, X_val_base, y_val_base, calculate_proba=True, verbose=True\n",
    "    )\n",
    "    all_metrics[model_name] = eval_results\n",
    "\n",
    "# Tworzenie DataFrame z metrykami\n",
    "metrics_df = pd.DataFrame(all_metrics, index=list(eval_results.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie modelu wykorzystując VotingClassifier dla danych upscalowanych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie estymatorów dla VotingClassifier\n",
    "qda = QDA(reg_param=0.1)\n",
    "hgb = HistGradientBoostingClassifier()\n",
    "et = ExtraTreesClassifier()\n",
    "\n",
    "# Utworzenie VotingClassifier\n",
    "VotClass = VotingClassifier(\n",
    "    estimators=[('qda', qda), ('hgb', hgb), ('et', et)],\n",
    "    voting='soft' \n",
    ")\n",
    "# Lista modeli do testowania\n",
    "models = [VotClass]\n",
    "\n",
    "# Tworzenie instancji klasy ModelEvaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Trenowanie i ewaluacja modeli\n",
    "all_metrics = {}\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # Utworzenie pipeline'u (możesz dodać kroki przetwarzania danych)\n",
    "    pipeline = Pipeline([(\"classifier\", model)])\n",
    "\n",
    "    # Trenowanie i ewaluacja pipeline'u\n",
    "    eval_results = evaluator.evaluate(\n",
    "        pipeline, X_train_Upsc, y_train_Upsc, X_val_Upsc, y_val_Upsc, calculate_proba=True, verbose=True\n",
    "    )\n",
    "    all_metrics[model_name] = eval_results\n",
    "\n",
    "# Tworzenie DataFrame z metrykami\n",
    "metrics_df = pd.DataFrame(all_metrics, index=list(eval_results.keys()))\n",
    "\n",
    "# Wyświetlanie DataFrame z metrykami\n",
    "# print(\"\\nMetryki w formie tabeli:\")\n",
    "# display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detekcja outlinerów oraz agumentacja danych - Testy Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import AffineTransform, warp, ProjectiveTransform\n",
    "from skimage.filters import gaussian, unsharp_mask, laplace\n",
    "from skimage.morphology import erosion, closing, opening,  disk\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "OPTUNA_module = optunahub.load_module(package=\"samplers/auto_sampler\")\n",
    "\n",
    "\n",
    "# X_train_Upsc, X_val_Upsc, y_train_Upsc, y_val_Upsc = train_test_split(\n",
    "#     smoothed_X_train, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "\n",
    "# Tworzenie instancji klasy ModelEvaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "def callback(study, trial):\n",
    "    print(f\"Trial {trial.number}:\")\n",
    "    print(f\"  Wartość: {trial.value}\")\n",
    "    print(f\"  Parametry: {trial.params}\")\n",
    "\n",
    "def create_voting_classifier():\n",
    "    \"\"\"Tworzy VotingClassifier z trzema estymatorami.\"\"\"\n",
    "\n",
    "    estimators = [\n",
    "        ('qda', QDA(reg_param=0.1)),\n",
    "        ('hgb', HistGradientBoostingClassifier()),\n",
    "        ('et', ExtraTreesClassifier())\n",
    "    ]\n",
    "    return VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "def augment_data(X_train, y_train, n_neighbors_lof, contamination_lof, \n",
    "                    contamination_forest):\n",
    "    \"\"\"\n",
    "    Augmentuje dane treningowe poprzez:\n",
    "        1. Wykrywanie outlierów za pomocą LOF i IsolationForest.\n",
    "        2. Augmentację danych dla outlierów (przesunięcie i obrót).\n",
    "        3. Redukcję wymiarów UMAP dla całego zestawu danych.\n",
    "        4. Wizualizację outlierów i ich augmentacji.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Redukcja wymiarów UMAP dla LOF\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    X_train_reduced = reducer.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "\n",
    "    # 2. Wykrywanie outlierów (LOF)\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors_lof, contamination=contamination_lof)\n",
    "    y_pred_lof = lof.fit_predict(X_train_reduced)\n",
    "\n",
    "    # 3. Isolation Forest na surowych danych\n",
    "    iso_forest = IsolationForest(contamination=contamination_forest)\n",
    "    y_pred_iso = iso_forest.fit_predict(X_train.reshape(X_train.shape[0], -1))\n",
    "\n",
    "    # 4. Połączenie wyników LOF i IsolationForest\n",
    "    y_pred_combined = (y_pred_lof + y_pred_iso) / 2\n",
    "    outlier_indices = np.where(y_pred_combined < 0)[0]\n",
    "    print(f\"Liczba outlinerów: {len(outlier_indices)}\")\n",
    "    \n",
    "    \n",
    "    # 5. Augmentacja danych dla outlierów\n",
    "    X_train_augmented = []\n",
    "    y_train_augmented = []\n",
    "\n",
    "    if len(outlier_indices) > 0:\n",
    "        n_outliers_to_show = min(12, len(outlier_indices))  # Ograniczenie liczby wyświetlanych outlierów\n",
    "        for i, idx in enumerate(outlier_indices[:n_outliers_to_show]):\n",
    "            image = X_train[idx]\n",
    "            label = y_train[idx]  # Pobranie etykiety dla outliera\n",
    "\n",
    "            # 1. Dodanie paddingu (czarne tło)\n",
    "            padded_image = np.pad(image, pad_width=35, mode='constant')  # Zwiększony padding\n",
    "            for j in range(4):  # Zmienna j zdefiniowana wewnątrz pętli\n",
    "                # 2. Transformacja afiniczna\n",
    "                transform = AffineTransform(scale=(np.random.uniform(0.97, 1.04), np.random.uniform(0.97, 1.04)),\n",
    "                                            rotation=np.deg2rad(np.random.randint(-20, 21)),\n",
    "                                            shear=np.deg2rad(np.random.randint(-5, 6))\n",
    "                                            )  \n",
    "                transformed_image = warp(padded_image, transform, mode='wrap', preserve_range=True)\n",
    "                \n",
    "                # # 3. Wyizolowanie cyfry po transformacji \n",
    "                thresh = transformed_image > 20  \n",
    "                contours = find_contours(thresh, 0.75)\n",
    "\n",
    "                # --- Obsługa błędów ---\n",
    "                try:\n",
    "                    cnt = contours[0]  # Wybór pierwszego konturu\n",
    "\n",
    "                    # Wyznaczenie bounding box\n",
    "                    minr = int(np.min(cnt[:, 0]))\n",
    "                    minc = int(np.min(cnt[:, 1]))\n",
    "                    maxr = int(np.max(cnt[:, 0]))\n",
    "                    maxc = int(np.max(cnt[:, 1]))\n",
    "\n",
    "                    # Sprawdzenie, czy bounding box ma poprawny kształt\n",
    "                    if minr == maxr or minc == maxc:\n",
    "                        print(\"Nieprawidłowy bounding box - pomijanie augmentacji dla tego obrazu.\")\n",
    "                        continue\n",
    "\n",
    "                except IndexError:\n",
    "                    print(\"Nie znaleziono konturu - pomijanie augmentacji dla tego obrazu.\")\n",
    "                    continue\n",
    "\n",
    "                # 4. Wycięcie cyfry na podstawie bounding box\n",
    "                isolated_digit = transformed_image[minr:maxr, minc:maxc]\n",
    "            \n",
    "                # --- Erozja ---\n",
    "                if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                    isolated_digit = erosion(isolated_digit, disk(1))\n",
    "                    \n",
    "                # --- Otwarcie  ---\n",
    "                if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                    isolated_digit = opening(isolated_digit, disk(1))\n",
    "                    \n",
    "                # --- Zamknięcie \n",
    "                if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                    isolated_digit = closing(isolated_digit, disk(1))\n",
    "\n",
    "                              \n",
    "                \n",
    "                # 5. Wycentrowanie cyfry w obrazie 42x42\n",
    "                new_image = np.zeros((42, 42), dtype=np.uint8)\n",
    "                row_offset = max(0, (42 - (maxr - minr)) // 2)\n",
    "                col_offset = max(0, (42 - (maxc - minc)) // 2)\n",
    "                new_image[row_offset:row_offset + maxr - minr, col_offset:col_offset + maxc - minc] = isolated_digit\n",
    "\n",
    "                isolated_digit = new_image\n",
    "\n",
    "                # 6. Dodanie do listy\n",
    "                X_train_augmented.append(isolated_digit)  # Dodaj augmentowany obraz\n",
    "                y_train_augmented.append(label)  # Dodaj etykietę\n",
    "\n",
    "    X_train_augmented = np.array(X_train_augmented)\n",
    "    y_train_augmented = np.array(y_train_augmented)\n",
    "\n",
    "    # Połączenie danych augmentowanych z oryginalnymi danymi\n",
    "    X_train_augmented = np.concatenate((X_train, X_train_augmented), axis=0)\n",
    "    y_train_augmented = np.concatenate((y_train, y_train_augmented), axis=0)\n",
    "\n",
    "    return X_train_augmented, y_train_augmented, outlier_indices, X_train_reduced\n",
    "    \n",
    "    \n",
    "def objective(trial, X, y, use_umap):\n",
    "    \"\"\"Funkcja celu dla Optuna z podziałami train-test split dla każdego zestawu hiperparametrów.\"\"\"\n",
    "\n",
    "    # Optymalizacja hiperparametrów\n",
    "    n_neighbors_lof = trial.suggest_int(\"n_neighbors_lof\", 20, 800)\n",
    "    contamination_lof = trial.suggest_float(\"contamination_lof\", 0.005, 0.35, step=0.0025)\n",
    "    contamination_forest = trial.suggest_float(\"contamination_forest\", 0.005, 0.35, step=0.0025)\n",
    "\n",
    "    if use_umap:\n",
    "        n_components_umap = trial.suggest_int(\"n_components_umap\", 2, 120)\n",
    "        n_neighbors_UMAP = trial.suggest_int(\"n_neighbors\", 2, 2400)\n",
    "        min_dist = trial.suggest_uniform(\"min_dist\", 0.0, 0.75)\n",
    "        spread = trial.suggest_float(\"spread\", 0.75, 2.0)\n",
    "        local_connectivity = trial.suggest_float(\"local_connectivity\", 1.0, 10.0)\n",
    "        set_op_mix_ratio = trial.suggest_float(\"set_op_mix_ratio\", 0.0, 1.0)\n",
    "\n",
    "    scores = []\n",
    "    for _ in range(3):\n",
    "        # Podział danych na zbiór treningowy i walidacyjny\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "            \n",
    "        # Augmentacja danych\n",
    "        X_train_augmented, y_train_augmented, _, _ = augment_data(\n",
    "            X_train, y_train, n_neighbors_lof, contamination_lof, contamination_forest\n",
    "        )\n",
    "\n",
    "        # Spłaszczenie danych przed ewaluacją\n",
    "        X_train_augmented = X_train_augmented.reshape(X_train_augmented.shape[0], -1)\n",
    "        X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "        # Ewaluacja modelu po augmentacji\n",
    "        model = create_voting_classifier()\n",
    "\n",
    "        if use_umap:\n",
    "            pipeline = Pipeline([\n",
    "                (\"reducer\", umap.UMAP(\n",
    "                    n_components=n_components_umap,\n",
    "                    n_neighbors=n_neighbors_UMAP,\n",
    "                    set_op_mix_ratio=set_op_mix_ratio,\n",
    "                    min_dist=min_dist,\n",
    "                    spread=spread,\n",
    "                    local_connectivity=local_connectivity\n",
    "                )),\n",
    "                (\"classifier\", model)\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                (\"classifier\", model)\n",
    "            ])\n",
    "\n",
    "        eval_results = evaluator.evaluate(\n",
    "            pipeline, X_train_augmented, y_train_augmented, X_val, y_val,\n",
    "            calculate_proba=False, verbose=False\n",
    "        )\n",
    "\n",
    "        # Zapisanie wyniku dla bieżącego podziału\n",
    "        scores.append(sum([eval_results[metric] * 0.25 for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\"]]))\n",
    "\n",
    "    # Zwrócenie uśrednionego wyniku z trzech podziałów\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Utworzenie storage ---\n",
    "storage = optuna.storages.RDBStorage(\"sqlite:///study_Auto_sampler_New_1.db\")\n",
    "\n",
    "# --- Utworzenie lub wczytanie study ---\n",
    "study_name_umap = \"Test_1_umap\"\n",
    "study_name_no_umap = \"Test_1_no_umap\"\n",
    "\n",
    "try:\n",
    "    study_umap = optuna.load_study(study_name=study_name_umap, storage=storage)\n",
    "    print(\"Wczytano study z UMAP z bazy danych.\")\n",
    "except KeyError:\n",
    "    study_umap = optuna.create_study(direction=\"maximize\", storage=storage, study_name=study_name_umap)\n",
    "    print(\"Utworzono nowe study z UMAP.\")\n",
    "\n",
    "try:\n",
    "    study_no_umap = optuna.load_study(study_name=study_name_no_umap, storage=storage)\n",
    "    print(\"Wczytano study bez UMAP z bazy danych.\")\n",
    "except KeyError:\n",
    "    study_no_umap = optuna.create_study(direction=\"maximize\", storage=storage, study_name=study_name_no_umap, sampler=OPTUNA_module.AutoSampler())\n",
    "    print(\"Utworzono nowe study bez UMAP.\")\n",
    "\n",
    "# Optymalizacja z UMAP\n",
    "#study_umap.optimize(lambda trial: objective(trial, X_val_Upsc, y_val_Upsc, use_umap=True), n_trials=150,  n_jobs=4, gc_after_trial=True)\n",
    "\n",
    "# Optymalizacja bez UMAP\n",
    "study_no_umap.optimize(lambda trial: objective(trial, smoothed_X_train, y_train, use_umap=False), n_trials=250,  n_jobs=-1, gc_after_trial=True)\n",
    "\n",
    "# Wyświetlenie najlepszych hiperparametrów dla agumentacji bez UMAP\n",
    "print(\"Najlepsze parametry:\", study_no_umap.best_params)\n",
    "print(\"Najlepszy wynik:\", study_no_umap.best_value)\n",
    "\n",
    "# Wyświetlenie najlepszych hiperparametrów dla agumentacji z UMAP\n",
    "# print(\"Najlepsze parametry:\", study_umap.best_params)\n",
    "# print(\"Najlepszy wynik:\", study_umap.best_value)\n",
    "\n",
    "# Baseline dla danych upscalowanyc bez redukcji wymiarów i agumentacji 0.9642205\n",
    "######## study_Auto_sampler_New_1 <--- Obecna wersja operacji affiniacji i morfologcznmych. \n",
    "# Optymalizcja z UMAP ---->: \n",
    "# bez Umap                   [I 2025-01-07 13:31:21,997] Trial 237 finished with value: 0.9660396137534527 and parameters: {'n_neighbors_lof': 222, 'contamination_lof': 0.1625, 'contamination_forest': 0.195}. Best is trial 237 with value: 0.9660396137534527.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tworzenie instancji klasy ModelEvaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "def create_voting_classifier():\n",
    "    \"\"\"Tworzy VotingClassifier z trzema estymatorami.\"\"\"\n",
    "\n",
    "    estimators = [\n",
    "        ('qda', QDA(reg_param=0.1)),\n",
    "        ('hgb', HistGradientBoostingClassifier()),\n",
    "        ('et', ExtraTreesClassifier())\n",
    "    ]\n",
    "    return VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "def augment_data(X_train, y_train, n_neighbors_lof, contamination_lof, \n",
    "                    contamination_forest):\n",
    "    \"\"\"\n",
    "    Augmentuje dane treningowe poprzez:\n",
    "        1. Wykrywanie outlierów za pomocą LOF i IsolationForest.\n",
    "        2. Augmentację danych dla outlierów (przesunięcie i obrót).\n",
    "        3. Redukcję wymiarów UMAP dla całego zestawu danych.\n",
    "        4. Wizualizację outlierów i ich augmentacji.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Redukcja wymiarów UMAP dla LOF\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    X_train_reduced = reducer.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "\n",
    "    # 2. Wykrywanie outlierów (LOF)\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors_lof, contamination=contamination_lof)\n",
    "    y_pred_lof = lof.fit_predict(X_train_reduced)\n",
    "\n",
    "    # 3. Isolation Forest na surowych danych\n",
    "    iso_forest = IsolationForest(contamination=contamination_forest)\n",
    "    y_pred_iso = iso_forest.fit_predict(X_train.reshape(X_train.shape[0], -1))\n",
    "\n",
    "    # 4. Połączenie wyników LOF i IsolationForest\n",
    "    y_pred_combined = (y_pred_lof + y_pred_iso) / 2\n",
    "    outlier_indices = np.where(y_pred_combined < 0)[0]\n",
    "    print(f\"Liczba outlinerów: {len(outlier_indices)}\")\n",
    "    \n",
    "    \n",
    "    # 5. Augmentacja danych dla outlierów\n",
    "    X_train_augmented = []\n",
    "    y_train_augmented = []\n",
    "\n",
    "    if len(outlier_indices) > 0:\n",
    "        n_outliers_to_show = min(12, len(outlier_indices))  # Ograniczenie liczby wyświetlanych outlierów\n",
    "        fig, axes = plt.subplots(n_outliers_to_show, 5, figsize=(15, 3 * n_outliers_to_show))\n",
    "        for i, idx in enumerate(outlier_indices[:n_outliers_to_show]):\n",
    "            image = X_train[idx]\n",
    "            label = y_train[idx]  # Pobranie etykiety dla outliera\n",
    "            # Oryginalny obraz\n",
    "            axes[i, 0].imshow(image, cmap='gray')\n",
    "            axes[i, 0].set_title(f'Outlier {i+1}')\n",
    "            axes[i, 0].axis('off')\n",
    "\n",
    "            # 1. Dodanie paddingu (czarne tło)\n",
    "            padded_image = np.pad(image, pad_width=35, mode='constant')  # Zwiększony padding\n",
    "            for j in range(4):  # Zmienna j zdefiniowana wewnątrz pętli\n",
    "                # 2. Transformacja afiniczna\n",
    "                transform = AffineTransform(scale=(np.random.uniform(0.97, 1.04), np.random.uniform(0.97, 1.04)),\n",
    "                                            rotation=np.deg2rad(np.random.randint(-20, 21)),\n",
    "                                            shear=np.deg2rad(np.random.randint(-5, 6))\n",
    "                                            )  \n",
    "                transformed_image = warp(padded_image, transform, mode='wrap', preserve_range=True)\n",
    "                \n",
    "                # # 3. Wyizolowanie cyfry po transformacji (wykrywanie konturów)\n",
    "                thresh = transformed_image > 20  # Zwiększony próg\n",
    "                contours = find_contours(thresh, 0.75)\n",
    "\n",
    "                # --- Obsługa błędów ---\n",
    "                try:\n",
    "                    cnt = contours[0]  # Wybór pierwszego konturu\n",
    "\n",
    "                    # Wyznaczenie bounding box\n",
    "                    minr = int(np.min(cnt[:, 0]))\n",
    "                    minc = int(np.min(cnt[:, 1]))\n",
    "                    maxr = int(np.max(cnt[:, 0]))\n",
    "                    maxc = int(np.max(cnt[:, 1]))\n",
    "\n",
    "                    # Sprawdzenie, czy bounding box ma poprawny kształt\n",
    "                    if minr == maxr or minc == maxc:\n",
    "                        print(\"Nieprawidłowy bounding box - pomijanie augmentacji dla tego obrazu.\")\n",
    "                        continue\n",
    "\n",
    "                except IndexError:\n",
    "                    print(\"Nie znaleziono konturu - pomijanie augmentacji dla tego obrazu.\")\n",
    "                    continue\n",
    "\n",
    "                # 4. Wycięcie cyfry na podstawie bounding box\n",
    "                isolated_digit = transformed_image[minr:maxr, minc:maxc]\n",
    "            \n",
    "                # --- Erozja ---\n",
    "                if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                    isolated_digit = erosion(isolated_digit, disk(1))\n",
    "                    \n",
    "                # --- Otwarcie  ---\n",
    "                if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                    isolated_digit = opening(isolated_digit, disk(1))\n",
    "                    \n",
    "                # --- Zamknięcie \n",
    "                if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                    isolated_digit = closing(isolated_digit, disk(1))\n",
    "                    \n",
    "                if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                    isolated_digit = closing(isolated_digit, disk(1))\n",
    "                              \n",
    "                \n",
    "                # 5. Wycentrowanie cyfry w obrazie 42x42\n",
    "                new_image = np.zeros((42, 42), dtype=np.uint8)\n",
    "                row_offset = max(0, (42 - (maxr - minr)) // 2)\n",
    "                col_offset = max(0, (42 - (maxc - minc)) // 2)\n",
    "                new_image[row_offset:row_offset + maxr - minr, col_offset:col_offset + maxc - minc] = isolated_digit\n",
    "\n",
    "                isolated_digit = new_image\n",
    "\n",
    "                # 6. Dodanie do listy\n",
    "                X_train_augmented.append(isolated_digit)  # Dodaj augmentowany obraz\n",
    "                y_train_augmented.append(label)  # Dodaj etykietę\n",
    "\n",
    "\n",
    "                axes[i, j+1].imshow(new_image, cmap='gray')\n",
    "                axes[i, j+1].axis('off')\n",
    "\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "    X_train_augmented = np.array(X_train_augmented)\n",
    "    y_train_augmented = np.array(y_train_augmented)\n",
    "\n",
    "    # Połączenie danych augmentowanych z oryginalnymi danymi\n",
    "    X_train_augmented = np.concatenate((X_train, X_train_augmented), axis=0)\n",
    "    y_train_augmented = np.concatenate((y_train, y_train_augmented), axis=0)\n",
    "\n",
    "    return X_train_augmented, y_train_augmented, outlier_indices, X_train_reduced\n",
    "\n",
    "\n",
    "# Zakładam, że masz zdefiniowane smoothed_X_train i y_train\n",
    "X_train_Upsc, X_val_Upsc, y_train_Upsc, y_val_Upsc = train_test_split(\n",
    "    smoothed_X_train, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "# Wywołanie funkcji augment_data\n",
    "X_train_augmented, y_train_augmented, outlier_indices, X_train_reduced = augment_data(\n",
    "    X_train_Upsc,\n",
    "    y_train_Upsc,\n",
    "    n_neighbors_lof=399,\n",
    "    contamination_lof=0.33,\n",
    "    contamination_forest=0.005\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial 34 finished with value: 0.9750009769150542 and parameters: {'n_neighbors_lof': 483, 'contamination_lof': 0.0475, 'contamination_forest': 0.10500000000000001, 'n_components_umap': 76, 'n_neighbors': 1436, 'min_dist': 0.13874920511220656, 'spread': 1.7486984209417518, 'local_connectivity': 2.5349453827107986, 'set_op_mix_ratio': 0.4209229951120764}. Best is trial 34 with value: 0.9750009769150542.\n",
    "from tensorflow.keras.applications import EfficientNetV2B0, EfficientNetV2S, EfficientNetV2M, EfficientNetV2L \n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def create_efficientnetv2_model(\n",
    "    input_shape,\n",
    "    num_classes,\n",
    "    efficientnet_version=\"B0\",\n",
    "    trainable=False,\n",
    "    dropout_rate=0.2,\n",
    "    weights=None\n",
    "):\n",
    "    \"\"\"Tworzy model oparty na EfficientNetV2.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Dane wejściowe.\n",
    "        num_classes: Liczba klas.\n",
    "        efficientnet_version: Wersja EfficientNetV2 ('B0', 'S', 'M', 'L').\n",
    "        trainable: Czy warstwy EfficientNetV2 mają być trenowalne.\n",
    "        dropout_rate: Współczynnik dropout.\n",
    "\n",
    "    Returns:\n",
    "        Model Keras.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wybór wersji EfficientNetV2\n",
    "    if efficientnet_version == \"B0\":\n",
    "        base_model = EfficientNetV2B0(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    elif efficientnet_version == \"S\":\n",
    "        base_model = EfficientNetV2S(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    elif efficientnet_version == \"M\":\n",
    "        base_model = EfficientNetV2M(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    elif efficientnet_version == \"L\":\n",
    "        base_model = EfficientNetV2L(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Nieprawidłowa wersja EfficientNetV2.\")\n",
    "\n",
    "    # Ustawienie warstw EfficientNetV2 jako trenowalne\n",
    "    base_model.trainable = trainable\n",
    "\n",
    "    # Budowa modelu\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Baseline data without transformations and agumentation \n",
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(smoothed_X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "\n",
    "# Odtworzenie augmentacji z najlepszymi parametrami\n",
    "X_train_augmented, y_train_augmented = augment_data(\n",
    "    X_train_base, \n",
    "    y_train_base, \n",
    "    n_neighbors_lof=399,\n",
    "    contamination_lof=0.33,\n",
    "    contamination_forest=0.005\n",
    ")\n",
    "\n",
    "X_train_base = np.expand_dims(X_train_base, axis=-1)\n",
    "X_val_base = np.expand_dims(X_val_base, axis=-1)  \n",
    "X_train_augmented = np.expand_dims(X_train_augmented, axis=-1)\n",
    "\n",
    "# --- Trenowanie modeli ---\n",
    "# Model na danych bez redukcji\n",
    "model_base = create_efficientnetv2_model(\n",
    "    input_shape=X_train_base.shape[1:], \n",
    "    num_classes=10, \n",
    "    efficientnet_version=\"B0\",\n",
    "    trainable=True,\n",
    "    weights=None,\n",
    ")\n",
    "\n",
    "model_base.compile(\n",
    "    optimizer=\"Lion\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"] \n",
    ")\n",
    "\n",
    "history_base = model_base.fit(\n",
    "    X_train_base,\n",
    "    y_train_base,\n",
    "    epochs=16,\n",
    "    batch_size=512,\n",
    "    validation_data=(X_val_base, y_val_base),\n",
    ")\n",
    "\n",
    "# Model na danych z redukcją\n",
    "model_transformed = create_efficientnetv2_model(\n",
    "    input_shape=X_train_augmented.shape[1:],  \n",
    "    num_classes=10, \n",
    "    efficientnet_version=\"B0\",\n",
    "    trainable=True,\n",
    "    weights=None,\n",
    ")\n",
    "\n",
    "model_transformed.compile(\n",
    "    optimizer=\"Lion\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"] \n",
    ")\n",
    "\n",
    "history_transformed = model_transformed.fit(\n",
    "    X_train_augmented,\n",
    "    y_train_augmented,\n",
    "    epochs=16,\n",
    "    batch_size=512,\n",
    "    validation_data=(X_val_base, y_val_base),\n",
    ")\n",
    "\n",
    "# --- Predykcja ---\n",
    "y_pred_base = model_base.predict(X_val_base)\n",
    "y_pred_transformed = model_transformed.predict(X_val_base)\n",
    "\n",
    "# --- Porównanie wyników ---\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Oblicza metryki.\"\"\"\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true_labels, y_pred_labels),\n",
    "        \"precision\": precision_score(y_true_labels, y_pred_labels, average=\"macro\"),\n",
    "        \"recall\": recall_score(y_true_labels, y_pred_labels, average=\"macro\"),\n",
    "        \"f1\": f1_score(y_true_labels, y_pred_labels, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "metrics_base = calculate_metrics(y_val_base, y_pred_base)\n",
    "metrics_transformed = calculate_metrics(y_val_base, y_pred_transformed)\n",
    "\n",
    "print(\"Metryki dla modelu bez agumentacji:\", metrics_base)\n",
    "print(\"Metryki dla modelu z agumentacją:\", metrics_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deklaracja funkcji dla ekstrakcji cech z obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_hog_parallel(images, orientations=None, pixels_per_cell=None, cells_per_block=None, n_jobs=-1):\n",
    "    \"\"\"Oblicza deskryptory HOG dla obrazu.\"\"\"\n",
    "    \n",
    "     # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if orientations is None:\n",
    "        orientations = 9\n",
    "    if pixels_per_cell is None:\n",
    "        pixels_per_cell = (8, 8)\n",
    "    if cells_per_block is None:\n",
    "        cells_per_block = (2, 2)\n",
    "    \n",
    "    def calculate_hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block):\n",
    "        \n",
    "        image = img_as_float32(image)\n",
    "\n",
    "        fd = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                cells_per_block=cells_per_block, visualize=False, channel_axis=None)\n",
    "\n",
    "        return fd\n",
    "    \n",
    "    return Parallel(n_jobs=n_jobs)(delayed(calculate_hog)(image, orientations, pixels_per_cell, cells_per_block) for image in images)\n",
    "\n",
    "\n",
    "def apply_gabor_filters_parallel(images, thetas=None, sigmas=None, frequencies=None, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Stosuje filtry Gabora do wielu obrazów równolegle.\n",
    "\n",
    "    Argumenty:\n",
    "        images: Lista obrazów wejściowych.\n",
    "        thetas: (Opcjonalnie) Lista kątów orientacji filtrów Gabora (w radianach). Jeśli None, używane są wartości domyślne.\n",
    "        sigmas: (Opcjonalnie) Lista odchyleń standardowych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        frequencies: (Opcjonalnie) Lista częstotliwości przestrzennych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        n_jobs: Liczba rdzeni procesora do wykorzystania podczas zrównoleglania. -1 oznacza użycie wszystkich dostępnych rdzeni.\n",
    "\n",
    "    Zwraca:\n",
    "        Listę tablic NumPy zawierających obrazy przefiltrowane filtrami Gabora, o typie danych float32.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if thetas is None:\n",
    "        thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    if sigmas is None:\n",
    "        sigmas = [1, 3]\n",
    "    if frequencies is None:\n",
    "        frequencies = [0.05, 0.25]\n",
    "\n",
    "    def apply_gabor_filters_single(image):\n",
    "        image = img_as_float32(image)\n",
    "        filtered_images = [gabor(image, frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)[0]\n",
    "                           for theta in thetas for sigma in sigmas for frequency in frequencies]\n",
    "        return np.array(filtered_images, dtype=np.float32)\n",
    "\n",
    "    return Parallel(n_jobs=n_jobs)(delayed(apply_gabor_filters_single)(image) for image in images)\n",
    "\n",
    "\n",
    "def extract_features_HOG_Gabor(X, hog_orientations=None, hog_pixels_per_cell=None, hog_cells_per_block=None, \n",
    "                              gabor_thetas=None, gabor_sigmas=None, gabor_frequencies=None):\n",
    "    \"\"\"Generator zwracający cechy HOG i Gabora dla obrazów.\"\"\"\n",
    "\n",
    "    hog_features_list = calculate_hog_parallel(X, orientations=hog_orientations, pixels_per_cell=hog_pixels_per_cell, cells_per_block=hog_cells_per_block)\n",
    "    gabor_features_list = apply_gabor_filters_parallel(X, thetas=gabor_thetas, sigmas=gabor_sigmas, frequencies=gabor_frequencies)\n",
    "\n",
    "    for hog_features, gabor_features in zip(hog_features_list, gabor_features_list):\n",
    "        \n",
    "        gabor_features_flattened = np.array([img.flatten() for img in gabor_features])\n",
    "\n",
    "        # Dopasowujemy kształt hog_features do gabor_features_flattened\n",
    "        hog_features_reshaped = np.repeat(hog_features.reshape(1, -1), gabor_features_flattened.shape[0], axis=0)\n",
    "\n",
    "        # Łączymy cechy HOG i Gabora w poziomie\n",
    "        concatenated_features = np.hstack((hog_features_reshaped, gabor_features_flattened))\n",
    "\n",
    "        concatenated_features = concatenated_features.flatten()\n",
    "\n",
    "        yield concatenated_features\n",
    "\n",
    "#################################################################################################################################################\n",
    "################################################  Directional filters, ORB    ###################################################################\n",
    "\n",
    "def apply_directional_filters(image, kernel_sizes=[3, 5]):\n",
    "    \"\"\"Stosuje filtry kierunkowe do obrazu.\"\"\"\n",
    "    filtered_images = []\n",
    "\n",
    "    # Wykrywanie krawędzi filtrem Canny'ego\n",
    "    edges = cv2.Canny(image, 50, 150)\n",
    "\n",
    "    def process_kernel(kernel_size):\n",
    "        if kernel_size % 2 == 0 or kernel_size > 31:\n",
    "            raise ValueError(\n",
    "                f\"Nieprawidłowy rozmiar jądra: {kernel_size}. Rozmiar jądra musi być nieparzysty i nie większy niż 31.\"\n",
    "            )\n",
    "        kernel_horizontal = cv2.getDerivKernels(1, 0, kernel_size,\n",
    "                                               normalize=True)\n",
    "        kernel_vertical = cv2.getDerivKernels(0, 1, kernel_size,\n",
    "                                             normalize=True)\n",
    "        filtered = cv2.sepFilter2D(edges, cv2.CV_32F, kernel_horizontal[0],\n",
    "                                  kernel_vertical[0])\n",
    "        return filtered.flatten()\n",
    "\n",
    "    with Pool() as pool:\n",
    "        filtered_images = pool.map(process_kernel, kernel_sizes)\n",
    "\n",
    "    # Analiza składowych spójnych\n",
    "    labels = label(edges)\n",
    "    props = regionprops(labels)\n",
    "    features = [prop.area for prop in props]\n",
    "\n",
    "    return np.concatenate(\n",
    "        [np.array(filtered_images).squeeze(), np.array(features)])\n",
    "\n",
    "\n",
    "def extract_orb_features(image, nfeatures=100):\n",
    "    \"\"\"Wyodrębnia cechy ORB z obrazu.\"\"\"\n",
    "    orb = cv2.ORB_create(nfeatures=nfeatures,\n",
    "                         fastThreshold=12,\n",
    "                         edgeThreshold=12)\n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        return []\n",
    "    return descriptors.flatten()\n",
    "\n",
    "\n",
    "def extract_combined_features(image, kernel_sizes=[3, 5], nfeatures=100):\n",
    "    \"\"\"Łączy cechy z filtrów kierunkowych i ORB.\"\"\"\n",
    "    directional_features = apply_directional_filters(image, kernel_sizes)\n",
    "    orb_features = extract_orb_features(image, nfeatures)\n",
    "\n",
    "    # Łączenie cech\n",
    "    combined_features = np.concatenate((directional_features, orb_features))\n",
    "\n",
    "    return combined_features\n",
    "   \n",
    "#################################################################################################################################################\n",
    "################################################  GLCM, Zernike    ##############################################################################\n",
    "def extract_glcm_features(image, distances=[1], angles=[0], properties=['contrast', 'energy', 'homogeneity', 'correlation']):\n",
    "    \"\"\"Wyodrębnia cechy GLCM z obrazu.\"\"\"\n",
    "    if len(image.shape) > 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    glcm = graycomatrix(image, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    features = []\n",
    "    for prop in properties:\n",
    "        features.extend(graycoprops(glcm, prop).flatten())\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testowanie modelu dla HOG i filtrów Gabora i porównanie wydajności przy zastosowaniu redukcji wymiarów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, y_train, use_pca=True):\n",
    "    \"\"\"\n",
    "    Trenuje i ocenia modele QDA i SVC, opcjonalnie z redukcją wymiarów PCA.\n",
    "\n",
    "    Argumenty:\n",
    "        X_train: Dane treningowe.\n",
    "        y_train: Etykiety treningowe.\n",
    "        use_pca: Flaga wskazująca, czy używać PCA (domyślnie True).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ekstrakcja cech HOG i Gabor za pomocą generatora\n",
    "    X_train_features = np.array(list(extract_features_HOG_Gabor(X_train)))\n",
    "\n",
    "    # Podział na zbiór treningowy i walidacyjny\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_features, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "    # Redukcja wymiarów (opcjonalnie)\n",
    "    if use_pca:\n",
    "        #pca_dim = PCA(n_components=20)\n",
    "        pca_dim = umap.UMAP(n_components=20, random_state=42)\n",
    "        X_train = pca_dim.fit_transform(X_train)\n",
    "        X_val = pca_dim.transform(X_val)\n",
    "\n",
    "    # Tworzenie i trenowanie modelu QDA\n",
    "    qda = QDA(reg_param=0.1)\n",
    "    qda.fit(X_train, y_train)\n",
    "\n",
    "    # Tworzenie i trenowanie modelu SVC\n",
    "    svc = SVC(kernel='rbf')\n",
    "    svc.fit(X_train, y_train)\n",
    "\n",
    "    # Ocena modelu\n",
    "    y_pred_qda = qda.predict(X_val)\n",
    "    accuracy_qda = accuracy_score(y_val, y_pred_qda)\n",
    "\n",
    "    y_pred_svc = svc.predict(X_val)\n",
    "    accuracy_svc = accuracy_score(y_val, y_pred_svc)\n",
    "\n",
    "    print(\"Dokładność QDA HOG i Gabor\", \"po PCA:\" if use_pca else \":\", accuracy_qda)\n",
    "    print(\"Dokładność SVC HOG i Gabor\", \"po PCA:\" if use_pca else \":\", accuracy_svc)\n",
    "\n",
    "# Test z PCA\n",
    "train_and_evaluate_models(X_train, y_train, use_pca=True)\n",
    "train_and_evaluate_models(X_train, y_train, use_pca=False)\n",
    "#cProfile.run('train_and_evaluate_models(X_train, y_train, use_pca=True)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTUNA dla HOG, Gabor, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", module=\"optuna\")\n",
    "\n",
    "\n",
    "# def objective(trial, X_train, y_train):\n",
    "#     \"\"\"\n",
    "#     Funkcja celu dla optymalizacji Optuna. \n",
    "#     Przyjmuje próbę (trial), dane treningowe (X_train) i etykiety (y_train).\n",
    "#     Zwraca dokładność QDA i SVM na danych walidacyjnych po zastosowaniu sekwencji transformacji.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Parametry HOG\n",
    "#     orientations      = trial.suggest_int(\"hog_orientations\", 2, 16)\n",
    "#     pixels_per_cell   = trial.suggest_categorical(\"hog_pixels_per_cell\", [str((2, 2)), str((2, 3)), str((3, 2)), str((3, 3)), str((4, 4)), str((5, 5)),str((6, 6)), str((7, 7))])\n",
    "#     cells_per_block   = trial.suggest_categorical(\"hog_cells_per_block\", [str((1, 1)), str((2, 2)), str((3, 3)), str((4, 4))])\n",
    "\n",
    "#     # Parametry filtrów Gabora\n",
    "#     gabor_thetas      = trial.suggest_categorical(\"gabor_thetas\", ([0],  [np.pi / 14],  [np.pi / 12], [0, np.pi / 14 ], [0, np.pi / 12 ] ))\n",
    "#     gabor_sigmas      = trial.suggest_categorical(\"gabor_sigmas\", ([0.33], [0.66], [1], [0.33 , 1.25], [0.66, 2.35], [1, 3]))\n",
    "#     gabor_frequencies = trial.suggest_categorical(\"gabor_frequencies\", ([0.025], [0.05], [0.125], [0.025, 0.125], [0.05, 0.175], [0.05, 0.25]))\n",
    "        \n",
    "#     #PCA\n",
    "#     PCA_n_components  =  trial.suggest_int(\"PCA_Components\", 4, 228)\n",
    "#     kernel = trial.suggest_categorical(\"kernel_pca_kernel\", ['linear', 'poly', 'rbf', 'sigmoid', 'cosine'])\n",
    "#     gamma = None\n",
    "#     if kernel in ['poly', 'rbf']:\n",
    "#         gamma = trial.suggest_loguniform(\"kernel_pca_gamma\", 0.001, 1)\n",
    "        \n",
    "#     # Powiększanie i wygładzanie\n",
    "#     # X_train_split = np.array([enlarge_and_smooth(img, new_size) for img in X_train])\n",
    "#     # X_val = np.array([enlarge_and_smooth(img, new_size) for img in X_val])\n",
    "    \n",
    "#     # Ekstrakcja cech HOG i Gabor\n",
    "#     X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=orientations, \n",
    "#         hog_pixels_per_cell=eval(pixels_per_cell), # Konwersja stringa na krotkę\n",
    "#         hog_cells_per_block=eval(cells_per_block),  # Konwersja stringa na krotkę\n",
    "#         gabor_thetas=gabor_thetas, \n",
    "#         gabor_sigmas=gabor_sigmas, \n",
    "#         gabor_frequencies=gabor_frequencies\n",
    "#     )))\n",
    "\n",
    "#     # Podział na zbiór treningowy i walidacyjny\n",
    "#     X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train, test_size=0.2, random_state=69)\n",
    "    \n",
    "    \n",
    "#     # Kernel PCA\n",
    "#     kpca = KernelPCA(n_components=PCA_n_components, kernel=kernel, gamma=gamma)\n",
    "#     X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "#     X_val_reduced = kpca.transform(X_val_filters)\n",
    "                \n",
    "#     # SVC\n",
    "#     svc = SVC(kernel='rbf')\n",
    "#     svc.fit(X_train_reduced, y_train_filters)\n",
    "#     y_pred_svm = svc.predict(X_val_reduced)\n",
    "#     accuracy_svm = accuracy_score(y_val_filters, y_pred_svm)\n",
    "\n",
    "#     return accuracy_svm\n",
    "\n",
    "# # Optymalizacji\n",
    "                            \n",
    "# study = optuna.create_study(directions=[\"maximize\"], sampler=optuna.samplers.CmaEsSampler() ) \n",
    "# study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, n_jobs=1)\n",
    "\n",
    "\n",
    "# print(\"Best parameters:\", study.best_params)\n",
    "# print(\"Best QDA accuracy:\", study.best_trial.values[0]) \n",
    "# print(\"Best SVM accuracy:\", study.best_trial.values[1]) \n",
    "\n",
    "# # [I 2024-09-17 05:43:37,964] Trial 13 finished with value: 0.99133 and parameters: {'hog_orientations': 16, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(3, 3)', 'gabor_thetas': [0.2243994752564138], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.025], 'PCA_Components': 209, 'kernel_pca_kernel': 'cosine'}. Best is trial 13 with value: 0.9913333333333333.\n",
    "# # [I 2024-09-17 08:45:58,345] Trial 19 finished with value: 0.99175 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(2, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.025], 'PCA_Components': 132, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 19 with value: 0.99175.\n",
    "# # [I 2024-09-17 10:45:29,605] Trial 23 finished with value: 0.99225 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(3, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 150, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:24:59,082] Trial 28 finished with value: 0.99108 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(1, 1)', 'gabor_thetas': [0, 0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 190, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:54:44,248] Trial 29 finished with value: 0.9915 and parameters: {'hog_orientations': 7, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05], 'PCA_Components': 164, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 18:02:03,444] Trial 37 finished with value: 0.99066 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05, 0.175], 'PCA_Components': 169, 'kernel_pca_kernel': 'rbf', 'kernel_pca_gamma': 0.0032186977490984547}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 19:02:58,946] Trial 39 finished with value: 0.9915 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0, 0.2243994752564138], 'gabor_sigmas': [1, 3], 'gabor_frequencies': [0.125], 'PCA_Components': 148, 'kernel_pca_kernel': 'linear'}. Best is trial 23 with value: 0.99225.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystanie wcześniej otrzymanch hiperparamatrów dla HOG i filtrów Gabora po KPCA w celu stworzenia wektora cech z obrazów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=6, \n",
    "#         hog_pixels_per_cell=eval('(3, 3)'), \n",
    "#         hog_cells_per_block=eval('(2, 2)'),  \n",
    "#         gabor_thetas=[0], \n",
    "#         gabor_sigmas=[1], \n",
    "#         gabor_frequencies=[0.05, 0.25]\n",
    "#     )))\n",
    "\n",
    "\n",
    "# kpca = KernelPCA(n_components=150, kernel='cosine')\n",
    "\n",
    "# X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train_ENC, test_size=0.2, random_state=69)\n",
    "\n",
    "# X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "# X_val_reduced = kpca.transform(X_val_filters)\n",
    "\n",
    "\n",
    "# np.save('X_train_reduced.npy', X_train_reduced)\n",
    "# np.save('X_val_reduced.npy', X_val_reduced)\n",
    "# np.save('y_train_filters.npy', y_train_filters)\n",
    "# np.save('y_val_filters.npy', y_val_filters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetV2B0, EfficientNetV2S, EfficientNetV2M, EfficientNetV2L \n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "smoothed_X_train = np.array([enlarge_and_smooth(image, new_size=(42, 42)) for image in X_train])\n",
    "\n",
    "# Baseline accuracy without transformations\n",
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(\n",
    "    smoothed_X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "\n",
    "X_train_base = np.expand_dims(X_train_base, axis=-1)  \n",
    "X_val_base = np.expand_dims(X_val_base, axis=-1) \n",
    "\n",
    "print(X_train_base.shape)\n",
    "print(X_val_base.shape)\n",
    "\n",
    "def create_efficientnetv2_model(\n",
    "    input_shape,\n",
    "    num_classes,\n",
    "    efficientnet_version=\"B0\",\n",
    "    trainable=False,\n",
    "    dropout_rate=0.2,\n",
    "    weights=None\n",
    "):\n",
    "    \"\"\"Tworzy model oparty na EfficientNetV2.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Dane wejściowe.\n",
    "        num_classes: Liczba klas.\n",
    "        efficientnet_version: Wersja EfficientNetV2 ('B0', 'S', 'M', 'L').\n",
    "        trainable: Czy warstwy EfficientNetV2 mają być trenowalne.\n",
    "        dropout_rate: Współczynnik dropout.\n",
    "\n",
    "    Returns:\n",
    "        Model Keras.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wybór wersji EfficientNetV2\n",
    "    if efficientnet_version == \"B0\":\n",
    "        base_model = EfficientNetV2B0(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    elif efficientnet_version == \"S\":\n",
    "        base_model = EfficientNetV2S(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    elif efficientnet_version == \"M\":\n",
    "        base_model = EfficientNetV2M(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    elif efficientnet_version == \"L\":\n",
    "        base_model = EfficientNetV2L(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Nieprawidłowa wersja EfficientNetV2.\")\n",
    "\n",
    "    # Ustawienie warstw EfficientNetV2 jako trenowalne\n",
    "    base_model.trainable = trainable\n",
    "\n",
    "    # Budowa modelu\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_efficientnetv2_model(\n",
    "    input_shape=X_train_base.shape[1:],  # Przekazanie pełnego kształtu\n",
    "    num_classes=10, \n",
    "    efficientnet_version=\"B0\",\n",
    "    trainable=True,\n",
    "    weights=None,\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(\n",
    "        optimizer=\"Lion\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"] \n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_base,\n",
    "    y_train_base,\n",
    "    epochs=16,\n",
    "    batch_size=512,\n",
    "    validation_data=(X_val_base, y_val_base),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pierwszy model na sieci konwolucyjnej dla bazy z HOG i Gabora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LayerNormalization , BatchNormalization, PReLU , Input, ActivityRegularization, GlobalAveragePooling1D, SeparableConv1D \n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Lion\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "X_train_reduced = np.load('X_train_reduced.npy')\n",
    "X_val_reduced = np.load('X_val_reduced.npy')\n",
    "y_train_filters = np.load('y_train_filters.npy')\n",
    "y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "def calculate_max_pool_size(input_length, prev_pool_size, min_output_size=2):\n",
    "    calculated_pool_size = input_length // prev_pool_size\n",
    "    return max(min_output_size, calculated_pool_size)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Funkcja celu dla Optuny.\"\"\"\n",
    "    #os.environ['TF_NUM_INTRAOP_THREADS'] = '1' ## Czy Kers działa na pojedycznym wątku ????\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    activation_functions = ['relu', 'elu', 'leaky_relu', 'tanh']\n",
    "    activation = trial.suggest_categorical('activation', activation_functions)\n",
    "\n",
    "    # Definiowanie modelu\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_reduced.shape[1], 1)))\n",
    "\n",
    "    # Pierwsza warstwa konwolucyjna\n",
    "    model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_1', 48, 450),\n",
    "                     kernel_size=trial.suggest_int('kernel_size_1', 2, 16),\n",
    "                     #activation=activation,\n",
    "                     padding='same', \n",
    "                     #dilation_rate=trial.suggest_int('dilation_rate_1', 1, 4)\n",
    "                     )\n",
    "              )\n",
    "    model.add(PReLU())                #$$$$$$$ LAYER przed PRelu\n",
    "    model.add(LayerNormalization())   ######\n",
    "    model.add(ActivityRegularization(l1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True), \n",
    "                                     l2=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "                                     )\n",
    "              )\n",
    "    pool_size_1 = trial.suggest_int('pool_size_1', 2, 10)\n",
    "    model.add(MaxPooling1D(pool_size=pool_size_1))\n",
    "\n",
    "\n",
    "\n",
    "    # Druga warstwa konwolucyjna\n",
    "    input_length_2 = X_train_reduced.shape[1] // pool_size_1\n",
    "    model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_2', 24, 400),\n",
    "                     kernel_size=trial.suggest_int('kernel_size_2', 2, 14),\n",
    "                     padding='same',\n",
    "                     #activation=activation,\n",
    "                     #dilation_rate=trial.suggest_int('dilation_rate_2', 1, 4)\n",
    "                     )\n",
    "              )\n",
    "    model.add(PReLU())\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(ActivityRegularization(l1=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True), \n",
    "                                     l2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "                                     )\n",
    "              )\n",
    "    pool_size_2 = trial.suggest_int('pool_size_2', 2, input_length_2 // 2)\n",
    "    #pool_size_2 = trial.suggest_int('pool_size_2', 2, 10)\n",
    "    model.add(MaxPooling1D(pool_size=pool_size_2))\n",
    "\n",
    "\n",
    "\n",
    "    # Trzecia warstwa konwolucyjna\n",
    "    #input_length_3 = input_length_2 // pool_size_2\n",
    "    model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_3', 32, 300),\n",
    "                     kernel_size=trial.suggest_int('kernel_size_3', 2, 12),\n",
    "                     padding='same',\n",
    "                     )\n",
    "              )\n",
    "    model.add(PReLU())\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Dropout(rate=trial.suggest_float('dropout_rate_3', 0.01, 0.5)\n",
    "                      )\n",
    "              )\n",
    "    #pool_size_3 = trial.suggest_int('pool_size_3', 2, max(2, input_length_3 // 2))\n",
    "    #pool_size_3 = trial.suggest_int('pool_size_3', 2, 8)\n",
    "    #model.add(MaxPooling1D(pool_size=pool_size_3))\n",
    "\n",
    "    # Warstwy Dense\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "    model.add(Dense(units=trial.suggest_int('dense_units_1', 40, 300), activation=activation))\n",
    "    model.add(Dropout(rate=trial.suggest_float('dropout_rate_4', 0.01, 0.75)))\n",
    "    \n",
    "    model.add(Dense(units=trial.suggest_int('dense_units_2', 20, 200), activation=activation))\n",
    "    model.add(Dropout(rate=trial.suggest_float('dropout_rate_5', 0.01, 0.65)))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Optymalizator\n",
    "    lr_initial = trial.suggest_float(\"lr_initial\", 1e-5, 1e-1, log=True)\n",
    "    decay_steps = trial.suggest_int(\"decay_steps\", 10, 150)\n",
    "    decay_rate = trial.suggest_float(\"decay_rate\", 0.01, 0.99)\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate=lr_initial,\n",
    "                                  decay_steps=decay_steps,\n",
    "                                  decay_rate=decay_rate)\n",
    "\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Lion'])\n",
    "    optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    optimizer = optimizer_class(learning_rate=lr_schedule)\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=16, start_from_epoch=10, min_delta = 0.00005)\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    model.fit(X_train_reduced, y_train_filters,\n",
    "              epochs=trial.suggest_int('epochs', 40, 150),\n",
    "              batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "              validation_data=(X_val_reduced, y_val_filters),\n",
    "              verbose=1,\n",
    "              callbacks=[early_stopping])\n",
    "\n",
    "    # Ewaluacja modelu\n",
    "    _, accuracy = model.evaluate(X_val_reduced, y_val_filters, verbose=0)\n",
    "\n",
    "    del model\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n",
    "print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "#[I 2024-10-04 05:18:28,377] Trial 26 finished with value: 0.9810000061988831 and parameters: {'activation': 'elu', 'conv1d_filters_1': 347, 'kernel_size_1': 7,  'l1_1': 0.0002141584459916536, 'l2_1': 4.201061472934882e-06, 'pool_size_1': 6, 'conv1d_filters_2': 77, 'kernel_size_2': 14, 'l1_2': 3.203529032832527e-06, 'l2_2': 0.00015793896479891322, 'pool_size_2': 2, 'conv1d_filters_3': 258, 'kernel_size_3': 4, 'dropout_rate_3': 0.3297488074504043, 'dense_units_1': 271, 'dropout_rate_4': 0.5097723961373417, 'dense_units_2': 78, 'dropout_rate_5': 0.15473996425601394, 'lr_initial': 0.0004175922071628326, 'decay_steps': 33, 'decay_rate': 0.9780884395210858, 'optimizer': 'Lion', 'epochs': 123, 'batch_size': 264}. Best is trial 26 with value: 0.9810000061988831.\n",
    "#[I 2024-10-04 14:58:14,236] Trial 61 finished with value: 0.9797499775886536 and parameters: {'activation': 'elu', 'conv1d_filters_1': 424, 'kernel_size_1': 13, 'l1_1': 0.0005501543495144062, 'l2_1': 0.02794377888054429,   'pool_size_1': 7, 'conv1d_filters_2': 79, 'kernel_size_2': 12, 'l1_2': 1.3699264627909325e-06, 'l2_2': 6.577814668979489e-06, 'pool_size_2': 3, 'conv1d_filters_3': 280, 'kernel_size_3': 3, 'dropout_rate_3': 0.21341586312675653, 'dense_units_1': 292, 'dropout_rate_4': 0.22415600475665626, 'dense_units_2': 106, 'dropout_rate_5': 0.09686035013603865, 'lr_initial': 0.0004430745906851504, 'decay_steps': 22, 'decay_rate': 0.9872365423551039, 'optimizer': 'Lion', 'epochs': 150, 'batch_size': 216}. Best is trial 26 with value: 0.9810000061988831.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieć CONV2D z zamrożonym PCA dla danych pierwotnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# import tensorflow_probability as tfp\n",
    "# import optuna\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "# X_train_image = X_train_image.astype('float32')\n",
    "# X_val_image = X_val_image.astype('float32')\n",
    "# X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "# X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "# def objective(trial):\n",
    "#     # Przestrzeń hiperparametrów\n",
    "#     num_filters1 = trial.suggest_int(\"num_filters1\", 32, 256)    # Pierwsza warstwa Conv2d\n",
    "#     num_filters2 = trial.suggest_int(\"num_filters2\", 64, 256)    # Druga warstwa Conv2d\n",
    "#     num_filters3 = trial.suggest_int(\"num_filters3\", 128, 256)   # Trzecia warstwa Conv2d\n",
    "#     n_components = trial.suggest_int(\"PCA_components\", 350, 500) # PCA\n",
    "\n",
    "#     l1_1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True)\n",
    "#     l2_1=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     l1_2=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True)\n",
    "#     l2_2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "\n",
    "#     l1_3=trial.suggest_float('l1_3', 1e-6, 1e-1, log=True)\n",
    "#     l2_3=trial.suggest_float('l2_3', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     activation_functions = ['relu', 'gelu', 'leaky_relu', 'silu', 'mish']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "    \n",
    "    \n",
    "#     # Warstwy konwolucyjne 2D\n",
    "#     conv_layers = [\n",
    "#         # Pierwsza warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters1,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_1),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_1),\n",
    "#                                 padding='same',\n",
    "#                                 input_shape=(X_train_image.shape[1:])\n",
    "#                                 ),\n",
    "        \n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2,strides=1,\n",
    "#                                  ),\n",
    "#         # Druga warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters2,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_2),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_2),                                \n",
    "#                                 padding='same',\n",
    "#                                 ),\n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "#         # Trzecia warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters3,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_3),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_3),                                \n",
    "#                                 padding='same'\n",
    "#                                 ),\n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        \n",
    "#         tf.keras.layers.Flatten(),\n",
    "#                 ]\n",
    "\n",
    "#     ### Obliczenia dla zamrożonego PCA na podstawie pierwszej inicjalizacji modelu\n",
    "#     conv_model = keras.models.Sequential(conv_layers)  \n",
    "#     X_train_PCA = conv_model.predict(X_train_image)\n",
    "    \n",
    "#     X_train_PCA = tf.reshape(X_train_PCA, [X_train_PCA.shape[0], -1])\n",
    "\n",
    "#     # Macierz kowariancji\n",
    "#     covariance_matrix = tfp.stats.covariance(X_train_PCA, sample_axis=0, event_axis=-1)\n",
    "#     # Wektory i wartości własne\n",
    "#     eigenvalues, eigenvectors = tf.linalg.eigh(covariance_matrix)\n",
    "#     sorted_indices = tf.argsort(eigenvalues, direction='DESCENDING')\n",
    "#     eigenvalues = tf.gather(eigenvalues, sorted_indices)\n",
    "#     eigenvectors = tf.gather(eigenvectors, sorted_indices, axis=1)\n",
    "    \n",
    "#     #### Opcja druga na SVD, ale wymaga jeszcze zabawy\n",
    "#     # s, u, v = tf.linalg.svd(covariance_matrix)\n",
    "#     # eigenvectors = v\n",
    "#     # eigenvalues = tf.square(s) \n",
    "    \n",
    "#     # Warstwa PCA\n",
    "#     model = keras.models.Sequential(conv_layers + [\n",
    "#         tf.keras.layers.Lambda(lambda x, eigenvectors: tf.matmul(x, eigenvectors), arguments={'eigenvectors': eigenvectors[:, :n_components]})\n",
    "#     ])\n",
    "\n",
    "#     # Output (klasyfikacja)\n",
    "#     model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "#     #model.summary()\n",
    "#     tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    \n",
    "\n",
    "#     # Optymalizator\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "#     # Początkową wartość learning rate\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "#     ## Kompilacja modelu\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=8, start_from_epoch=8, min_delta=0.0001)\n",
    "    \n",
    "#     # ReduceLROnPlateau\n",
    "#     reduce_lr = ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.9, step=0.1),\n",
    "#         patience=trial.suggest_int(\"reduce_lr_patience\", 2, 12),\n",
    "#         min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "#     )\n",
    "    \n",
    "#     # Trenowanie i ocena modelu\n",
    "#     history = model.fit(X_train_image, y_train_image,\n",
    "#                         epochs=trial.suggest_int('epochs', 15, 80),\n",
    "#                         batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "#                         validation_data=(X_val_image, y_val_image),\n",
    "#                         callbacks=[early_stopping, reduce_lr]\n",
    "#                         )\n",
    "    \n",
    "#     accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) \n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "# ### dwa tygodnie więc lepiej aby te wartości stanowiły dobrą podstawę...\n",
    "# #Trial 76 finished with value:  0.9906666874885559 and parameters: {'num_filters1': 194, 'num_filters2': 167, 'num_filters3': 221, 'PCA_components': 383, 'l1_1': 1.4917571854023011e-06, 'l2_1': 0.0003844462705458024,  'l1_2': 0.0011921068803372,     'l2_2': 0.00021948428596782946, 'l1_3': 0.05677951548917238,    'l2_3': 7.714986268695782e-05,  'activation': 'relu', 'optimizer': 'AdamW', 'lr_initial': 0.006869840110235172, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.028747844465778e-06,  'epochs': 51, 'batch_size': 479}.\n",
    "# #Trial 99 finished with value:  0.9911666512489319 and parameters: {'num_filters1': 222, 'num_filters2': 220, 'num_filters3': 202, 'PCA_components': 367, 'l1_1': 4.875844948451946e-06,  'l2_1': 0.00014963179483695857, 'l1_2': 2.3622464527905993e-06, 'l2_2': 0.0007506481920246097,  'l1_3': 0.030678868126237845,   'l2_3': 0.00016514043221099142, 'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.00653453290497133,  'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.8941869280860333e-06, 'epochs': 54, 'batch_size': 331}.\n",
    "# #Trial 108 finished with value: 0.9915000200271606 and parameters: {'num_filters1': 205, 'num_filters2': 206, 'num_filters3': 220, 'PCA_components': 370, 'l1_1': 9.95600828812184e-06,   'l2_1': 4.680137051239973e-05,  'l1_2': 4.9076000996726795e-06, 'l2_2': 8.638060092445147e-05,  'l1_3': 0.0361709248993415,     'l2_3': 0.0007397174884371887,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006269166650314777, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 5.203896150606987e-06,  'epochs': 62, 'batch_size': 239}.\n",
    "# #Trial 121 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 189, 'num_filters2': 200, 'num_filters3': 220, 'PCA_components': 387, 'l1_1': 8.32443244291399e-05,   'l2_1': 5.963850113018811e-05,  'l1_2': 4.8017415472934674e-06, 'l2_2': 3.9423405128005956e-05, 'l1_3': 0.00047971791859799064, 'l2_3': 0.0014900662405900767,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006533479568732247, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 6, 'reduce_lr_min_lr': 7.315164005922789e-06,  'epochs': 58, 'batch_size': 249}.\n",
    "# #Trial 134 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 180, 'num_filters2': 196, 'num_filters3': 234, 'PCA_components': 390, 'l1_1': 4.148854564296224e-05,  'l2_1': 2.3209137723337733e-05, 'l1_2': 0.0009209306936723504,  'l2_2': 1.9816087702628798e-05, 'l1_3': 0.0008993453039409338,  'l2_3': 0.002218879345590759,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007404316715937198, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 3.862546224445297e-07,  'epochs': 61, 'batch_size': 368}.\n",
    "# #Trial 151 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 201, 'num_filters2': 189, 'num_filters3': 229, 'PCA_components': 403, 'l1_1': 6.15892270118e-05,      'l2_1': 1.6056284141459273e-05, 'l1_2': 0.0009167391639733724,  'l2_2': 1.4409315946656477e-05, 'l1_3': 0.0008899963895277467,  'l2_3': 0.005080838638011425,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007378956521980373, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 1.9759552872681682e-07, 'epochs': 63, 'batch_size': 416}.\n",
    "# #Trial 153 finished with value: 0.9905833601951599 and parameters: {'num_filters1': 158, 'num_filters2': 190, 'num_filters3': 229, 'PCA_components': 405, 'l1_1': 3.317865892202326e-05,  'l2_1': 1.715760099034611e-05,  'l1_2': 0.00028475050803045533, 'l2_2': 7.144046413508896e-06,  'l1_3': 0.0010057548651594868,  'l2_3': 0.001544820248650564,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007667044780464368, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 1.6284942102179344e-07, 'epochs': 64, 'batch_size': 420}.\n",
    "# #Trial 189 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 187, 'num_filters2': 194, 'num_filters3': 191, 'PCA_components': 381, 'l1_1': 0.0001222508587314991,  'l2_1': 5.2900939483419044e-05, 'l1_2': 4.050810455502403e-05,  'l2_2': 0.00017281194315162931, 'l1_3': 0.0014341620209873955,  'l2_3': 0.0025511470455798512,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006996548564114207, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 6, 'reduce_lr_min_lr': 9.663271049317225e-07,  'epochs': 60, 'batch_size': 219}.\n",
    "# #Trial 205 finished with value: 0.9904999732971191 and parameters: {'num_filters1': 210, 'num_filters2': 189, 'num_filters3': 198, 'PCA_components': 483, 'l1_1': 8.253178154361272e-05,  'l2_1': 1.7866032331171518e-05, 'l1_2': 0.0008019764970918962,  'l2_2': 1.0316537375038554e-05, 'l1_3': 0.000838618983241391,   'l2_3': 0.008249547665798275,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006570979651986537, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.6114120319881016e-07, 'epochs': 63, 'batch_size': 560}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2 dynamic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# import tensorflow_probability as tfp\n",
    "# import optuna\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau \n",
    "\n",
    "# X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "# X_train_image = X_train_image.astype('float32')\n",
    "# X_val_image = X_val_image.astype('float32')\n",
    "# X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "# X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "# def objective(trial):\n",
    "#     # Przestrzeń hiperparametrów\n",
    "#     num_filters1 = trial.suggest_int(\"num_filters1\", 180, 230)    # Pierwsza warstwa Conv2d\n",
    "#     num_filters2 = trial.suggest_int(\"num_filters2\", 180, 230)    # Druga warstwa Conv2d\n",
    "#     num_filters3 = trial.suggest_int(\"num_filters3\", 180, 260)    # Trzecia warstwa Conv2d\n",
    "#     n_components = trial.suggest_int(\"PCA_components\", 350, 420)  # PCA\n",
    "    \n",
    "#     l1_1=trial.suggest_float('l1_1', 1e-6, 5e-3, log=True)\n",
    "#     l2_1=trial.suggest_float('l2_1', 5e-5, 5e-3, log=True)\n",
    "    \n",
    "#     l1_2=trial.suggest_float('l1_2', 1e-6, 1e-3, log=True)\n",
    "#     l2_2=trial.suggest_float('l2_2', 1e-6, 1e-5, log=True)\n",
    "\n",
    "#     l1_3=trial.suggest_float('l1_3', 1e-6, 1e-1, log=True)\n",
    "#     l2_3=trial.suggest_float('l2_3', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     activation_functions = ['gelu', 'leaky_relu', 'silu', 'mish']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "    \n",
    "    \n",
    "#     class DynamicPCA(keras.layers.Layer):\n",
    "#         def __init__(self, n_components, layer, update_freq=2, threshold=0.5, **kwargs):  \n",
    "#             super(DynamicPCA, self).__init__(**kwargs)\n",
    "#             self.n_components = n_components\n",
    "#             self.threshold = threshold\n",
    "#             self.layer = layer  \n",
    "#             self.update_freq = update_freq\n",
    "#             self.val_loss = None\n",
    "#             self.eigenvectors = None\n",
    "#             self.epoch_count = 0\n",
    "\n",
    "#         def build(self, input_shape):\n",
    "#             self.eigenvectors = self.add_weight(\n",
    "#                 shape=(input_shape[-1], self.n_components),\n",
    "#                 initializer=\"zeros\",\n",
    "#                 trainable=True,\n",
    "#             )\n",
    "\n",
    "#         def call(self, inputs):\n",
    "#             self.epoch_count += 1\n",
    "#             if (self.val_loss is not None and \n",
    "#                 self.val_loss < self.threshold and \n",
    "#                 self.epoch_count % self.update_freq == 0):\n",
    "#                 print(\"DynamicPCA: Aktualizacja wektorów własnych!\")\n",
    "#                 print(f\"val_loss: {self.val_loss}, threshold: {self.threshold}, epoch_count: {self.epoch_count}, update_freq: {self.update_freq}\")\n",
    "                \n",
    "#                 # Pobierz wagi bezpośrednio z warstwy\n",
    "#                 weights = self.layer.get_weights()  \n",
    "#                 flat_weights = tf.concat([tf.reshape(w, [-1]) for w in weights], axis=0)\n",
    "#                 # Normalizacja wag\n",
    "#                 flat_weights = tf.math.l2_normalize(flat_weights)\n",
    "#                 # Obliczanie nowych wektorów własnych\n",
    "#                 new_eigenvectors = self.calculate_pca(flat_weights)\n",
    "#                 # Aktualizacja wag (wektorów własnych)\n",
    "#                 self.eigenvectors.assign(new_eigenvectors)\n",
    "\n",
    "#             # Mnożenie przez wektory własne\n",
    "#             output = tf.matmul(inputs, self.eigenvectors)\n",
    "#             return output\n",
    "\n",
    "#         def calculate_pca(self, inputs):\n",
    "#             print(flat_weights.shape)\n",
    "#             # Obliczanie macierzy kowariancji\n",
    "#             covariance_matrix = tf.linalg.cov(inputs)  \n",
    "#             # Wektory i wartości własne\n",
    "#             eigenvalues, eigenvectors = tf.linalg.eigh(covariance_matrix)\n",
    "#             sorted_indices = tf.argsort(eigenvalues, direction='DESCENDING')\n",
    "#             eigenvectors = tf.gather(eigenvectors, sorted_indices, axis=1)\n",
    "#             return eigenvectors[:, :self.n_components]\n",
    "\n",
    "#     class MyCallback(keras.callbacks.Callback):\n",
    "#         def on_epoch_end(self, epoch, logs=None):\n",
    "#             pca_layer = self.model.get_layer('dynamic_pca')\n",
    "#             pca_layer.val_loss = logs.get('val_loss')\n",
    "#             print(logs.get('val_loss'))\n",
    "\n",
    "#     # Definiowanie wejścia\n",
    "#     image_input = keras.Input(shape=X_train_image.shape[1:])  \n",
    "\n",
    "#     # Warstwy konwolucyjne\n",
    "#     conv1 = keras.layers.Conv2D(num_filters1, kernel_size=(2, 2), activation=activation)(image_input)\n",
    "#     pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "#     conv2 = keras.layers.Conv2D(num_filters2, kernel_size=(2, 2), activation=activation)(pool1)\n",
    "#     pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "#     conv3 = keras.layers.Conv2D(num_filters3, kernel_size=(2, 2), activation=activation)(pool2)\n",
    "#     pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "#     flat_cnn = keras.layers.Flatten()(pool3)\n",
    "\n",
    "#     # Dense 1\n",
    "#     dense = keras.layers.Dense(units=trial.suggest_int('dense_units_1', 60, 240), activation=activation)(flat_cnn) \n",
    "#     dense = keras.layers.Dropout(rate=trial.suggest_float('dropout_rate_1', 0.01, 0.75))(dense)\n",
    "\n",
    "#     # Warstwa DynamicPCA \n",
    "#     pca_output = DynamicPCA(n_components=n_components, layer=conv3, name='dynamic_pca', threshold=1.25)(flat_cnn)  \n",
    "    \n",
    "#     merged = keras.layers.Concatenate()([dense, pca_output])\n",
    "\n",
    "#     dense2 = keras.layers.Dense(units=trial.suggest_int('dense_units_2', 60, 240), activation=activation)(merged)  \n",
    "#     dense2 = keras.layers.Dropout(rate=trial.suggest_float('dropout_rate_2', 0.01, 0.75))(dense2)\n",
    "\n",
    "#     # Output (klasyfikacja)\n",
    "#     outputs = keras.layers.Dense(10, activation='softmax')(dense2)\n",
    "\n",
    "#     # Utworzenie modelu\n",
    "#     model = keras.Model(inputs=image_input, outputs=outputs)\n",
    "\n",
    "#     # Utworzenie modelu\n",
    "#     model = keras.Model(inputs=image_input, outputs=outputs)\n",
    "#     #model.summary()\n",
    "#     tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    \n",
    "\n",
    "#     # Optymalizator\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "#     # Początkową wartość learning rate\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "#     ## Kompilacja modelu\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=10, start_from_epoch=8, min_delta=0.0001)\n",
    "    \n",
    "#     # ReduceLROnPlateau\n",
    "#     reduce_lr = ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.5, step=0.1),\n",
    "#         patience=trial.suggest_int(\"reduce_lr_patience\", 4, 8),\n",
    "#         min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-5, log=True)\n",
    "#     )\n",
    "    \n",
    "#     # Trenowanie i ocena modelu\n",
    "#     history = model.fit(X_train_image, y_train_image,\n",
    "#                         epochs=trial.suggest_int('epochs', 20, 80),\n",
    "#                         batch_size=trial.suggest_int('batch_size', 164, 768),\n",
    "#                         validation_data=(X_val_image, y_val_image),\n",
    "#                         validation_freq=1,\n",
    "#                         callbacks=[early_stopping, reduce_lr, MyCallback()]\n",
    "#                         )\n",
    "    \n",
    "#     accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) \n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model bazujący na dwóch potokach danych. Surowych przechodzących przez Conv2d, oraz LSTM dla danych z HOG i Gabor. Wykorzystanie mechanizmu atencji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "\n",
    "from tensorflow.keras.layers import ( Input, LSTM, GlobalAveragePooling1D, Permute, PReLU, Flatten, Reshape, Conv2D, MaxPooling2D, Concatenate, Attention, Dense,\n",
    "                                     LayerNormalization, ActivityRegularization, Dropout, GlobalAveragePooling2D, AveragePooling2D, AveragePooling1D, GlobalAveragePooling1D, \n",
    "                                     GlobalMaxPool1D, GlobalMaxPool2D, MaxPool2D, SeparableConv1D, RandomRotation, BatchNormalization\n",
    "                                    )\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import gc\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "X_train_reduced = np.load('X_train_reduced.npy')\n",
    "X_val_reduced = np.load('X_val_reduced.npy')\n",
    "y_train_filters = np.load('y_train_filters.npy')\n",
    "y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "X_train_image = X_train_image.astype('float32')\n",
    "X_val_image = X_val_image.astype('float32')\n",
    "X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    "\n",
    "class SpectralNorm(Constraint):\n",
    "    \"\"\"\n",
    "    Spectral Normalization constraint.\n",
    "\n",
    "    Args:\n",
    "        iteration: Number of iterations for power iteration.\n",
    "    \"\"\"\n",
    "    def __init__(self, iteration=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.iteration = iteration\n",
    "        self.u = None\n",
    "\n",
    "    def __call__(self, w):\n",
    "        w_shape = w.shape.as_list()\n",
    "        w = tf.reshape(w, [-1, w_shape[-1]])\n",
    "\n",
    "        if self.u is None:\n",
    "            self.u = tf.Variable(\n",
    "                initial_value=tf.keras.initializers.RandomNormal()(shape=[1, w_shape[-1]]),\n",
    "                trainable=False\n",
    "            )\n",
    "\n",
    "        if self.iteration == 1:\n",
    "            v_hat = tf.einsum('ij,kj->ik', self.u, w) \n",
    "            v_hat = v_hat / tf.linalg.norm(v_hat, ord=2, axis=-1, keepdims=True)\n",
    "            u_hat = tf.einsum('ij,jk->ik', v_hat, w)\n",
    "            u_hat = u_hat / tf.linalg.norm(u_hat, ord=2, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            def body(i, u_hat):\n",
    "                v_hat = tf.einsum('ij,kj->ik', u_hat, w)\n",
    "                v_hat = v_hat / tf.linalg.norm(v_hat, ord=2, axis=-1, keepdims=True)\n",
    "                u_hat = tf.einsum('ij,jk->ik', v_hat, w)\n",
    "                u_hat = u_hat / tf.linalg.norm(u_hat, ord=2, axis=-1, keepdims=True)\n",
    "                return i + 1, u_hat\n",
    "\n",
    "            _, u_hat = tf.while_loop(\n",
    "                lambda i, _: i < self.iteration,\n",
    "                body,\n",
    "                loop_vars=[0, self.u]\n",
    "            )\n",
    "\n",
    "        self.u.assign(u_hat)  # Aktualizacja self.u po pętli\n",
    "\n",
    "        w_norm = w / tf.linalg.norm(w, ord=2, axis=0, keepdims=True)\n",
    "        w_norm = tf.reshape(w_norm, w_shape)\n",
    "\n",
    "        return w_norm\n",
    "    \n",
    "def create_sepconv1d_layer(units, activation, l1, l2):\n",
    "    return SeparableConv1D(\n",
    "        units,\n",
    "        kernel_size=1,\n",
    "        padding='same',\n",
    "        activation=activation,\n",
    "        depthwise_constraint=SpectralNorm,\n",
    "        bias_regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    )\n",
    "\n",
    "def create_conv2d_layer(filters, activation, l1, l2):\n",
    "    return Conv2D(\n",
    "        filters,\n",
    "        kernel_size=(2, 2),\n",
    "        activation=activation,\n",
    "        kernel_constraint=SpectralNorm,\n",
    "        bias_regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    )    \n",
    "    \n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    # Dane wejściowe\n",
    "    image_input = Input(shape=X_train_image.shape[1:])\n",
    "    flattened_input = Input(shape=X_train_reduced.shape[1:])\n",
    "\n",
    "    ### Hiperparamatry \n",
    "    activation_functions = ['gelu', 'leaky_relu', 'silu', 'mish']\n",
    "    activation = trial.suggest_categorical('activation', activation_functions)\n",
    "        \n",
    "    filters1_1D=trial.suggest_int('Sep-Conv2d filters1', 32, 384)  # Pierwsza warstwa SeparableConv1D\n",
    "    filters2_1D=trial.suggest_int('Sep-Conv2d filters2', 32, 384)  # Druga warstwa SeparableConv1D\n",
    "    filters3_1D=trial.suggest_int('Sep-Conv2d filters3', 32, 384)  # Trzecia warstwa SeparableConv1D\n",
    "    \n",
    "    l1_1_1D=trial.suggest_float('l1_1_1D', 1e-6, 1e-1, log=True)\n",
    "    l2_1_1D=trial.suggest_float('l2_1_1D', 1e-6, 1e-1, log=True)\n",
    "    l1_2_1D=trial.suggest_float('l1_2_1D', 1e-6, 1e-1, log=True)\n",
    "    l2_2_1D=trial.suggest_float('l2_2_1D', 1e-6, 1e-1, log=True)\n",
    "    l1_3_1D=trial.suggest_float('l1_3_1D', 1e-6, 1e-1, log=True)\n",
    "    l2_3_1D=trial.suggest_float('l2_3_1D', 1e-6, 1e-1, log=True)\n",
    "       \n",
    "    filters1_2D = trial.suggest_int(\"Conv2d filters1\", 32, 384)    # Pierwsza warstwa Conv2d\n",
    "    filters2_2D = trial.suggest_int(\"Conv2d filters2\", 32, 384)    # Druga warstwa Conv2d\n",
    "    filters3_2D = trial.suggest_int(\"Conv2d filters3\", 32, 384)    # Trzecia warstwa Conv2d\n",
    "\n",
    "    l1_1_2D=trial.suggest_float('l1_1_2D', 1e-6, 1e-1, log=True)\n",
    "    l2_1_2D=trial.suggest_float('l2_1_2D', 1e-6, 1e-1, log=True)\n",
    "    l1_2_2D=trial.suggest_float('l1_2_2D', 1e-6, 1e-1, log=True)\n",
    "    l2_2_2D=trial.suggest_float('l2_2_2D', 1e-6, 1e-1, log=True)\n",
    "    l1_3_2D=trial.suggest_float('l1_3_2D', 1e-6, 1e-1, log=True)\n",
    "    l2_3_2D=trial.suggest_float('l2_3_2D', 1e-6, 1e-1, log=True)\n",
    "\n",
    "    # Gałąź SeparableConv1D\n",
    "    Sep_Conv1D_1 = create_sepconv1d_layer(filters1_1D, activation, l1_1_1D, l2_1_1D)(flattened_input)\n",
    "    Sep_Conv1D_2 = create_sepconv1d_layer(filters2_1D, activation, l1_2_1D, l2_2_1D)(Sep_Conv1D_1)\n",
    "    Sep_Conv1D_3 = create_sepconv1d_layer(filters3_1D, activation, l1_3_1D, l2_3_1D)(Sep_Conv1D_2)\n",
    "    # Pooling \n",
    "    Sep_pool_Conv1D = GlobalAveragePooling1D()(Sep_Conv1D_3)    \n",
    "\n",
    "    # Gałąź Conv2D\n",
    "        ## Pierwsza warstwa\n",
    "    conv1 = create_conv2d_layer(filters1_2D, activation, l1_1_2D, l2_1_2D)(image_input)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        ## Druga warstwazz\n",
    "    conv2 = create_conv2d_layer(filters2_2D, activation, l1_2_2D, l2_2_2D)(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        ## Trzecia warstwa\n",
    "    conv3 = create_conv2d_layer(filters3_2D, activation, l1_3_2D, l2_3_2D)(pool2)\n",
    "    pool3 = MaxPool2D(pool_size=(3, 3))(conv3)  \n",
    "    \n",
    "    flat_cnn = Flatten()(pool3)\n",
    "\n",
    "    #Dense gałąź SeparableConv1D\n",
    "    dense1 = Dense(units=trial.suggest_int('dense SeparableConv1D', 32, 384), activation=activation)(Sep_pool_Conv1D)\n",
    "    dense1 = Dropout(rate=trial.suggest_float('dropout_rate_1', 0.0, 0.75))(dense1)  \n",
    "\n",
    "    #Dense gałąź Conv2D    \n",
    "    dense2 = Dense(units=trial.suggest_int('dense Conv2D', 32, 384), activation=activation)(flat_cnn)\n",
    "    dense2 = Dropout(rate=trial.suggest_float('dropout_rate_2', 0.0, 0.75))(dense2)\n",
    "    \n",
    "    # Połączenie gałęzi\n",
    "    merged = Concatenate()([dense1, dense2])\n",
    "\n",
    "    merged = Reshape((merged.shape[1], 1))(merged)\n",
    "\n",
    "    # Attention Layer\n",
    "    attention_output = Attention()([merged, merged])\n",
    "    attention_output = Reshape((attention_output.shape[1],))(attention_output)\n",
    "    \n",
    "    #Dense\n",
    "    dense3 = Dense(units=trial.suggest_int('dense przed Output', 36, 384), activation=activation)(attention_output)\n",
    "    dense3 = Dropout(rate=trial.suggest_float('dropout przed Output', 0.0, 0.75))(dense3)\n",
    "\n",
    "    #Output\n",
    "    output = Dense(10, activation='softmax')(dense3)\n",
    "    model = Model(inputs=[image_input, flattened_input], outputs=output)\n",
    "    \n",
    "    #tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    #model.summary()\n",
    "    \n",
    "    # ReduceLROnPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.6, step=0.1),\n",
    "        patience=trial.suggest_int(\"reduce_lr_patience\", 2, 14),\n",
    "        min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "    )\n",
    "\n",
    "    # Optymalizator\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['AdamW', 'Lion'])\n",
    "    optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "    # Początkową wartość learning rate\n",
    "    lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "    optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6, start_from_epoch=12, min_delta=0.0001)\n",
    "    #log_dir = \"logs/\"  # Katalog, w którym będą zapisywane dane TensorBoard\n",
    "    #tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1 )\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    history = model.fit(\n",
    "        [X_train_image, X_train_reduced], \n",
    "        y_train_filters,\n",
    "        epochs=trial.suggest_int('epochs', 20, 90),\n",
    "        batch_size=trial.suggest_int('batch_size', 164, 768),\n",
    "        validation_data=([X_val_image, X_val_reduced], y_val_filters),\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, reduce_lr],# , tensorboard\n",
    "    )\n",
    "\n",
    "    best_val_accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "    # Ewaluacja modelu na danych testowych\n",
    "    #loss, accuracy = model.evaluate([X_test, X_test], y_test_ENC, verbose=1)\n",
    "    #print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    del model\n",
    "    return best_val_accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=200, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n",
    "print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "#[I 2024-10-25 02:17:14,226] Trial 55 finished with value:  0.9590833187103271 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 23,  'Sep-Conv2d filters2': 58,  'Sep-Conv2d filters3': 85, 'l1_1_1D': 0.0002263330199021373,   'l2_1_1D': 4.2547366301065874e-06,  'l1_2_1D': 1.8831463765106328e-05,  'l2_2_1D': 2.6849906489521574e-06, 'l1_3_1D': 5.376750360962702e-06, 'l2_3_1D': 4.5507289814394546e-05, 'Conv2d filters1': 27,  'Conv2d filters2': 75, 'Conv2d filters3': 66,  'l1_1_2D': 0.03570924628671723,    'l2_1_2D': 0.00013702268815391088,'l1_2_2D': 0.004180188847206343,   'l2_2_2D': 0.015697748113667306,   'l1_3_2D': 1.1093102758419453e-06, 'l2_3_2D': 2.3039556898400452e-05, 'dense SeparableConv1D': 113, 'dropout_rate_1': 0.25988069757258775, 'dense Conv2D': 55,  'dropout_rate_2': 0.1729409318039701,   'dense przed Output': 213, 'dropout przed Output': 0.692278722477647,    'reduce_lr_factor': 0.1, 'reduce_lr_patience': 8, 'reduce_lr_min_lr': 1.2382721420683838e-05, 'optimizer': 'AdamW', 'lr_initial': 0.007665780845145814, 'epochs': 81, 'batch_size': 173}. Best is trial 55 with value: 0.9590833187103271.\n",
    "#[I 2024-10-25 05:23:25,094] Trial 82 finished with value:  0.9622499942779541 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 22,  'Sep-Conv2d filters2': 28,  'Sep-Conv2d filters3': 100, 'l1_1_1D': 6.186293277794836e-05,  'l2_1_1D': 0.0003913398305461908,   'l1_2_1D': 1.5927923474696962e-06,  'l2_2_1D': 7.009282960520817e-06,  'l1_3_1D': 7.676137877709806e-06, 'l2_3_1D': 0.00022663346915615475, 'Conv2d filters1': 24,  'Conv2d filters2': 74, 'Conv2d filters3': 92,  'l1_1_2D': 0.0993169614612255,     'l2_1_2D': 0.0006883072766765725, 'l1_2_2D': 0.0005616491418975441,  'l2_2_2D': 1.0689411586574192e-05, 'l1_3_2D': 4.1278686535916746e-05, 'l2_3_2D': 4.837146281888261e-06,  'dense SeparableConv1D': 155, 'dropout_rate_1': 0.0509776183623675,  'dense Conv2D': 29,  'dropout_rate_2': 0.024717745882008837, 'dense przed Output': 178, 'dropout przed Output': 0.7445169215013963, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 4, 'reduce_lr_min_lr': 6.725637729646196e-06,  'optimizer': 'AdamW', 'lr_initial': 0.01728017154953551,  'epochs': 85, 'batch_size': 191}. Best is trial 82 with value: 0.9622499942779541.\n",
    "#[I 2024-10-25 15:22:24,767] Trial 142 finished with value: 0.9635000228881836 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 22,  'Sep-Conv2d filters2': 28,  'Sep-Conv2d filters3': 107, 'l1_1_1D': 6.120269265952697e-05,  'l2_1_1D': 7.956576920829512e-06,   'l1_2_1D': 1.2494724596667829e-06,  'l2_2_1D': 7.6581362167544e-06,    'l1_3_1D': 0.0039961697402018225, 'l2_3_1D': 0.00021250335195961356, 'Conv2d filters1': 28,  'Conv2d filters2': 72, 'Conv2d filters3': 99 , 'l1_1_2D': 0.07544375319709569,    'l2_1_2D': 0.000936229620640654,  'l1_2_2D': 0.001250431002984649,   'l2_2_2D': 0.01324640444949233,    'l1_3_2D': 0.0012955495310992596,  'l2_3_2D': 6.618279886291623e-06,  'dense SeparableConv1D': 151, 'dropout_rate_1': 0.08650725184529029, 'dense Conv2D': 29,  'dropout_rate_2': 0.0500476697674726,   'dense przed Output': 205, 'dropout przed Output': 0.7360088478595872,   'reduce_lr_factor': 0.4, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 9.207463977521836e-07,  'optimizer': 'AdamW', 'lr_initial': 0.01209682198565378,  'epochs': 88, 'batch_size': 228}. Best is trial 142 with value: 0.9635000228881836.\n",
    "#[I 2024-11-01 00:07:23,390] Trial 24 finished with value:  0.9449999928474426 and parameters: {'activation': 'mish',        'Sep-Conv2d filters1': 123, 'Sep-Conv2d filters2': 245, 'Sep-Conv2d filters3': 289, 'l1_1_1D': 0.08989218874411514,    'l2_1_1D': 1.1472596089503972e-06,  'l1_2_1D': 0.0026057385621594867,   'l2_2_1D': 0.00018978883951458408, 'l1_3_1D': 1.191033349842715e-06, 'l2_3_1D': 0.00014292949482563498, 'Conv2d filters1': 308, 'Conv2d filters2': 81, 'Conv2d filters3': 326, 'l1_1_2D': 2.9840369417992957e-06, 'l2_1_2D': 0.0014037071972283193, 'l1_2_2D': 0.0002640492511862146,  'l2_2_2D': 0.08499221386872775,    'l1_3_2D': 0.020374031922578733,   'l2_3_2D': 0.00010028036351417339, 'dense SeparableConv1D': 379, 'dropout_rate_1': 0.0544686509758308,  'dense Conv2D': 138, 'dropout_rate_2': 0.09593865740145555,  'dense przed Output': 384, 'dropout przed Output': 0.6382663359384837, 'reduce_lr_factor': 0.3, 'reduce_lr_patience': 4, 'reduce_lr_min_lr': 7.741233462096205e-06,  'optimizer': 'AdamW', 'lr_initial': 0.006707953904631179, 'epochs': 65, 'batch_size': 257}. Best is trial 24 with value: 0.9449999928474426.\n",
    "#[I 2024-11-05 15:57:39,996] Trial 101 finished with value: 0.9495833516120911 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 148, 'Sep-Conv2d filters2': 32,  'Sep-Conv2d filters3': 63, 'l1_1_1D': 0.0004897725865476358,   'l2_1_1D': 0.0003049226492905848,   'l1_2_1D': 5.00894966144464e-06,    'l2_2_1D': 0.00014051697363473006, 'l1_3_1D': 0.051135669099956614,  'l2_3_1D': 0.0009950758155242705,  'Conv2d filters1': 52,  'Conv2d filters2': 58, 'Conv2d filters3': 249, 'l1_1_2D': 0.014787666006730659,   'l2_1_2D': 0.0005948328558539133, 'l1_2_2D': 5.343054689024963e-06,  'l2_2_2D': 4.5277461875969285e-06, 'l1_3_2D': 1.9548002422161054e-06, 'l2_3_2D': 0.0009410443070360079,  'dense SeparableConv1D': 319, 'dropout_rate_1': 0.2533943711642646,  'dense Conv2D': 311, 'dropout_rate_2': 0.10208101037728201,  'dense przed Output': 90,  'dropout przed Output': 0.481006546242034,   'reduce_lr_factor': 0.5, 'reduce_lr_patience': 2, 'reduce_lr_min_lr': 5.559247664816983e-05,  'optimizer': 'AdamW', 'lr_initial': 0.00964928371074687,  'epochs': 66, 'batch_size': 423}. Best is trial 101 with value: 0.9495833516120911.\n",
    "#[I 2024-11-06 02:29:49,053] Trial 121 finished with value: 0.9535833597183228 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 119, 'Sep-Conv2d filters2': 49,  'Sep-Conv2d filters3': 158, 'l1_1_1D': 0.00023280964689116444, 'l2_1_1D': 0.0008751788463496611,   'l1_2_1D': 0.04610519254480619,     'l2_2_1D': 2.4445376899197913e-06, 'l1_3_1D': 0.06955230495640688,   'l2_3_1D': 0.004678025581652909,   'Conv2d filters1': 37,  'Conv2d filters2': 54, 'Conv2d filters3': 253, 'l1_1_2D': 0.016409226434426858,  'l2_1_2D': 0.00015292748828121323, 'l1_2_2D': 1.177904875672071e-05,  'l2_2_2D': 2.6668468645182726e-06, 'l1_3_2D': 6.7648129463989756e-06, 'l2_3_2D': 0.0007509210891800366,  'dense SeparableConv1D': 347, 'dropout_rate_1': 0.2855797128973145,  'dense Conv2D': 368, 'dropout_rate_2': 0.07539865516979521,  'dense przed Output': 73,  'dropout przed Output': 0.5059616088424707, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 2, 'reduce_lr_min_lr': 3.887676551167433e-05, 'optimizer': 'AdamW', 'lr_initial': 0.00852437932688485, 'epochs': 73, 'batch_size': 407}. Best is trial 121 with value: 0.9535833597183228."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
