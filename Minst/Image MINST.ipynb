{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Przetwarzanie danych ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Wizualizacja ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Preprocessing ---\n",
    "from scipy.signal import correlate2d\n",
    "from scipy.stats import bartlett, levene, fligner\n",
    "from skimage.metrics import structural_similarity, variation_of_information, normalized_root_mse\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    silhouette_samples,\n",
    "    homogeneity_completeness_v_measure,\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    fowlkes_mallows_score,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "import umap\n",
    "\n",
    "# --- Modelowanie ---\n",
    "import optuna\n",
    "import optunahub\n",
    "from sklearn.cluster import FeatureAgglomeration, KMeans\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# --- Ewaluacja ---\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# --- Przetwarzanie obrazów ---\n",
    "import cv2 as cv2\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog, graycomatrix, graycoprops\n",
    "from skimage.filters import gabor_kernel, gabor\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import img_as_float32\n",
    "from skimage.transform import resize\n",
    "\n",
    "# --- Zrównoleglanie ---\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# --- Inne ---\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import sqlite3\n",
    "\n",
    "# --- Sklearn ---\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# --- Skimage ---\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from skimage.filters import gaussian, unsharp_mask\n",
    "from skimage.measure import find_contours\n",
    "from skimage.morphology import erosion, closing, opening,  disk\n",
    "\n",
    "# --- Dodatkowe importy ---\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"12\" \n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.base import BaseEstimator\n",
    "import warnings\n",
    "from typing import List, Dict, Optional, Any, Tuple, Union, Literal, Callable\n",
    "\n",
    "try:\n",
    "    import optunahub\n",
    "    OPTUNAHUB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNAHUB_AVAILABLE = False\n",
    "    print(\"Ostrzeżenie: Moduł optunahub nie jest zainstalowany. Używam domyślnego TPESampler.\")\n",
    "import warnings\n",
    "import traceback\n",
    "import gc\n",
    "from typing import List, Dict, Optional, Any, Tuple, Union, Literal, Callable\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from esda.moran import Moran\n",
    "from libpysal.weights import lat2W\n",
    "from libpysal.weights import KNN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import entropy\n",
    "import statsmodels.api as sm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from IPython.display import display, Markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('mnist_train.csv')\n",
    "test  = pd.read_csv('mnist_test.csv')\n",
    "train.info()\n",
    "test.info()\n",
    "\n",
    "# Podział na cechy i etykiety\n",
    "X_train = train.drop('label', axis=1).values.reshape(-1, 28, 28)\n",
    "y_train = train['label'].values\n",
    "\n",
    "X_test = test.drop('label', axis=1).values.reshape(-1, 28, 28)\n",
    "y_test = test['label'].values\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Wizualizacja podstawowych cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_statistics(X, label):\n",
    "    \"\"\"Oblicza i wizualizuje statystyki dla danej klasy.\"\"\"\n",
    "\n",
    "    # Obliczenie statystyk\n",
    "    mean_pixels = np.mean(X, axis=0)\n",
    "    std_pixels = np.std(X, axis=0)\n",
    "    median_pixels = np.median(X, axis=0)\n",
    "    q25_pixels = np.quantile(X, 0.25, axis=0)\n",
    "    q75_pixels = np.quantile(X, 0.75, axis=0)\n",
    "    min_pixels = np.mean(np.percentile(X, [0, 2.5], axis=0), axis=0)\n",
    "    max_pixels = np.mean(np.percentile(X, [97.5, 100], axis=0), axis=0)\n",
    "\n",
    "    # Wizualizacja wszystkich statysty\n",
    "    _, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "\n",
    "    sns.heatmap(mean_pixels, cmap='gray', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f\"Średnia\")\n",
    "\n",
    "    sns.heatmap(std_pixels, cmap='viridis', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(f\"Odchylenie std.\")\n",
    "\n",
    "    axes[0, 2].imshow(median_pixels, cmap='gray')\n",
    "    axes[0, 2].set_title(\"Mediana\")\n",
    "    axes[0, 2].axis('off')\n",
    "\n",
    "    axes[0, 3].imshow(q25_pixels, cmap='gray')\n",
    "    axes[0, 3].set_title(\"Kwartyl 25%\")\n",
    "    axes[0, 3].axis('off')\n",
    "\n",
    "    axes[1, 0].imshow(q75_pixels, cmap='gray')\n",
    "    axes[1, 0].set_title(\"Kwartyl 75%\")\n",
    "    axes[1, 0].axis('off')\n",
    "\n",
    "    axes[1, 1].imshow(min_pixels, cmap='gray')\n",
    "    axes[1, 1].set_title(\"2.5% minimum\")\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    axes[1, 2].imshow(max_pixels, cmap='gray')\n",
    "    axes[1, 2].set_title(\"2.5% maksimum\")\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "    axes[1, 3].hist(X.flatten(), bins=15)\n",
    "    axes[1, 3].set_title(f\"Histogram\\nwartości pikseli\")\n",
    "    axes[1, 3].set_xlabel('Wartość piksela')\n",
    "    axes[1, 3].set_ylabel('Liczba wystąpień')\n",
    "\n",
    "    plt.suptitle(f\"Etykieta: {label}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Grupowanie po etykietach w zbiorze treningowym\n",
    "for label in range(10):\n",
    "    X_train_group = X_train[y_train == label]\n",
    "    X_test_group = X_test[y_test == label]\n",
    "\n",
    "    # Wizualizacja statystyk dla zbioru treningowego\n",
    "    print(\"Zbiór treningowy:\")\n",
    "    visualize_statistics(X_train_group, label)\n",
    "\n",
    "    # Wizualizacja statystyk dla zbioru testowego\n",
    "    print(\"\\nZbiór testowy:\")\n",
    "    visualize_statistics(X_test_group, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porownaj_srednie_obrazow(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Porównuje średnie obrazy dla każdej klasy w zbiorze treningowym i testowym \n",
    "    za pomocą trzech metryk: SSIM, NRMSE i Variation of Information.\n",
    "\n",
    "    Args:\n",
    "        X_train: Zbiór treningowy obrazów (numpy array).\n",
    "        y_train: Etykiety dla zbioru treningowego (numpy array).\n",
    "        X_test: Zbiór testowy obrazów (numpy array).\n",
    "        y_test: Etykiety dla zbioru testowego (numpy array).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame zawierający wyniki dla każdej metryki i klasy.\n",
    "    \"\"\"\n",
    "\n",
    "    wyniki = {}\n",
    "    for label in range(10):\n",
    "        X_train_group = X_train[y_train == label]\n",
    "        X_test_group = X_test[y_test == label]\n",
    "\n",
    "        # Obliczenie średnich obrazów\n",
    "        sredni_obraz_train = np.mean(X_train_group, axis=0)\n",
    "        sredni_obraz_test = np.mean(X_test_group, axis=0)\n",
    "\n",
    "        # Obliczenie metryk\n",
    "        ssim = structural_similarity(sredni_obraz_train, sredni_obraz_test, data_range=sredni_obraz_train.max() - sredni_obraz_train.min())\n",
    "        nrmse = normalized_root_mse(sredni_obraz_train, sredni_obraz_test)\n",
    "\n",
    "        # Variation of Information\n",
    "        vi = variation_of_information(sredni_obraz_train.astype(int), sredni_obraz_test.astype(int)) \n",
    "\n",
    "        wyniki[label] = {\n",
    "            'ssim': ssim,\n",
    "            'nrmse': nrmse,\n",
    "            'vi': vi[0],  \n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(wyniki).transpose()  # Utwórz DataFrame i transponuj go\n",
    "\n",
    "wyniki_df = porownaj_srednie_obrazow(X_train, y_train, X_test, y_test)\n",
    "display(wyniki_df)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# SSIM\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x=wyniki_df.index, y=wyniki_df['ssim'], hue=wyniki_df.index, palette=\"viridis\", legend=False)\n",
    "plt.title('SSIM')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Wartość SSIM')\n",
    "plt.ylim(0.97, 1)\n",
    "for i, v in enumerate(wyniki_df['ssim']):\n",
    "    plt.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# NRMSE\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x=wyniki_df.index, y=wyniki_df['nrmse'], hue=wyniki_df.index, palette=\"viridis\", legend=False)\n",
    "plt.title('NRMSE')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Wartość NRMSE')\n",
    "\n",
    "for i, v in enumerate(wyniki_df['nrmse']):\n",
    "    plt.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# VI\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x=wyniki_df.index, y=wyniki_df['vi'], hue=wyniki_df.index, palette=\"viridis\", legend=False)\n",
    "plt.title('Variation of Information')\n",
    "plt.xlabel('Klasa')\n",
    "plt.ylabel('Wartość VI')\n",
    "\n",
    "for i, v in enumerate(wyniki_df['vi']):\n",
    "    plt.text(i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podsumowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyżej przedstawiono analizę podstawowych własności zbioru danych MINST, skupiając się na zestawieniu średnich obrazów należących do poszczególnych cyfr (etykiet) w zbiorach treningowym i testowym. \\\n",
    "Na początek wizualizacja średnich wartości obrazów, dla etykiet będących w danych zbiorach. Przedstawiono statystyki opisujące wartość średnią, odchylenie standardowem medianę, kwartyle (25% i 75%), 2.5% minimum, 2.5% maksimum oraz histogram wartości pikseli, dla lokaliacji przestrzennej danego piksela. \\ \n",
    "W kolejnym etapie podobie dla średnich obrazów obliczono statystyki pomiędzędzy zbiorem treningowy a testowym: \n",
    "* SSIM, który mierzy strukturalne podobieństwo między dwoma (średnimi/wypadkowymi) obrazami. Wartość 1 oznacza idealne dopasowanie. \n",
    "* NRMSE, czyli błąd średniokwadratowy. Niższe wartości NRMSE wskazują na mniejszy błąd i większe podobieństwo. \n",
    "* VI, który informuje o ilości informacji utraconej lub uzyskanej. Niższe wartości VI wskazują na większe podobieństwo informacyjne.\n",
    "\n",
    "Wizualna anliza statystyk dla obrazów nie przedstawia znaczących różnic po między zbiorami. Dostrzec można różnicę w obrazach gdzie wykreślone jest odchylenie standardowe. Największe różnice widoczne są w przypadku 2.5% kwantyla z wartości maksymalnych. Większa przestrzeń pikseli o maksymalnej jasnośości wystepuje w zbiorze treningowym względem zbioru \"testowego\". Można przypuszczać, żę wynika to z tego ze w związku z większą liczbą elementów w zbiorze treningowym, można spotkać więcej etykiet które znajdują się na skrajach granic ograniczjąych. \\\n",
    "W przypadku statystyk dla średnich obrazów. Warttości wszystkich etykiet obliczonych dla SSIM są bardzo wysokie, co wyrażnie sugeruje duże struktualne podobieństwo pomiędzy średninimi obrazami. W przypadku VI możemy zauważyć coś na pierszy, rzut dość nie prawdopodobnego, mianowicie wartość dla cyfry 1, wynosi około 0.41, podczas gdy dla pozostały jest w zakresie około 0.8-0.95. Należy mieć jednak na uwadze, że nie wskazuje to znacznie większego podobiebieństwa wzklędem pozostałych etykiet. Za to na mniejszą ilości informacji potrzebnych na opisanie względnych róznic w średnim obrazie pomiędzy dwoma zbiorami. Można przypuszczać, że liczba \"1\" z względu na swój kształtm, który praktycznie jest pionową linią, wymga mniejszje reprezentacji przestrzennej, a tym samym może to przekłądać się na mniejszą wariację przestrzenną pomiędzy zbiorami. W związku z czym VI należy interpretować w kontekście waiancji danych. Natomiast z względu na uzupełnienie tego metrykami SSIM i NRMSE, daje to inny, bardziej związany z teorią informacji wgląd na podobieństwo między zbiorami treningowym i testowym. \\ \n",
    "\\\n",
    "Analiza wykazała duże podobieństwo między zbiorem treningowym i testowym MNIST pod względem średniego rozkładu przestrzennego wartości pikseli dla poszczególnych cyfr. Zarówno wizualizacje statystyk opisowych, jak i metryki porównujące średnie obrazy (SSIM, NRMSE, VI) potwierdzają tę obserwację. Wskazuje to na wysokie podobieństwo między zbiorem treningowym, a testowym i sugeruje że zbiór testowy jest reprezentatywny dla zbioru treningowego. Więc modele uczenia maszynowego wytrenowane na zbiorze treningowym powinny dobrze generalizować na nowe, niewidziane dane ze zbioru testowego. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza miar zależności przestrzennych, oraz tekstur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_glcm_features(image, distances=[1], angles=[0, np.pi / 4, np.pi / 2, 3 * np.pi / 4]):\n",
    "    \"\"\"Oblicza cechy tekstury z macierzy GLCM dla danego obrazu.\"\"\"\n",
    "\n",
    "    glcm = graycomatrix(\n",
    "        image,\n",
    "        distances=distances,\n",
    "        angles=angles,\n",
    "        levels=256,\n",
    "        symmetric=True,\n",
    "        normed=True,\n",
    "    )\n",
    "\n",
    "    # Obliczenie wszystkich cech GLCM jednocześnie\n",
    "    features = {}\n",
    "    props = [\"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\", \"correlation\", \"ASM\"]\n",
    "    for prop in props:\n",
    "      features[prop] = graycoprops(glcm, prop)[0, 0]\n",
    "\n",
    "    # Dodanie wariancji i entropii\n",
    "    features[\"variance\"] = np.var(image)\n",
    "    hist = np.histogram(image, bins=256, range=(0, 256))[0]\n",
    "    features[\"entropy\"] = entropy(hist)\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_spatial_autocorrelation(image, nlags=10):\n",
    "    \"\"\"Oblicza średnią i odchylenie standardowe autokorelacji przestrzennej.\"\"\"\n",
    "    autocorr = sm.tsa.acf(image.flatten(), nlags=min(nlags, image.size - 1), fft=True)\n",
    "    return np.mean(autocorr), np.std(autocorr)\n",
    "\n",
    "\n",
    "def calculate_moran_index(image):\n",
    "    \"\"\"Oblicza indeks Morana dla danego obrazu.\"\"\"\n",
    "    w = lat2W(*image.shape)\n",
    "    moran_i = Moran(image.flatten(), w)\n",
    "    return moran_i.I\n",
    "\n",
    "\n",
    "def calculate_spatial_features(images, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Oblicza miary zależności przestrzennych i cechy tekstur dla obrazów.\n",
    "\n",
    "    Args:\n",
    "        images: Tablica obrazów w skali szarości.\n",
    "        n_jobs: Liczba rdzeni do użycia w Parallel. -1 oznacza użycie wszystkich dostępnych rdzeni.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame z obliczonymi cechami.\n",
    "    \"\"\"\n",
    "\n",
    "    def _calculate_features(image_id, image):\n",
    "        \"\"\"Funkcja wewnętrzna do obliczania cech dla pojedynczego obrazu.\"\"\"\n",
    "\n",
    "        # Konwersja obrazu do typu uint8 w zakresie 0-255\n",
    "        if image.dtype != np.uint8:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "\n",
    "        # Miary zależności przestrzennych\n",
    "        autocorr_mean, autocorr_std = calculate_spatial_autocorrelation(image)\n",
    "        moran_i_mean = calculate_moran_index(image)\n",
    "\n",
    "        # Analiza tekstur\n",
    "        glcm_features = calculate_glcm_features(image)\n",
    "\n",
    "        return {\n",
    "            \"image_id\": image_id,\n",
    "            \"autocorr_mean\": autocorr_mean,\n",
    "            \"autocorr_std\": autocorr_std,\n",
    "            \"moran_I\": moran_i_mean,\n",
    "            **glcm_features,\n",
    "        }\n",
    "\n",
    "    features = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_calculate_features)(image_id, image)\n",
    "        for image_id, image in enumerate(images)\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "# 1. Obliczenia dla całego zbioru treningowego\n",
    "features_original = calculate_spatial_features(X_train)\n",
    "features_original['label'] = y_train\n",
    "\n",
    "# 2. Statystyki opisowe dla całego zbioru\n",
    "print(\"Statystyki dla całego zbioru:\")\n",
    "display(features_original.drop(columns=['image_id', 'label']).describe())\n",
    "\n",
    "# 3. Statystyki opisowe i wykresy dla poszczególnych cyfr\n",
    "features_by_label = []\n",
    "for label in range(10):\n",
    "    features_label = features_original[features_original['label'] == label].copy()\n",
    "    features_by_label.append(features_label)\n",
    "    print(f\"\\nStatystyki dla cyfry {label}:\")\n",
    "    display(features_label.drop(columns=['image_id', 'label']).describe())\n",
    "\n",
    "# 4. Połączenie danych dla wizualizacji\n",
    "combined_features = pd.concat(features_by_label, ignore_index=True)\n",
    "\n",
    "# 5. Wizualizacja\n",
    "features_to_plot = [\"moran_I\", \"autocorr_mean\", \"autocorr_std\", \"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\", \"correlation\", \"ASM\", \"variance\", \"entropy\"]\n",
    "\n",
    "# Wykresy tylko dla poszczególnych cyfr (bez całego zbioru)\n",
    "for feature in features_to_plot:\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.violinplot(x='label', y=feature, data=combined_features)\n",
    "    plt.title(f\"Rozkład cechy '{feature}' dla różnych cyfr\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja wartości i wektrów własnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_eigen(X, label):\n",
    "    \"\"\"Analizuje i wizualizuje wartości i wektory własne dla danej klasy,\n",
    "    wraz ze skumulowaną wariancją.\"\"\"\n",
    "\n",
    "    # Spłaszczenie danych\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    # Obliczenie macierzy kowariancji\n",
    "    cov_matrix = np.cov(X_flat, rowvar=False)\n",
    "\n",
    "    # Obliczenie wartości i wektorów własnych\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # Sortowanie wartości własnych\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # Obliczenie skumulowanej wariancji\n",
    "    total_variance = np.sum(eigenvalues)\n",
    "    explained_variance_ratio = eigenvalues / total_variance\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    num_eigenvectors_to_display = 9\n",
    "    \n",
    "    # Siatka dla wykresów\n",
    "    fig = plt.figure(figsize=(24, 12))\n",
    "    gs = fig.add_gridspec(3, num_eigenvectors_to_display, height_ratios=[2, 2, 1])\n",
    "\n",
    "    # Wykres wartości własnych\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Numer wartości własnej')\n",
    "    ax1.set_ylabel('Wartość własna', color=color)\n",
    "    ax1.plot(eigenvalues, marker='o', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True)\n",
    "    ax1.set_title(f\"Wartości własne, wektory własne i skumulowana wariancja dla etykiety {label}\")\n",
    "\n",
    "    # Utworzenie drugiej osi Y dla powiększenia\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Wartość własna (powiększenie)', color=color)\n",
    "    ax2.plot(range(200, len(eigenvalues)), eigenvalues[200:], marker='o', color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Wykres skumulowanej wariancji\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    color = 'tab:green'\n",
    "    ax3.set_xlabel('Liczba składowych')\n",
    "    ax3.set_ylabel('Skumulowana wariancja', color=color)\n",
    "    ax3.plot(cumulative_variance, marker='o', color=color)\n",
    "    ax3.tick_params(axis='y', labelcolor=color)\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Dodanie linii poziomej na wykresie skumulowanej wariancji dla 95%\n",
    "    ax3.axhline(y=0.98, color='r', linestyle='--')\n",
    "    ax3.text(0, 0.98, '98%', color='r', va='bottom', ha='left')\n",
    "\n",
    "    # Wizualizacja wektorów własnych\n",
    "    for i in range(num_eigenvectors_to_display):\n",
    "        ax = fig.add_subplot(gs[2, i])\n",
    "        ax.imshow(eigenvectors[:, i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(f\"Wektor własny {i+1}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iteracja po etykietach\n",
    "for label in range(10):\n",
    "    X_train_group = X_train[y_train == label]\n",
    "    analyze_eigen(X_train_group, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podsumowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdźmy do analizy wartości i wektorów własnych, obliczonych dla każdej cyfry (etykiety) w zbiorze treningowym MNIST. Wartości własne informują nas o 'znaczeniu' odpowiadających im składowych głównych (wektorów własnych) i pozwalają określić, ile z nich jest potrzebnych do opisania większości wariancji danych. Innymi słowy, większa wartość własna oznacza, że odpowiadająca jej składowa główna wyjaśnia większą część zmienności w danych. Z definicji, wektor własny to taki wektor, który po przemnożeniu przez macierz zachowuje swój kierunek, zmieniając co najwyżej swoją długość. \n",
    "\n",
    "Wektory własne macierzy kowariancji, ianczej składowe główne pozwalają wskazać kierunki największej wariancji w przestrzeni danych 784-wymiarowej (28x28 pikseli) obrazów z boiru MINST. Wartość własna informuje jak duża jest natomiast wariancja danych wzdłuż odpowiadającego jej wektora własnego, czyli składowej głównej. Po transformacji danych do przestrzeni wyznaczonej przez wektory własne, współrzędne wektora wzdłuż odpowiadającego mu kierunku są przemnożone przez wartość własną. \n",
    "\n",
    "Analizując wykres wartości własny możemy zoabczyć, że znacząca część wariancji danych jest wyjaśniona przez pierwsze 100 składowych głównych. Dodatkowo, wszystkie etykiety poza \"1\" potrzebują około 500 wartości własnych (czerwona linia na wykresie), a etykieta \"1\" 400 do wyjaśnienia większości wariancji co potwierdza wcześniejsze przypuszczenia. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metody składowych głównych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality_to_k(X_train, y_train, method_name, n_components):\n",
    "    \"\"\"\n",
    "    Redukuje wymiarowość danych MNIST za pomocą określonej metody do k wymiarów.\n",
    "\n",
    "    Args:\n",
    "        X_train: Dane treningowe MNIST.\n",
    "        y_train: Etykiety danych treningowych.\n",
    "        method_name: Nazwa metody redukcji wymiarów.\n",
    "        n_components: Liczba wymiarów, do których należy zredukować dane.\n",
    "\n",
    "    Returns:\n",
    "        Dane po redukcji wymiarów, czas trwania operacji.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "    methods = {\n",
    "        \"PCA\": PCA,\n",
    "        \"KernelPCA\": KernelPCA,\n",
    "        \"t-SNE\": TSNE,\n",
    "        \"UMAP\": umap.UMAP,\n",
    "        \"LLE\": LocallyLinearEmbedding,\n",
    "        \"LDA\": LinearDiscriminantAnalysis,\n",
    "    }\n",
    "\n",
    "    if method_name not in methods:\n",
    "        raise ValueError(f\"Nieznana metoda: {method_name}\")\n",
    "\n",
    "    # Ustawienie parametrów dla każdej metody\n",
    "    method_params = {\n",
    "        \"PCA\": {\"n_components\": n_components, \"random_state\": 42},\n",
    "        \"KernelPCA\": {\"n_components\": n_components, \"kernel\": \"cosine\", \"random_state\": 42},\n",
    "        \"t-SNE\": {\"n_components\": n_components, \"random_state\": 42, \"perplexity\": 30},\n",
    "        \"UMAP\": {\"n_components\": n_components, \"random_state\": 42, \"n_neighbors\": 42},\n",
    "        \"LLE\": {\"n_components\": n_components, \"random_state\": 42, \"n_neighbors\": 42,  'method': 'ltsa', \"eigen_solver\": \"dense\"},\n",
    "        \"LDA\": {\"n_components\": 3},\n",
    "    }\n",
    "\n",
    "    method = methods[method_name](**method_params[method_name])\n",
    "\n",
    "    start_time = time.time()\n",
    "    if method_name == \"LDA\":\n",
    "        X_transformed = method.fit_transform(X_train_flat, y_train)\n",
    "    else:\n",
    "        X_transformed = method.fit_transform(X_train_flat)\n",
    "    end_time = time.time()\n",
    "\n",
    "    return X_transformed, end_time - start_time\n",
    "\n",
    "\n",
    "def cluster_and_evaluate(X, y_true, clustering_method, method_name=\"Raw Data\", skip_silhouette=False):\n",
    "    \"\"\"\n",
    "    Przeprowadza klasteryzację, ocenia jej jakość i zwraca wyniki w formie słownika.\n",
    "\n",
    "    Args:\n",
    "        X: Dane (surowe lub po redukcji).\n",
    "        y_true: Prawdziwe etykiety.\n",
    "        clustering_method: Nazwa metody klasteryzacji.\n",
    "        method_name: Nazwa metody redukcji wymiarów (domyślnie \"Raw Data\").\n",
    "        skip_silhouette: Czy pominąć obliczanie silhouette_score (domyślnie False).\n",
    "\n",
    "    Returns:\n",
    "        Słownik z wynikami.\n",
    "    \"\"\"\n",
    "\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    clusterers = {\n",
    "        \"KMeans\": KMeans(n_clusters=10, random_state=42, n_init=\"auto\"),\n",
    "        \"GMM\": GaussianMixture(n_components=10, random_state=42),\n",
    "        \"Agglomerative\": AgglomerativeClustering(n_clusters=10),\n",
    "        \"DBSCAN\": DBSCAN(eps=0.5, min_samples=5), \n",
    "        \"Ward\": AgglomerativeClustering(n_clusters=10, linkage='ward')\n",
    "    }\n",
    "\n",
    "    if clustering_method not in clusterers:\n",
    "        raise ValueError(f\"Nieznana metoda klasteryzacji: {clustering_method}\")\n",
    "    \n",
    "    # Obsługa przypadku, gdy danych jest zbyt mało dla OPTICS lub DBSCAN\n",
    "    if (clustering_method == \"OPTICS\" or clustering_method == \"DBSCAN\") and X_flat.shape[0] < 10:\n",
    "        print(f\"Warning: Zbyt mało danych dla {clustering_method} po redukcji {method_name}. Pomijanie {clustering_method}.\")\n",
    "        return {\n",
    "            \"Reduction Method\": method_name,\n",
    "            \"Clustering Method\": clustering_method,\n",
    "            \"Silhouette Score (Clustering)\": None,\n",
    "            \"Adjusted Rand Score\": None,\n",
    "            \"Adjusted Mutual Information\": None,\n",
    "            \"Fowlkes-Mallows Score\": None,\n",
    "            \"Homogeneity\": None,\n",
    "            \"Completeness\": None,\n",
    "            \"V-Measure\": None,\n",
    "            \"Clustering Time (s)\": 0,\n",
    "            \"Reduction Time (s)\": 0 if method_name == \"Raw Data\" else None,\n",
    "            \"y_pred\": None,\n",
    "        }\n",
    "\n",
    "    clusterer = clusterers[clustering_method]\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    y_pred = clusterer.fit_predict(X_flat)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    n_labels = len(np.unique(y_pred))\n",
    "\n",
    "    ars = adjusted_rand_score(y_true, y_pred)\n",
    "    amis = adjusted_mutual_info_score(y_true, y_pred)\n",
    "    fms = fowlkes_mallows_score(y_true, y_pred)\n",
    "    homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(y_true, y_pred)\n",
    "\n",
    "    if n_labels <= 1:\n",
    "        print(f\"Warning: {clustering_method} found only one cluster. Silhouette score set to None.\")\n",
    "        silhouette = None\n",
    "    elif (method_name == \"Raw Data\" and skip_silhouette) or (method_name == \"Raw Data\" and not skip_silhouette):\n",
    "        print(\"Pominięto obliczanie silhouette_score dla surowych danych.\")\n",
    "        silhouette = None\n",
    "    elif method_name != \"Raw Data\":\n",
    "        # Użyto mniejszej podpróbki (5000) dla silhouette_score\n",
    "        if X_flat.shape[0] > 5000:\n",
    "            sample_size = 10000\n",
    "            indices = np.random.choice(X_flat.shape[0], sample_size, replace=False)\n",
    "            X_sample = X_flat[indices]\n",
    "            y_sample = y_pred[indices]\n",
    "            silhouette = silhouette_score(X_sample, y_sample)\n",
    "        else:\n",
    "            silhouette = silhouette_score(X_flat, y_pred)\n",
    "    else:\n",
    "        silhouette = None\n",
    "\n",
    "    return {\n",
    "        \"Reduction Method\": method_name,\n",
    "        \"Clustering Method\": clustering_method,\n",
    "        \"Silhouette Score (Clustering)\": silhouette,\n",
    "        \"Adjusted Rand Score\": ars,\n",
    "        \"Adjusted Mutual Information\": amis,\n",
    "        \"Fowlkes-Mallows Score\": fms,\n",
    "        \"Homogeneity\": homogeneity,\n",
    "        \"Completeness\": completeness,\n",
    "        \"V-Measure\": v_measure,\n",
    "        \"Clustering Time (s)\": time_taken,\n",
    "        \"Reduction Time (s)\": 0 if method_name == \"Raw Data\" else None,\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "\n",
    "method_names = [\"PCA\", \"KernelPCA\", \"LLE\", \"LDA\", \"t-SNE\", \"UMAP\"]\n",
    "clustering_methods = [\"KMeans\", \"GMM\"]\n",
    "results = []\n",
    "\n",
    "# Opcje\n",
    "n_components = 3 \n",
    "cluster_on_raw_data = False\n",
    "skip_silhouette_on_raw = False\n",
    "\n",
    "# Klasteryzacja na surowych danych\n",
    "if cluster_on_raw_data:\n",
    "    print(\"\\n--- Klasteryzacja na surowych danych ---\\n\")\n",
    "    for clustering_method in clustering_methods:\n",
    "        print(f\"Rozpoczynam klasteryzację: {clustering_method}\")\n",
    "        result = cluster_and_evaluate(\n",
    "            X_train, y_train, clustering_method, \"Raw Data\", skip_silhouette_on_raw\n",
    "        )\n",
    "        print(f\"Zakończono klasteryzację. Czas: {result['Clustering Time (s)']:.2f}s\")\n",
    "        results.append(result)\n",
    "\n",
    "# Klasteryzacja na danych po redukcj\n",
    "print(\"\\n--- Klasteryzacja po redukcji wymiarów ---\\n\")\n",
    "for method_name in method_names:\n",
    "    if method_name == \"LDA\":\n",
    "        n_components_method = 9\n",
    "    else:\n",
    "        n_components_method = n_components\n",
    "\n",
    "    print(f\"Rozpoczynam obliczenia dla: {method_name} (redukcja do {n_components_method} wymiarów)\")\n",
    "    X_transformed, reduction_time = reduce_dimensionality_to_k(X_train, y_train, method_name, n_components_method)\n",
    "    print(f\"Zakończono redukcję wymiarów. Czas: {reduction_time:.2f}s, Wymiary po redukcji: {X_transformed.shape[1]}\")\n",
    "\n",
    "    # Wizualizacja dla t-SNE i UMAP\n",
    "    if method_name in [\"t-SNE\", \"UMAP\"] and X_transformed.shape[1] <= 3:\n",
    "        if X_transformed.shape[1] == 3:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            scatter = ax.scatter(X_transformed[:, 0], X_transformed[:, 1], X_transformed[:, 2], c=y_train, cmap='viridis')\n",
    "            legend1 = ax.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "            ax.add_artist(legend1)\n",
    "            plt.title(f\"{method_name} - Redukcja do 3 wymiarów\")\n",
    "            plt.show()\n",
    "\n",
    "    for clustering_method in clustering_methods:\n",
    "        print(f\"Rozpoczynam klasteryzację: {clustering_method}\")\n",
    "        result = cluster_and_evaluate(\n",
    "            X_transformed, y_train, clustering_method, f\"{method_name} ({n_components_method} wym.)\", skip_silhouette_on_raw\n",
    "        )\n",
    "        print(f\"Zakończono klasteryzację. Czas: {result['Clustering Time (s)']:.2f}s\")\n",
    "        result[\"Reduction Method\"] = f\"{method_name} ({n_components_method} wym.)\"\n",
    "        result[\"Reduction Time (s)\"] = reduction_time\n",
    "        results.append(result)\n",
    "\n",
    "# Wyniki\n",
    "results_df = pd.DataFrame(results).drop(\"y_pred\", axis=1)\n",
    "display(Markdown(\"### Zbiorcze wyniki klasteryzacji\"))\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdzmy do kolejnego zagadnienia związanego z redukcją wymiarów dla danych w zbiorze treningowym. . Celem eksperymentu było sprawdzenie, jak dobrze te metody radzą sobie z separacją klastrów odpowiadających poszczególnym cyfrom. \n",
    "* PCA (Principal Component Analysis): Podstawowaa lliniowa metoda redukcji wymiarowości, która identyfikuje kierunki największej wariancji w danych.\n",
    "* KernelPCA (Kernel Principal Component Analysis): Wersja PCA  w któej zasotosowano jądro kosinusowego, umożliwiająca nieliniową redukcję wymiarowości.\n",
    "* LLE (Locally Linear Embedding): Nieliniowa metoda, stara się zachować lokalne relacje sąsiedztwa między punktami danych.\n",
    "* LDA (Linear Discriminant Analysis): Metoda nadzorowana, która szuka liniowej kombinacji cech najlepiej separujących klasy (w tym przypadku cyfry).\n",
    "* t-SNE (t-distributed Stochastic Neighbor Embedding): Nieliniowa metoda, która skupia się na zachowaniu lokalnych struktur i tworzeniu wyraźnie oddzielonych klastrów.\n",
    "* UMAP (Uniform Manifold Approximation and Projection): Nieliniowa metoda, która stara się zachować zarówno lokalną, jak i globalną strukturę danych.\n",
    "\n",
    "Do oceny jakości redukcji wymiarowości użyto następujących metryk:\n",
    "\n",
    "* Silhouette Score: Miara jakości separacji klastrów (dla metod nienadzorowanych).\n",
    "* Homogeneity, Completeness, V-measure: Miary zgodności wyników klasteryzacji K-means z prawdziwymi etykietami.\n",
    "\n",
    "Na podstawie uzyskanych wyników oraz wizualizacji UMAP okazał się najskuteczniejszą metodą. Osiągnął on najwyższy silhouette score (0.462), co wskazuje na najlepszą separację klastrów w przestrzeni dwuwymiarowej. Ponadto, UMAP uzyskał najwyższe wartości homogeneity (0.855), completeness (0.865) i V-measure (0.860), co świadczy o najlepszej zgodności wyników klasteryzacji z prawdziwymi etykietami. Czas obliczeniowy UMAP (34.11 s) był stosunkowo krótki. UMAP zapewanił dobrą separację klastrów dla większości cyfr. Widoczne są punkty/etykiety, które znajdują się w skupiskach innych cyfr. Może to wskazywać na outliery, błędy w etykietach nietypowy sposób pisania danej cyfry, lub ograniczenia wynikające z redukcji do dwóch wymiarów.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testy statystyczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import (\n",
    "    bartlett,\n",
    "    levene,\n",
    "    fligner,\n",
    "    f_oneway,\n",
    "    kruskal,\n",
    "    shapiro,  # Dodany test Shapiro-Wilka\n",
    "    anderson  # Dodany test Andersona-Darlinga\n",
    ")\n",
    "from IPython.display import Markdown, display\n",
    "import time\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "\n",
    "def reduce_dimensionality(X_train, y_train, method_name, n_components=2):\n",
    "    \"\"\"\n",
    "    Redukuje wymiarowość danych za pomocą określonej metody (PCA lub UMAP).\n",
    "\n",
    "    Args:\n",
    "        X_train: Dane treningowe.\n",
    "        y_train: Etykiety danych treningowych.\n",
    "        method_name: Nazwa metody redukcji wymiarów ('PCA' lub 'UMAP').\n",
    "        n_components: Liczba komponentów do zachowania.\n",
    "\n",
    "    Returns:\n",
    "        X_transformed: Dane po redukcji wymiarów.\n",
    "        time_taken: Czas obliczeń.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method_name == \"PCA\":\n",
    "        method = PCA(n_components=n_components)\n",
    "        X_transformed = method.fit_transform(X_train_flat)\n",
    "\n",
    "        # Informacje o PCA są dostępne tylko po dopasowaniu\n",
    "        if n_components == 'mle':\n",
    "            n_components_actual = method.n_components_\n",
    "            print(f\"  Liczba komponentów wybrana przez MLE: {n_components_actual}\")\n",
    "        else:\n",
    "            n_components_actual = n_components\n",
    "\n",
    "        explained_variance_ratio = method.explained_variance_ratio_\n",
    "        print(f\"  Wyjaśniona wariancja przez {n_components_actual} składowe: {explained_variance_ratio.sum() * 100:.2f}%\")\n",
    "\n",
    "    elif method_name == \"UMAP\":\n",
    "        method = umap.UMAP(n_components=n_components, random_state=42)\n",
    "        X_transformed = method.fit_transform(X_train_flat)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Nieznana metoda: {method_name}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return X_transformed, time_taken\n",
    "\n",
    "def analyze_covariance(X, method_name):\n",
    "    \"\"\"\n",
    "    Oblicza i zwraca miary związane z macierzą kowariancji danych X.\n",
    "\n",
    "    Args:\n",
    "        X: Dane (po redukcji wymiarów).\n",
    "        method_name: Nazwa metody redukcji wymiarów.\n",
    "\n",
    "    Returns:\n",
    "        Ramkę danych (DataFrame) z wynikami.\n",
    "    \"\"\"\n",
    "    total_variance = np.sum(np.var(X, axis=0))\n",
    "\n",
    "    results = {\n",
    "        \"Method\": method_name,\n",
    "        \"Total Variance\": total_variance,\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([results])\n",
    "\n",
    "def perform_statistical_tests(X_reduced, y, method_name, n_components):\n",
    "    \"\"\"\n",
    "    Przeprowadza testy statystyczne na danych X_reduced pogrupowanych według etykiet y.\n",
    "\n",
    "    Args:\n",
    "        X_reduced: Dane po redukcji wymiarów.\n",
    "        y: Etykiety.\n",
    "        method_name: Nazwa metody redukcji wymiarów.\n",
    "        n_components: Liczba komponentów użytych do redukcji\n",
    "\n",
    "    Returns:\n",
    "        Ramki danych z wynikami testów.\n",
    "    \"\"\"\n",
    "    groups = [X_reduced[y == label] for label in np.unique(y)]\n",
    "    groups = [group for group in groups if group.shape[0] > 1]\n",
    "\n",
    "    if len(groups) < 2:\n",
    "        print(f\"Za mało grup do przeprowadzenia testów statystycznych dla metody {method_name}. Pozostało {len(groups)} grup.\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Test normalności - Anderson-Darling\n",
    "    normality_test_name = \"Anderson-Darling\"\n",
    "    normality_results = []\n",
    "    for i, group in enumerate(groups):\n",
    "        result = anderson(group.flatten())  # Spłaszczamy grupę do 1D\n",
    "        normality_results.append({\"statistic\": result.statistic, \"critical_values\": result.critical_values, \"significance_levels\": result.significance_level})\n",
    "\n",
    "    # Testy homogeniczności wariancji\n",
    "    homogeneity_tests = {\"Bartlett\": bartlett, \"Levene\": levene, \"Fligner-Killeen\": fligner}\n",
    "    homogeneity_results = {}\n",
    "    for name, test in homogeneity_tests.items():\n",
    "        try:\n",
    "            stat, p = test(*groups)\n",
    "            homogeneity_results[name] = {\"statistic\": stat, \"p-value\": p}\n",
    "        except ValueError as e:\n",
    "            print(f\"Błąd w teście {name}: {e}\")\n",
    "            homogeneity_results[name] = {\"statistic\": np.nan, \"p-value\": np.nan}\n",
    "\n",
    "    # Testy różnic między grupami\n",
    "    # Kruskal-Wallis - nieparametryczny, nie zakłada normalności\n",
    "    difference_tests = {\"Kruskal-Wallis\": kruskal}\n",
    "    difference_results = {}\n",
    "    for name, test in difference_tests.items():\n",
    "        try:\n",
    "            stat, p = test(*groups)\n",
    "            difference_results[name] = {\"statistic\": stat, \"p-value\": p}\n",
    "        except ValueError as e:\n",
    "            print(f\"Błąd w teście {name}: {e}\")\n",
    "            difference_results[name] = {\"statistic\": np.nan, \"p-value\": np.nan}\n",
    "\n",
    "    # Tworzenie ramek danych z wynikami\n",
    "    normality_df = pd.DataFrame(\n",
    "        [\n",
    "            f\"Stat: {res['statistic']:.3f}, Crit. values: {res['critical_values']}, Sig. levels: {res['significance_levels']}\"\n",
    "            for res in normality_results\n",
    "        ],\n",
    "        index=[f\"Group {i}\" for i in range(len(groups))],\n",
    "        columns=[normality_test_name]\n",
    "    )\n",
    "\n",
    "    homogeneity_df = pd.DataFrame(homogeneity_results).T\n",
    "    homogeneity_df.index.name = \"Test\"\n",
    "    homogeneity_df.reset_index(inplace=True)\n",
    "\n",
    "    difference_df = pd.DataFrame(difference_results).T\n",
    "    difference_df.index.name = \"Test\"\n",
    "    difference_df.reset_index(inplace=True)\n",
    "\n",
    "    # Wizualizacja rozkładów dla pierwszych dwóch wymiarów\n",
    "    if n_components >=2:\n",
    "        for i, group in enumerate(groups):\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            # Histogram\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(group[:, 0], bins=20)\n",
    "            plt.title(f\"Histogram - {method_name} - Group {i} - Dim 1\")\n",
    "\n",
    "            # Q-Q plot\n",
    "            plt.subplot(1, 2, 2)\n",
    "            qqplot(group[:, 0], line='s', ax=plt.gca())\n",
    "            plt.title(f\"Q-Q Plot - {method_name} - Group {i} - Dim 1\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return normality_df, homogeneity_df, difference_df\n",
    "\n",
    "# Uruchomienie analizy\n",
    "if __name__ == \"__main__\":\n",
    "    method_names = [\"PCA\", \"UMAP\"]\n",
    "    n_components_pca = 500\n",
    "    n_components_umap = 500\n",
    "\n",
    "    covariance_results = pd.DataFrame()\n",
    "    normality_tests_results = {}\n",
    "    homogeneity_tests_results = {}\n",
    "    difference_tests_results = {}\n",
    "\n",
    "    for method_name in method_names:\n",
    "        if method_name == \"PCA\":\n",
    "            n_components = n_components_pca\n",
    "        elif method_name == \"UMAP\":\n",
    "            n_components = n_components_umap\n",
    "        \n",
    "        print(f\"Metoda: {method_name}, Liczba komponentów: {n_components}\")\n",
    "        X_transformed, reduction_time = reduce_dimensionality(\n",
    "            X_train, y_train, method_name, n_components=n_components\n",
    "        )\n",
    "        print(f\"Czas redukcji wymiarowości: {reduction_time:.2f}s\")\n",
    "\n",
    "        # Analiza kowariancji\n",
    "        cov_analysis_df = analyze_covariance(X_transformed, method_name)\n",
    "        covariance_results = pd.concat([covariance_results, cov_analysis_df], ignore_index=True)\n",
    "\n",
    "        # Testy statystyczne\n",
    "        normality_df, homogeneity_df, difference_df = perform_statistical_tests(X_transformed, y_train, method_name, n_components)\n",
    "        normality_tests_results[f\"{method_name}_{n_components}\"] = normality_df\n",
    "        homogeneity_tests_results[f\"{method_name}_{n_components}\"] = homogeneity_df\n",
    "        difference_tests_results[f\"{method_name}_{n_components}\"] = difference_df\n",
    "\n",
    "        # Wizualizacja dla 2 wymiarów (tylko UMAP)\n",
    "        if method_name == \"UMAP\":\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            for label in np.unique(y_train):\n",
    "                plt.scatter(X_transformed[y_train == label, 0], X_transformed[y_train == label, 1], label=label)\n",
    "            plt.title(f\"Wizualizacja - {method_name} ({n_components} komponenty)\")\n",
    "            plt.xlabel(\"Wymiar 1\")\n",
    "            plt.ylabel(\"Wymiar 2\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    # Wyniki\n",
    "    display(Markdown(\"### Analiza kowariancji\"))\n",
    "    display(covariance_results)\n",
    "\n",
    "    display(Markdown(\"### Testy normalności (Anderson-Darling)\"))\n",
    "    for key, normality_df in normality_tests_results.items():\n",
    "        display(Markdown(f\"#### {key}\"))\n",
    "        display(normality_df)\n",
    "\n",
    "    display(Markdown(\"### Testy homogeniczności wariancji\"))\n",
    "    for key, homogeneity_df in homogeneity_tests_results.items():\n",
    "        display(Markdown(f\"#### {key}\"))\n",
    "        display(homogeneity_df)\n",
    "\n",
    "    display(Markdown(\"### Testy różnic między grupami (Kruskal-Wallis)\"))\n",
    "    for key, difference_df in difference_tests_results.items():\n",
    "        display(Markdown(f\"#### {key}\"))\n",
    "        display(difference_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizaualizacja sąsiadów outlinerów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def calculate_centroids(data, labels):\n",
    "    \"\"\"Oblicza centroidy dla każdej klasy.\"\"\"\n",
    "    unique_labels, inverse = np.unique(labels, return_inverse=True)\n",
    "    counts = np.bincount(inverse)\n",
    "    sums = np.zeros((len(unique_labels), data.shape[1]))\n",
    "    np.add.at(sums, inverse, data)\n",
    "    return sums / counts[:, None]\n",
    "\n",
    "def find_outlier_indices(X_transformed, y_train, threshold_factor=2.0):\n",
    "    \"\"\"\n",
    "    Automatycznie znajduje punkty, które są daleko od centroidu swojej klasy\n",
    "    i blisko centroidu innej klasy w przestrzeni zredukowanej.\n",
    "    Zwraca listę krotek (indeks, odległość do centroidu swojej klasy, odległość do najbliższego centroidu innej klasy).\n",
    "    \"\"\"\n",
    "    centroids_reduced = calculate_centroids(X_transformed, y_train)\n",
    "    distances = np.linalg.norm(X_transformed[:, None, :] - centroids_reduced[None, :, :], axis=2)\n",
    "\n",
    "    outlier_data = []\n",
    "    for label in np.unique(y_train):\n",
    "        indices_this_label = np.where(y_train == label)[0]\n",
    "        distances_to_own_centroid = distances[indices_this_label, label]\n",
    "\n",
    "        threshold = np.mean(distances_to_own_centroid) + threshold_factor * np.std(distances_to_own_centroid)\n",
    "\n",
    "        distances_copy = distances[indices_this_label, :].copy()\n",
    "        distances_copy[:, label] = np.inf\n",
    "        min_distances_to_other_centroid = np.min(distances_copy, axis=1)\n",
    "\n",
    "        outlier_indices_this_label = indices_this_label[\n",
    "            (distances_to_own_centroid > threshold)\n",
    "        ]\n",
    "\n",
    "        outlier_data.extend(\n",
    "            list(\n",
    "                zip(\n",
    "                    outlier_indices_this_label,\n",
    "                    distances_to_own_centroid[distances_to_own_centroid > threshold],\n",
    "                    min_distances_to_other_centroid[distances_to_own_centroid > threshold],\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return outlier_data\n",
    "\n",
    "def analyze_outlier(X_train, y_train, X_transformed, outlier_data, knn, method_name=\"UMAP\", max_outliers=10):\n",
    "    \"\"\"\n",
    "    Analizuje pojedynczy outlier: wyświetla obraz, odległości do centroidów\n",
    "    i etykiety najbliższych sąsiadów.\n",
    "    Ogranicza liczbę analizowanych outlierów do max_outliers.\n",
    "    \"\"\"\n",
    "    centroids_original = calculate_centroids(X_train, y_train)\n",
    "\n",
    "    print(f\"Analiza outlierów dla metody {method_name}:\")\n",
    "\n",
    "    for i, (index, dist_own, dist_other) in enumerate(outlier_data):\n",
    "        if i >= max_outliers:\n",
    "            break\n",
    "\n",
    "        label = y_train[index]\n",
    "        dist_original_own = np.linalg.norm(X_train[index] - centroids_original[label])\n",
    "\n",
    "        print(f\"\\nIndeks: {index}, Etykieta: {label}\")\n",
    "        print(f\"Odległość do centroidu swojej klasy (przestrzeń oryginalna): {dist_original_own:.2f}\")\n",
    "        print(f\"Odległość do centroidu swojej klasy (przestrzeń zredukowana): {dist_own:.2f}\")\n",
    "        print(f\"Minimalna odległość do centroidu innej klasy (przestrzeń zredukowana): {dist_other:.2f}\")\n",
    "\n",
    "        distances, indices = knn.kneighbors(X_train[index][np.newaxis, :])\n",
    "        print(\"\\nIndeksy najbliższych sąsiadów:\", indices[0])\n",
    "        print(\"Etykiety najbliższych sąsiadów:\", y_train[indices[0]])\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for j in range(6):\n",
    "            plt.subplot(1, 6, j + 1)\n",
    "            if j == 0:\n",
    "                img = X_train[index].reshape(28, 28)\n",
    "                title = f\"Outlier\\nIndeks: {index}\\nEtykieta: {label}\"\n",
    "            else:\n",
    "                neighbor_index = indices[0][j - 1]\n",
    "                img = X_train[neighbor_index].reshape(28, 28)\n",
    "                title = f\"Sąsiad: {y_train[neighbor_index]}\"\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "            plt.title(title)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.suptitle(f\"Analiza Outliera dla metody {method_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Spłaszczenie danych\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "# Redukcja wymiarów UMAP\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_transformed = reducer.fit_transform(X_train_flat)\n",
    "\n",
    "#Wizualizacja\n",
    "df = pd.DataFrame(X_transformed, columns=[\"UMAP wymiar 1\", \"UMAP wymiar 2\"])\n",
    "df[\"Cyfra\"] = y_train\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(\n",
    "    x=\"UMAP wymiar 1\",\n",
    "    y=\"UMAP wymiar 2\",\n",
    "    hue=\"Cyfra\",\n",
    "    data=df,\n",
    "    palette=\"viridis\",\n",
    "    alpha=0.5,\n",
    "    legend=\"full\",\n",
    ")\n",
    "plt.title(\"UMAP MNIST\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=6, algorithm=\"auto\")\n",
    "knn.fit(X_train_flat)\n",
    "\n",
    "# Znajdowanie outlierów na podstawie odległości do centroidów\n",
    "outlier_data = find_outlier_indices(X_transformed, y_train, threshold_factor=2.5)\n",
    "\n",
    "# Analiza outlierów\n",
    "analyze_outlier(X_train_flat, y_train, X_transformed, outlier_data, knn, method_name=\"UMAP\", max_outliers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape \n",
    "X_train = X_train.reshape(-1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 28, 28)\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "y_train_ENC = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_ENC = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_train_ENC:\", y_train_ENC.shape)\n",
    "\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"Shape of y_test_ENC:\", y_test_ENC.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacja histogramem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cv2.normalize(X_train, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_64F)\n",
    "X_train = X_train.astype(np.uint8)\n",
    "# Wyrównywanie histogramu\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i] = cv2.equalizeHist(X_train[i])\n",
    "\n",
    "X_test = cv2.normalize(X_test, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_64F)\n",
    "X_test = X_test.astype(np.uint8)\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i] = cv2.equalizeHist(X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powiększanie i wygładzenie obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlarge_and_smooth(image, new_size):\n",
    "    \"\"\"Powiększa obraz za pomocą interpolacji bicubic.\"\"\"\n",
    "    enlarged_image = cv2.resize(image, new_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "    return enlarged_image\n",
    "\n",
    "# # Sztuczne zwiększenie rozmiaru obrazu \n",
    "# sample_indices = np.random.choice(X_train.shape[0], 5, replace=False)\n",
    "\n",
    "# # Dla każdego wybranego obrazu\n",
    "# for idx in sample_indices:\n",
    "#     # Oryginalny obraz\n",
    "#     original_image = X_train[idx]\n",
    "\n",
    "#     # Powiększony i wygładzony obraz\n",
    "#     enlarged_smoothed_image = enlarge_and_smooth(original_image, new_size=(42, 42))\n",
    "\n",
    "#     # Wyświetl oryginalny i przekształcony obraz obok siebie\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "#     print(\"Shape of original_image:\", original_image.shape)\n",
    "#     print(\"Shape of enlarged_smoothed_image:\", enlarged_smoothed_image.shape)\n",
    "\n",
    "#     sns.heatmap(original_image, cmap=\"viridis\", ax=axes[0]) \n",
    "#     axes[0].set_title(f'Oryginalny obraz ({idx})')\n",
    "\n",
    "#     sns.heatmap(enlarged_smoothed_image, cmap=\"viridis\", ax=axes[1]) \n",
    "#     axes[1].set_title(f'Powiększony i wygładzony obraz (Lanczos4) ({idx})')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split na danych treningowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=69)\n",
    "print(\"Rozmiar X_train_base:\", X_train_base.shape)\n",
    "print(\"Rozmiar X_val_base:\", X_val_base.shape)\n",
    "print(\"Rozmiar y_train_base:\", y_train_base.shape)\n",
    "print(\"Rozmiar y_val_base:\", y_val_base.shape)\n",
    "\n",
    "### Upscalowanie danych\n",
    "smoothed_X_train = np.array([enlarge_and_smooth(image, new_size=(42, 42)) for image in X_train])\n",
    "\n",
    "X_train_Upsc, X_val_Upsc, y_train_Upsc, y_val_Upsc = train_test_split(\n",
    "    smoothed_X_train, y_train, test_size=0.2, random_state=69)\n",
    "print(\"Rozmiar X_train_Upsc:\", X_train_Upsc.shape)\n",
    "print(\"Rozmiar X_val_Upsc:\", X_val_Upsc.shape)\n",
    "print(\"Rozmiar y_train_Upsc:\", y_train_Upsc.shape)\n",
    "print(\"Rozmiar y_val_Upsc:\", y_val_Upsc.shape)\n",
    "\n",
    "### Upscalowanie danych testowych\n",
    "X_test = np.array([enlarge_and_smooth(image, new_size=(42, 42)) for image in X_test])\n",
    "print(\"Rozmiar X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa Ewaulacji modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X: np.ndarray) -> np.ndarray:\n",
    "    return X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Transformer do użycia w pipeline\n",
    "flatten_transformer = FunctionTransformer(flatten)\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Klasa do ewaluacji modeli klasyfikacji.\n",
    "    Umożliwia przekazanie pipeline'u, opcjonalne dodanie kroku spłaszczania,\n",
    "    oblicza wybrane metryki (logika sterowana danymi) z konfigurowalnym trybem uśredniania,\n",
    "    i (jeśli verbose=True) wyświetla wyniki oraz generuje obiekty wykresów.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics: Optional[List[str]] = None,\n",
    "        average_mode: Literal[\"macro\", \"micro\", \"weighted\", \"binary\"] = \"macro\",\n",
    "        display_labels: Optional[List[str]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicjalizuje ModelEvaluator.\n",
    "        (Opis Args bez zmian)\n",
    "        \"\"\"\n",
    "        self.metrics = metrics if metrics is not None else [\n",
    "            \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
    "        ]\n",
    "        # Walidacja average_mode\n",
    "        valid_average_modes = [\"macro\", \"micro\", \"weighted\", \"binary\", None] # Dodano None jako możliwy (choć nie domyślny)\n",
    "        if average_mode not in valid_average_modes:\n",
    "            raise ValueError(f\"Nieprawidłowy average_mode. Wybierz jeden z: {valid_average_modes}\")\n",
    "        self.average_mode = average_mode\n",
    "        self.display_labels = display_labels\n",
    "\n",
    "        self._metric_calculators: Dict[str, Tuple[Callable, Dict[str, Any]]] = {\n",
    "            \"accuracy\": (accuracy_score, {}), # Accuracy nie potrzebuje dodatkowych arg.\n",
    "            \"precision\": (precision_score, {\"average\": self.average_mode, \"zero_division\": 0}),\n",
    "            \"recall\": (recall_score, {\"average\": self.average_mode, \"zero_division\": 0}),\n",
    "            \"f1\": (f1_score, {\"average\": self.average_mode, \"zero_division\": 0}),\n",
    "        }\n",
    "\n",
    "        # Sprawdź, czy wymagane metryki są obsługiwane (oparte na kluczach mapowania + roc_auc)\n",
    "        supported_metrics = set(self._metric_calculators.keys()) | {\"roc_auc\"}\n",
    "        for metric in self.metrics:\n",
    "            if metric not in supported_metrics:\n",
    "                warnings.warn(f\"Metryka '{metric}' nie jest bezpośrednio obsługiwana przez tę klasę.\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        pipeline: Pipeline,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        X_val: np.ndarray,\n",
    "        y_val: np.ndarray,\n",
    "        add_flatten_step: bool = False,\n",
    "        calculate_proba: bool = False,\n",
    "        roc_pos_label: Union[int, str] = 1,\n",
    "        verbose: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Trenuje i ewaluuje podany pipeline na danych walidacyjnych.\n",
    "        \"\"\"\n",
    "        eval_pipeline = pipeline\n",
    "\n",
    "        if add_flatten_step:\n",
    "            if verbose:\n",
    "                print(\"INFO: Dodaję krok 'flatten' na początku pipeline'u.\")\n",
    "            eval_pipeline = Pipeline([\n",
    "                (\"flatten\", flatten_transformer),\n",
    "                *pipeline.steps\n",
    "            ])\n",
    "\n",
    "        # Trenowanie pipeline'u\n",
    "        eval_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predykcje\n",
    "        y_pred = eval_pipeline.predict(X_val)\n",
    "\n",
    "        # Prawdopodobieństwa\n",
    "        y_prob = None\n",
    "        if calculate_proba and \"roc_auc\" in self.metrics:\n",
    "            y_prob = self._get_probabilities(eval_pipeline, X_val, verbose)\n",
    "\n",
    "        # Obliczanie metryk (używa zrefaktoryzowanej metody)\n",
    "        metric_results = self._calculate_metrics(y_val, y_pred, y_prob, roc_pos_label, calculate_proba)\n",
    "\n",
    "        # Przygotowanie wyników do zwrócenia\n",
    "        output: Dict[str, Any] = {}\n",
    "        metrics_df = pd.DataFrame([metric_results]) # Utwórz DF z wynikami metryk\n",
    "        output['metrics'] = metrics_df\n",
    "\n",
    "        if verbose:\n",
    "            self._display_verbose_output(\n",
    "                y_val, y_pred, y_prob, roc_pos_label, eval_pipeline, metrics_df, metric_results, output\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    # --- Metody pomocnicze dla Refaktoryzacji 2 ---\n",
    "    def _get_model_name(self, eval_pipeline: Union[Pipeline, BaseEstimator]) -> str:\n",
    "        \"\"\"Pobiera nazwę ostatniego kroku pipeline'u lub nazwę modelu.\"\"\"\n",
    "        if isinstance(eval_pipeline, Pipeline):\n",
    "             if not eval_pipeline.steps: return \"Pusty Pipeline\"\n",
    "             final_estimator = eval_pipeline.steps[-1][1]\n",
    "             if isinstance(final_estimator, str) and final_estimator == \"passthrough\":\n",
    "                 if len(eval_pipeline.steps) > 1:\n",
    "                      final_estimator = eval_pipeline.steps[-2][1]\n",
    "                 else:\n",
    "                      return \"Pipeline (tylko passthrough?)\"\n",
    "             return final_estimator.__class__.__name__\n",
    "        else:\n",
    "             return eval_pipeline.__class__.__name__\n",
    "\n",
    "    def _display_metrics_table(self, metrics_df: pd.DataFrame):\n",
    "        \"\"\"Wyświetla tabelę z metrykami.\"\"\"\n",
    "        print(\"\\nMetryki:\")\n",
    "        try:\n",
    "            display(metrics_df)\n",
    "        except ImportError:\n",
    "            print(metrics_df.to_string())\n",
    "\n",
    "    def _display_classification_report(self, y_true: np.ndarray, y_pred: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"Formatuje i wyświetla raport klasyfikacji, zwraca DataFrame raportu.\"\"\"\n",
    "        print(\"\\nRaport Klasyfikacji:\")\n",
    "        report_df = self._format_classification_report(y_true, y_pred)\n",
    "        try:\n",
    "            display(report_df)\n",
    "        except ImportError:\n",
    "            print(report_df.to_string())\n",
    "        return report_df\n",
    "\n",
    "    def _generate_and_collect_plots(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        y_prob: Optional[np.ndarray],\n",
    "        roc_pos_label: Union[int, str],\n",
    "        metric_results: Dict[str, Optional[float]],\n",
    "        output_dict: Dict[str, Any] \n",
    "    ):\n",
    "        \"\"\"Generuje obiekty wykresów i zapisuje je w słowniku wyjściowym.\"\"\"\n",
    "        output_dict['figures'] = {}\n",
    "        print(\"\\nGenerowanie wykresów (obiekty zwrócone w słowniku 'figures'):\")\n",
    "\n",
    "        # Macierz pomyłek\n",
    "        try:\n",
    "            fig_cm, ax_cm = self._plot_confusion_matrix(y_true, y_pred)\n",
    "            output_dict['figures']['confusion_matrix'] = (fig_cm, ax_cm)\n",
    "            print(\"- Wygenerowano macierz pomyłek.\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Nie udało się wygenerować macierzy pomyłek: {e}\")\n",
    "\n",
    "        # Krzywa ROC\n",
    "        if \"roc_auc\" in metric_results and metric_results[\"roc_auc\"] is not None and y_prob is not None:\n",
    "            try:\n",
    "                roc_data = self._calculate_roc_data(y_true, y_prob, roc_pos_label)\n",
    "                if roc_data:\n",
    "                    auc_score, fpr, tpr = roc_data\n",
    "                    fig_roc, ax_roc = self._plot_roc_curve(fpr, tpr, auc_score)\n",
    "                    output_dict['figures']['roc_curve'] = (fig_roc, ax_roc)\n",
    "                    print(\"- Wygenerowano krzywą ROC.\")\n",
    "            except Exception as e:\n",
    "                 warnings.warn(f\"Nie udało się wygenerować krzywej ROC: {e}\")\n",
    "        elif \"roc_auc\" in self.metrics: \n",
    "             print(\"- Nie można wygenerować krzywej ROC (brak danych ROC lub prawdopodobieństw).\")\n",
    "\n",
    "\n",
    "    def _display_verbose_output(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        y_prob: Optional[np.ndarray],\n",
    "        roc_pos_label: Union[int, str],\n",
    "        eval_pipeline: Union[Pipeline, BaseEstimator],\n",
    "        metrics_df: pd.DataFrame,\n",
    "        metric_results: Dict[str, Optional[float]],\n",
    "        output_dict: Dict[str, Any] # Słownik do zapisu raportu i figur\n",
    "    ):\n",
    "        \"\"\"Główna metoda pomocnicza do obsługi wyjścia w trybie verbose.\"\"\"\n",
    "        print(\"\\n--- Wyniki Ewaluacji ---\")\n",
    "        model_name = self._get_model_name(eval_pipeline)\n",
    "        print(f\"Model (ostatni krok): {model_name}\")\n",
    "\n",
    "        self._display_metrics_table(metrics_df)\n",
    "        report_df = self._display_classification_report(y_true, y_pred)\n",
    "        output_dict['classification_report'] = report_df \n",
    "        self._generate_and_collect_plots(y_true, y_pred, y_prob, roc_pos_label, metric_results, output_dict)\n",
    "\n",
    "    def _get_probabilities(\n",
    "        self,\n",
    "        model: BaseEstimator,\n",
    "        X: np.ndarray,\n",
    "        verbose: bool = True\n",
    "    ) -> Optional[np.ndarray]:\n",
    "        \"\"\"Prywatna metoda do bezpiecznego pobierania prawdopodobieństw.\"\"\"\n",
    "        try:\n",
    "            y_prob_all = model.predict_proba(X)\n",
    "            if y_prob_all.shape[1] >= 2:\n",
    "                return y_prob_all[:, 1] \n",
    "            elif y_prob_all.shape[1] == 1:\n",
    "                warnings.warn(\"Metoda predict_proba zwróciła tylko jedną kolumnę.\")\n",
    "                return y_prob_all[:, 0]\n",
    "            else:\n",
    "                warnings.warn(\"Metoda predict_proba zwróciła pusty wynik.\")\n",
    "                return None\n",
    "        except AttributeError:\n",
    "            if verbose:\n",
    "                warnings.warn(\"Model nie posiada metody `predict_proba`. Metryka ROC AUC nie zostanie obliczona.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                warnings.warn(f\"Wystąpił błąd podczas wywołania `predict_proba`: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _calculate_metrics(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        y_prob: Optional[np.ndarray],\n",
    "        roc_pos_label: Union[int, str],\n",
    "        calculate_proba: bool\n",
    "    ) -> Dict[str, Optional[float]]:\n",
    "        \"\"\"Prywatna metoda do obliczania wybranych metryk (sterowana danymi).\"\"\"\n",
    "        results: Dict[str, Optional[float]] = {}\n",
    "        base_args = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "        for metric_name in self.metrics:\n",
    "            if metric_name == \"roc_auc\":\n",
    "                continue # Obsługa ROC AUC poniżej\n",
    "\n",
    "            if metric_name in self._metric_calculators:\n",
    "                func, specific_args = self._metric_calculators[metric_name]\n",
    "                try:\n",
    "                    # Połącz argumenty bazowe i specyficzne dla danej metryki\n",
    "                    all_args = {**base_args, **specific_args}\n",
    "                    results[metric_name] = func(**all_args)\n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Nie udało się obliczyć metryki '{metric_name}': {e}\")\n",
    "                    results[metric_name] = None\n",
    "\n",
    "        # Obsługa ROC AUC\n",
    "        results[\"roc_auc\"] = None\n",
    "        if \"roc_auc\" in self.metrics:\n",
    "            if calculate_proba:\n",
    "                if y_prob is not None:\n",
    "                    roc_data = self._calculate_roc_data(y_true, y_prob, roc_pos_label)\n",
    "                    if roc_data:\n",
    "                        results[\"roc_auc\"] = roc_data[0] # Zapisz tylko wartość AUC\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                     pass \n",
    "\n",
    "        return results\n",
    "\n",
    "    def _calculate_roc_data(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_prob: np.ndarray,\n",
    "        pos_label: Union[int, str] = 1\n",
    "    ) -> Optional[Tuple[float, np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Oblicza AUC oraz FPR, TPR dla krzywej ROC.\"\"\"\n",
    "        # (Implementacja bez zmian)\n",
    "        try:\n",
    "            if len(y_true) != len(y_prob): raise ValueError(\"Długości y_true i y_prob nie są zgodne.\")\n",
    "            unique_labels = np.unique(y_true)\n",
    "            if pos_label not in unique_labels:\n",
    "                warnings.warn(f\"Etykieta pozytywna '{pos_label}' nie znaleziona w y_true. Unikalne etykiety: {unique_labels}. Nie można obliczyć ROC AUC.\")\n",
    "                return None\n",
    "            if len(unique_labels) < 2:\n",
    "                warnings.warn(f\"W y_true znaleziono tylko jedną klasę: {unique_labels}. Nie można obliczyć ROC AUC.\")\n",
    "                return None\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_prob, pos_label=pos_label)\n",
    "            roc_auc_score = auc(fpr, tpr)\n",
    "            return roc_auc_score, fpr, tpr\n",
    "        except ValueError as e:\n",
    "            warnings.warn(f\"Błąd podczas obliczania ROC: {e}. Sprawdź format danych i `pos_label`.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Nieoczekiwany błąd podczas obliczania ROC: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _plot_confusion_matrix(\n",
    "        self, y_true: np.ndarray, y_pred: np.ndarray\n",
    "    ) -> Tuple[plt.Figure, plt.Axes]:\n",
    "        \"\"\"Generuje obiekt wykresu macierzy pomyłek.\"\"\"\n",
    "        # (Implementacja bez zmian)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        labels = self.display_labels if self.display_labels else np.unique(y_true)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot(ax=ax, cmap=plt.cm.Blues, values_format='d')\n",
    "        ax.set_title(\"Macierz pomyłek\")\n",
    "        return fig, ax\n",
    "\n",
    "    def _format_classification_report(\n",
    "        self, y_true: np.ndarray, y_pred: np.ndarray\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Generuje raport klasyfikacji jako Pandas DataFrame.\"\"\"\n",
    "        # (Implementacja bez zmian)\n",
    "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df['support'] = report_df['support'].astype(int)\n",
    "        report_df[['precision', 'recall', 'f1-score']] = report_df[['precision', 'recall', 'f1-score']].round(3)\n",
    "        return report_df\n",
    "\n",
    "    def _plot_roc_curve(\n",
    "        self, fpr: np.ndarray, tpr: np.ndarray, roc_auc: float\n",
    "    ) -> Tuple[plt.Figure, plt.Axes]:\n",
    "        \"\"\"Generuje obiekt wykresu krzywej ROC.\"\"\"\n",
    "        # (Implementacja bez zmian)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"Krzywa ROC (pole = {roc_auc:.3f})\")\n",
    "        ax.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Losowy klasyfikator\")\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel(\"False Positive Rate (FPR)\")\n",
    "        ax.set_ylabel(\"True Positive Rate (TPR)\")\n",
    "        ax.set_title(\"Krzywa ROC\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.grid(alpha=0.3)\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie modelu wykorzystując VotingClassifier dla danych orginalnaych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definicja modeli  ---\n",
    "qda = QDA(reg_param=0.1)\n",
    "hgb = HistGradientBoostingClassifier(random_state=42) \n",
    "et = ExtraTreesClassifier(random_state=42) \n",
    "\n",
    "# Utworzenie VotingClassifier\n",
    "VotClass = VotingClassifier(\n",
    "    estimators=[('qda', qda), ('hgb', hgb), ('et', et)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "models = [VotClass]\n",
    "\n",
    "# Tworzenie instancji klasy ModelEvaluator \n",
    "evaluator = ModelEvaluator(\n",
    "    metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], \n",
    "    average_mode=\"macro\"\n",
    ")\n",
    "\n",
    "all_metrics_data = {}\n",
    "\n",
    "# Trenowanie i ewaluacja modeli\n",
    "for model in models:\n",
    "    if hasattr(model, '__class__') and hasattr(model.__class__, '__name__'):\n",
    "         model_name = model.__class__.__name__\n",
    "    else:\n",
    "         model_name = str(model)[:30]\n",
    "\n",
    "    print(f\"\\n--- Ewaluacja modelu: {model_name} ---\")\n",
    "\n",
    "    # Utworzenie pipeline'u\n",
    "    pipeline = Pipeline([\n",
    "        (\"classifier\", model) \n",
    "    ])\n",
    "\n",
    "    # Trenowanie i ewaluacja pipeline'u\n",
    "    eval_results = evaluator.evaluate(\n",
    "        pipeline=pipeline,\n",
    "        X_train=X_train_base,\n",
    "        y_train=y_train_base,\n",
    "        X_val=X_val_base,\n",
    "        y_val=y_val_base,\n",
    "        add_flatten_step=True,  # Ustaw wg potrzeb danych\n",
    "        calculate_proba=True,   # Włączone dla roc_auc i voting='soft'\n",
    "        roc_pos_label=1,        # Dostosuj etykietę klasy pozytywnej\n",
    "        verbose=True            # Czy drukować wyniki na bieżąco?\n",
    "    )\n",
    "\n",
    "    if 'metrics' in eval_results and isinstance(eval_results['metrics'], pd.DataFrame):\n",
    "        all_metrics_data[model_name] = eval_results['metrics']\n",
    "    else:\n",
    "        print(f\"OSTRZEŻENIE: Brak DataFrame 'metrics' w wynikach dla modelu {model_name}\")\n",
    "\n",
    "    # Opcjonalnie: figury i raporty\n",
    "    # if 'figures' in eval_results:\n",
    "    #     all_figures[model_name] = eval_results['figures']\n",
    "    # if 'classification_report' in eval_results:\n",
    "    #     all_reports[model_name] = eval_results['classification_report']\n",
    "\n",
    "if all_metrics_data:\n",
    "    summary_metrics_df = pd.concat(all_metrics_data)\n",
    "    if isinstance(summary_metrics_df.index, pd.MultiIndex):\n",
    "        summary_metrics_df.index = summary_metrics_df.index.get_level_values(0)\n",
    "\n",
    "    print(\"\\n--- Zbiorcze Metryki dla Wszystkich Modeli ---\")\n",
    "    try:\n",
    "        display(summary_metrics_df)\n",
    "    except ImportError:\n",
    "        print(summary_metrics_df.to_string())\n",
    "else:\n",
    "    print(\"\\nNie zebrano żadnych danych metryk do wyświetlenia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie modelu wykorzystując VotingClassifier dla danych upscalowanych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definicja modeli  ---\n",
    "qda = QDA(reg_param=0.1)\n",
    "hgb = HistGradientBoostingClassifier(random_state=42) \n",
    "et = ExtraTreesClassifier(random_state=42) \n",
    "\n",
    "# Utworzenie VotingClassifier\n",
    "VotClass = VotingClassifier(\n",
    "    estimators=[('qda', qda), ('hgb', hgb), ('et', et)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "models = [VotClass]\n",
    "\n",
    "# Tworzenie instancji klasy ModelEvaluator \n",
    "evaluator = ModelEvaluator(\n",
    "    metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], \n",
    "    average_mode=\"macro\"\n",
    ")\n",
    "\n",
    "all_metrics_data = {}\n",
    "\n",
    "# Trenowanie i ewaluacja modeli\n",
    "for model in models:\n",
    "    if hasattr(model, '__class__') and hasattr(model.__class__, '__name__'):\n",
    "         model_name = model.__class__.__name__\n",
    "    else:\n",
    "         model_name = str(model)[:30]\n",
    "\n",
    "    print(f\"\\n--- Ewaluacja modelu: {model_name} ---\")\n",
    "\n",
    "    # Utworzenie pipeline'u\n",
    "    pipeline = Pipeline([\n",
    "        (\"classifier\", model) \n",
    "    ])\n",
    "\n",
    "    # Trenowanie i ewaluacja pipeline'u\n",
    "    eval_results = evaluator.evaluate(\n",
    "        pipeline=pipeline,\n",
    "        X_train=X_train_Upsc,\n",
    "        y_train=y_train_Upsc,\n",
    "        X_val=X_val_Upsc,\n",
    "        y_val=y_val_Upsc,\n",
    "        add_flatten_step=True,  # Ustaw wg potrzeb danych\n",
    "        calculate_proba=True,   # Włączone dla roc_auc i voting='soft'\n",
    "        roc_pos_label=1,        # Dostosuj etykietę klasy pozytywnej\n",
    "        verbose=True            # Czy drukować wyniki na bieżąco?\n",
    "    )\n",
    "\n",
    "    if 'metrics' in eval_results and isinstance(eval_results['metrics'], pd.DataFrame):\n",
    "        all_metrics_data[model_name] = eval_results['metrics']\n",
    "    else:\n",
    "        print(f\"OSTRZEŻENIE: Brak DataFrame 'metrics' w wynikach dla modelu {model_name}\")\n",
    "\n",
    "    # Opcjonalnie: figury i raporty\n",
    "    # if 'figures' in eval_results:\n",
    "    #     all_figures[model_name] = eval_results['figures']\n",
    "    # if 'classification_report' in eval_results:\n",
    "    #     all_reports[model_name] = eval_results['classification_report']\n",
    "\n",
    "if all_metrics_data:\n",
    "    summary_metrics_df = pd.concat(all_metrics_data)\n",
    "    if isinstance(summary_metrics_df.index, pd.MultiIndex):\n",
    "        summary_metrics_df.index = summary_metrics_df.index.get_level_values(0)\n",
    "\n",
    "    print(\"\\n--- Zbiorcze Metryki dla Wszystkich Modeli ---\")\n",
    "    try:\n",
    "        display(summary_metrics_df)\n",
    "    except ImportError:\n",
    "        print(summary_metrics_df.to_string())\n",
    "else:\n",
    "    print(\"\\nNie zebrano żadnych danych metryk do wyświetlenia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detekcja outlinerów oraz agumentacja danych - Testy Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import optunahub\n",
    "    OPTUNAHUB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNAHUB_AVAILABLE = False\n",
    "    print(\"Ostrzeżenie: Moduł optunahub nie jest zainstalowany. Używam domyślnego TPESampler.\")\n",
    "import warnings\n",
    "import traceback\n",
    "import gc\n",
    "from typing import List, Dict, Optional, Any, Tuple, Union, Literal, Callable\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "# --- Konfiguracja Logowania ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Konfiguracja Globalna ---\n",
    "RANDOM_STATE = 42\n",
    "TARGET_IMG_SIZE = 42\n",
    "\n",
    "# Konfiguracja Pre-obliczenia UMAP dla LOF/Weak\n",
    "PRE_UMAP_CONFIG = {\n",
    "    'n_components': 2,\n",
    "    'n_neighbors': 30,\n",
    "    'min_dist': 0.1,\n",
    "    'metric': 'euclidean',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "\n",
    "# Konfiguracja Augmentacji\n",
    "AUGMENTATION_CONFIG = {\n",
    "    'padding_val': 35,\n",
    "    'sobel_thresh': 50,\n",
    "    'morph_disk_size': 1,\n",
    "    'affine_scale_range': (0.97, 1.04),\n",
    "    'affine_rot_range_deg': (-18, 19),\n",
    "    'affine_shear_range_deg': (-5, 6),\n",
    "    'affine_trans_range': (-5, 6)\n",
    "    # n_augmentations_per_sample będzie optymalizowane\n",
    "}\n",
    "\n",
    "# Konfiguracja Optuny\n",
    "OPTUNA_CONFIG = {\n",
    "    'n_trials_umap': 50, # Dostosuj\n",
    "    'n_trials_no_umap': 500,# Dostosuj\n",
    "    'n_jobs': 10,# WAŻNE: Zacznij od 1 dla debugowania!\n",
    "    'db_path': f\"sqlite:///study_Refactored_v2.db\", # Dynamiczna nazwa bazy\n",
    "    'study_name_prefix_umap': \"Aug_w_UMAP_Refactored\",\n",
    "    'study_name_prefix_no_umap': \"Aug_no_UMAP_Refactored\"\n",
    "}\n",
    "\n",
    "# Flaga kontrolująca, które study uruchomić\n",
    "RUN_WITH_UMAP_IN_PIPELINE = False # Ustaw na True lub False\n",
    "\n",
    "# Ignorowanie ostrzeżeń\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) # Zmniejsz gadatliwość Optuny\n",
    "\n",
    "# --- Funkcja Flatten i Transformer ---\n",
    "def flatten(X: np.ndarray) -> np.ndarray:\n",
    "    if X.ndim <= 2: return X\n",
    "    return X.reshape(X.shape[0], -1)\n",
    "flatten_transformer = FunctionTransformer(flatten)\n",
    "\n",
    "# --- Inicjalizacja Ewaluatora ---\n",
    "evaluator = ModelEvaluator(\n",
    "    metrics=[\"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    "    average_mode=\"macro\"\n",
    ")\n",
    "\n",
    "# --- Callback dla Optuny ---\n",
    "def callback(study, trial):\n",
    "    value_str = f\"{trial.value:.6f}\" if trial.value is not None else \"None (błąd?)\"\n",
    "    logging.info(f\"Trial {trial.number} zakończony. Wartość: {value_str}.\")\n",
    "\n",
    "# --- Funkcje Pomocnicze Identyfikacji Indeksów ---\n",
    "def calculate_centroids(X, y):\n",
    "    classes = np.unique(y)\n",
    "    centroids, valid_classes = [], []\n",
    "    for k in classes:\n",
    "        Xk = X[y == k]\n",
    "        if Xk.size > 0 and np.issubdtype(Xk.dtype, np.number):\n",
    "             if Xk.ndim == 1: Xk = Xk.reshape(-1, 1)\n",
    "             if Xk.ndim > 1:\n",
    "                try: centroids.append(Xk.mean(axis=0)); valid_classes.append(k)\n",
    "                except Exception as e: logging.warning(f\"Nie obliczono centroidu dla klasy {k}: {e}\")\n",
    "    return np.array(centroids), np.array(valid_classes)\n",
    "\n",
    "def find_weakly_described_indices(X_transformed: np.ndarray, y_train: np.ndarray, threshold_factor: float) -> np.ndarray:\n",
    "    logging.info(\"      - Uruchamiam find_weakly_described_indices...\")\n",
    "    try:\n",
    "        weakly_described_indices = []\n",
    "        unique_labels_train = np.unique(y_train)\n",
    "        if not np.issubdtype(X_transformed.dtype, np.number): logging.warning(\"X_transformed nie numeryczne.\"); return np.array([], dtype=int)\n",
    "        if not np.issubdtype(y_train.dtype, np.number): logging.warning(\"y_train nie numeryczne.\"); return np.array([], dtype=int)\n",
    "        centroids_reduced, centroid_labels = calculate_centroids(X_transformed, y_train)\n",
    "        if centroids_reduced.shape[0] < 2: logging.warning(f\"Znaleziono {centroids_reduced.shape[0]} centroidów.\"); return np.array([], dtype=int)\n",
    "        if X_transformed.shape[1]!=centroids_reduced.shape[1]: logging.warning(\"Niezgodność wymiarów X i centroidów.\"); return np.array([],dtype=int)\n",
    "\n",
    "        label_to_centroid_idx = {label: idx for idx, label in enumerate(centroid_labels)}\n",
    "        distances = np.linalg.norm(X_transformed.astype(np.float32)[:, None, :] - centroids_reduced.astype(np.float32)[None, :, :], axis=2)\n",
    "\n",
    "        for label in unique_labels_train:\n",
    "            if label not in label_to_centroid_idx: continue\n",
    "            centroid_idx_for_label = label_to_centroid_idx[label]\n",
    "            indices_this_label = np.where(y_train == label)[0]\n",
    "            if indices_this_label.size == 0: continue\n",
    "            distances_subset = distances[indices_this_label, :]\n",
    "            if distances_subset.shape[1] < 2: continue\n",
    "            distances_copy = distances_subset.copy()\n",
    "            distances_copy[:, centroid_idx_for_label] = np.inf\n",
    "            min_distances_to_other_centroid = np.min(distances_copy, axis=1)\n",
    "            mean_dist = np.mean(min_distances_to_other_centroid)\n",
    "            threshold = 1e-9 if mean_dist < 1e-9 else mean_dist / threshold_factor\n",
    "            weakly_described_indices_this_label = indices_this_label[ (min_distances_to_other_centroid < threshold) ]\n",
    "            weakly_described_indices.extend(weakly_described_indices_this_label)\n",
    "\n",
    "        result_indices = np.unique(np.array(weakly_described_indices)).astype(int)\n",
    "        logging.info(f\"      - find_weakly_described_indices zakończono ({len(result_indices)} słabo opisanych).\")\n",
    "        return result_indices\n",
    "    except Exception as e:\n",
    "        logging.error(f\"BŁĄD w find_weakly_described_indices: {e}\", exc_info=True)\n",
    "        return np.array([], dtype=int)\n",
    "\n",
    "def _detect_outliers(X_train: np.ndarray, X_train_reduced_for_lof: np.ndarray, n_neighbors_lof: int, contamination_lof: float, contamination_forest: float) -> np.ndarray:\n",
    "    logging.info(\"      - Rozpoczynam detekcję outlierów (LOF + IF)...\")\n",
    "    y_pred_lof = np.ones(X_train.shape[0], dtype=int)\n",
    "    y_pred_iso = np.ones(X_train.shape[0], dtype=int)\n",
    "    # LOF\n",
    "    logging.info(\"        - Uruchamiam LOF...\")\n",
    "    actual_n_neighbors_lof = min(n_neighbors_lof, X_train_reduced_for_lof.shape[0] - 1)\n",
    "    if actual_n_neighbors_lof <= 0: logging.warning(\"Za mało próbek dla LOF.\")\n",
    "    else:\n",
    "        lof = LocalOutlierFactor(n_neighbors=actual_n_neighbors_lof, contamination=contamination_lof, n_jobs=-1)\n",
    "        try: y_pred_lof = lof.fit_predict(X_train_reduced_for_lof); logging.info(f\"LOF OK ({np.sum(y_pred_lof == -1)}).\")\n",
    "        except Exception as e: logging.error(f\"BŁĄD LOF: {e}\")\n",
    "    # Isolation Forest\n",
    "    logging.info(\"        - Uruchamiam IF...\")\n",
    "    iso_forest = IsolationForest(contamination=contamination_forest, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    try:\n",
    "        X_flat = flatten(X_train); y_pred_iso = iso_forest.fit_predict(X_flat); del X_flat; gc.collect()\n",
    "        logging.info(f\"IF OK ({np.sum(y_pred_iso == -1)}).\")\n",
    "    except Exception as e: logging.error(f\"BŁĄD IF: {e}\")\n",
    "    # Kombinacja OR\n",
    "    outlier_indices = np.where((y_pred_lof == -1) | (y_pred_iso == -1))[0]\n",
    "    logging.info(f\"      - Detekcja outlierów zakończona. Znaleziono: {len(outlier_indices)}.\")\n",
    "    return outlier_indices\n",
    "\n",
    "# --- Klasa Augmentacji Obrazów ---\n",
    "class ImageAugmenter:\n",
    "    def __init__(self, config: Dict = AUGMENTATION_CONFIG, target_size: int = TARGET_IMG_SIZE):\n",
    "        self.config = config\n",
    "        self.target_size = target_size\n",
    "        logging.info(\"Inicjalizacja ImageAugmenter zakończona.\")\n",
    "\n",
    "    def _apply_affine(self, image: np.ndarray) -> np.ndarray:\n",
    "        padded = np.pad(image, pad_width=self.config['padding_val'], mode='constant', constant_values=0)\n",
    "        transform = AffineTransform(\n",
    "            scale=np.random.uniform(*self.config['affine_scale_range'], size=2),\n",
    "            rotation=np.deg2rad(np.random.randint(*self.config['affine_rot_range_deg'])),\n",
    "            shear=np.deg2rad(np.random.randint(*self.config['affine_shear_range_deg'])),\n",
    "            translation=np.random.randint(*self.config['affine_trans_range'], size=2))\n",
    "        # Używaj float64 dla warp, potem clip i konwersja\n",
    "        warped = warp(padded.astype(np.float64)/255.0, transform, mode='constant', cval=0, preserve_range=False, output_shape=padded.shape)\n",
    "        return np.clip(warped * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    def _extract_digit(self, image: np.ndarray) -> Optional[np.ndarray]:\n",
    "        try:\n",
    "            if image.ndim == 3: image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # Zwiększony kernel Sobela\n",
    "            gx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5); gy = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "            mag = cv2.magnitude(gx, gy)\n",
    "            if np.max(mag) <= 1e-6: return None\n",
    "            mag_norm = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            _, thresh = cv2.threshold(mag_norm, self.config['sobel_thresh'], 255, cv2.THRESH_BINARY)\n",
    "            contours = find_contours(thresh, 0.5)\n",
    "            if not contours: return None\n",
    "            # Znajdź największy kontur wg pola bounding box\n",
    "            largest_contour = max(contours, key=lambda c: np.prod(np.max(c, axis=0) - np.min(c, axis=0)))\n",
    "            minr, minc = np.min(largest_contour, axis=0).astype(int)\n",
    "            maxr, maxc = np.max(largest_contour, axis=0).astype(int)\n",
    "            if minr >= maxr or minc >= maxc: return None\n",
    "            isolated = image[minr:maxr+1, minc:maxc+1]\n",
    "            return isolated if isolated.size > 0 else None\n",
    "        except Exception: return None\n",
    "\n",
    "    def _apply_morph(self, image: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "        if image is None or image.size == 0: return None\n",
    "        op_choice = np.random.choice([None, 'erosion', 'opening', 'closing'])\n",
    "        if op_choice is None: return image\n",
    "        try:\n",
    "            kernel = disk(self.config['morph_disk_size'])\n",
    "            if op_choice == 'erosion': result = erosion(image, kernel)\n",
    "            elif op_choice == 'opening': result = opening(image, kernel)\n",
    "            else: result = closing(image, kernel) # closing\n",
    "            return result if result.size > 0 else None\n",
    "        except Exception: return None\n",
    "\n",
    "    def _resize_center(self, image: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "        if image is None or image.size == 0: return None\n",
    "        try:\n",
    "            if image.dtype != np.uint8: image = np.clip(image, 0, 255).astype(np.uint8)\n",
    "            h, w = image.shape[:2]\n",
    "            if h <= 0 or w <= 0: return None\n",
    "            if h > self.target_size or w > self.target_size:\n",
    "                scale = min(self.target_size / h, self.target_size / w)\n",
    "                nh, nw = max(1, int(h * scale)), max(1, int(w * scale))\n",
    "                image = cv2.resize(image, (nw, nh), interpolation=cv2.INTER_AREA)\n",
    "                h, w = image.shape[:2]\n",
    "\n",
    "            canvas = np.zeros((self.target_size, self.target_size), dtype=np.uint8)\n",
    "            r_off = max(0, (self.target_size - h) // 2); c_off = max(0, (self.target_size - w) // 2)\n",
    "            ph = min(h, self.target_size - r_off); pw = min(w, self.target_size - c_off)\n",
    "            src_frag = image[:ph, :pw]\n",
    "            target_slice = (slice(r_off, r_off + ph), slice(c_off, c_off + pw))\n",
    "            if canvas[target_slice].shape == src_frag.shape:\n",
    "                canvas[target_slice] = src_frag; return canvas\n",
    "            else: return None\n",
    "        except Exception: return None\n",
    "\n",
    "    def augment_batch(self, X_train, y_train, indices_to_augment, n_augmentations_per_sample):\n",
    "        \"\"\" Główna metoda augmentacji dla podanych indeksów. \"\"\"\n",
    "        if not isinstance(indices_to_augment, np.ndarray) or indices_to_augment.ndim != 1:\n",
    "             logging.warning(\"Nieprawidłowy `indices_to_augment`. Pomijam augmentację.\")\n",
    "             return X_train.copy(), y_train.copy()\n",
    "        if indices_to_augment.size == 0:\n",
    "            logging.info(\"      - Brak indeksów do augmentacji.\")\n",
    "            return X_train.copy(), y_train.copy()\n",
    "\n",
    "        logging.info(f\"      - Rozpoczynam augmentację {len(indices_to_augment)} próbek ({n_augmentations_per_sample} aug/próbkę)...\")\n",
    "        X_aug_list, y_aug_list = [], []\n",
    "        added_count = 0\n",
    "\n",
    "        indices_iter = tqdm(indices_to_augment, desc=\"Augmentacja\", leave=False, ncols=100) if len(indices_to_augment) > 500 else indices_to_augment\n",
    "\n",
    "        for idx in indices_iter:\n",
    "            if idx >= len(X_train): continue\n",
    "            original_image = X_train[idx].astype(np.uint8)\n",
    "            label = y_train[idx]\n",
    "            for _ in range(n_augmentations_per_sample):\n",
    "                # Łańcuch transformacji\n",
    "                img = self._apply_affine(original_image)\n",
    "                img = self._extract_digit(img)\n",
    "                img = self._apply_morph(img)\n",
    "                img = self._resize_center(img)\n",
    "                if img is not None:\n",
    "                    X_aug_list.append(img)\n",
    "                    y_aug_list.append(label)\n",
    "                    added_count += 1\n",
    "\n",
    "        logging.info(f\"      - Augmentacja zakończona. Dodano {added_count} obrazów.\")\n",
    "\n",
    "        if not X_aug_list:\n",
    "             logging.info(\"      - Nie wygenerowano poprawnych augmentacji.\")\n",
    "             return X_train.copy(), y_train.copy()\n",
    "\n",
    "        try: # Konkatenacja\n",
    "            X_train_augmented = np.array(X_aug_list)\n",
    "            y_train_augmented = np.array(y_aug_list)\n",
    "            # Upewnij się, że X_train jest 3D\n",
    "            if len(X_train.shape) == 2:\n",
    "                side = int(np.sqrt(X_train.shape[1])); X_train = X_train.reshape(-1, side, side)\n",
    "\n",
    "            if X_train.shape[1:] == X_train_augmented.shape[1:]:\n",
    "                X_train_combined = np.concatenate((X_train, X_train_augmented), axis=0)\n",
    "                y_train_combined = np.concatenate((y_train, y_train_augmented), axis=0)\n",
    "            else: warnings.warn(\"...\"); X_train_combined,y_train_combined=X_train.copy(),y_train.copy()\n",
    "        except Exception as e_concat: logging.error(f\"BŁĄD konkatenacji: {e_concat}\"); X_train_combined,y_train_combined=X_train.copy(),y_train.copy()\n",
    "\n",
    "        logging.info(f\"      - Dane połączone. Nowy rozmiar X_train: {X_train_combined.shape}\")\n",
    "        return X_train_combined, y_train_combined\n",
    "\n",
    "# --- ZOPTYMALIZOWANA funkcja create_pipeline ---\n",
    "def create_pipeline(use_umap: bool, model_instance: BaseEstimator, **umap_params) -> Pipeline:\n",
    "    \"\"\"Tworzy pipeline z flatten, opcjonalnie UMAP i przekazanym modelem.\"\"\"\n",
    "    steps = [(\"flatten\", flatten_transformer)]\n",
    "    if use_umap:\n",
    "        umap_params_filtered = {k: v for k, v in umap_params.items() if v is not None}\n",
    "        umap_params_filtered.setdefault('random_state', RANDOM_STATE); umap_params_filtered.setdefault('n_jobs', 1)\n",
    "        try: steps.append((\"reducer\", umap.UMAP(**umap_params_filtered)))\n",
    "        except Exception as e: logging.warning(f\"Błąd UMAP: {e}. Pomijam redukcję.\");\n",
    "    steps.append((\"classifier\", model_instance))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "# --- ZREFAKTORYZOWANE Funkcje dla Objective ---\n",
    "def _suggest_hyperparameters(trial: optuna.Trial, use_umap_flag: bool) -> Dict[str, Any]:\n",
    "    \"\"\"Sugeruje hiperparametry dla triala Optuny.\"\"\"\n",
    "    params = {\n",
    "        'n_neighbors_lof': trial.suggest_int(\"n_neighbors_lof\", 20, 300, step=10),\n",
    "        'contamination_lof': trial.suggest_float(\"contamination_lof\", 0.01, 0.25, step=0.01),\n",
    "        'contamination_forest': trial.suggest_float(\"contamination_forest\", 0.01, 0.25, step=0.01),\n",
    "        'n_augmentations_per_sample': trial.suggest_int(\"n_augmentations_per_sample\", 1, 3),\n",
    "        'weak_threshold_factor': trial.suggest_float(\"weak_threshold_factor\", 0.2, 3.0),\n",
    "        'qda_reg_param': trial.suggest_float(\"qda_reg_param\", 0.01, 0.3, step=0.05),\n",
    "        'hgb_learning_rate': trial.suggest_float(\"hgb_learning_rate\", 0.02, 0.15, log=True),\n",
    "        'hgb_max_leaf_nodes': trial.suggest_int(\"hgb_max_leaf_nodes\", 25, 50),\n",
    "        'et_n_estimators': trial.suggest_int(\"et_n_estimators\", 80, 250, step=10),\n",
    "        'et_max_depth': trial.suggest_int(\"et_max_depth\", 8, 25),\n",
    "        'et_min_samples_split': trial.suggest_int(\"et_min_samples_split\", 2, 10),\n",
    "        'et_min_samples_leaf': trial.suggest_int(\"et_min_samples_leaf\", 1, 5)\n",
    "    }\n",
    "    if use_umap_flag:\n",
    "        params['n_components_umap'] = trial.suggest_int(\"n_components_umap\", 15, 60, step=5)\n",
    "        params['n_neighbors_umap'] = trial.suggest_int(\"n_neighbors_umap\", 15, 300, step=15)\n",
    "        params['min_dist'] = trial.suggest_float(\"min_dist\", 0.05, 0.4)\n",
    "        params['spread'] = trial.suggest_float(\"spread\", 0.9, 1.1)\n",
    "        # Stałe parametry UMAP można dodać w `create_pipeline`\n",
    "    return params\n",
    "\n",
    "def _build_classifier(params: Dict[str, Any]) -> BaseEstimator:\n",
    "    \"\"\"Buduje VotingClassifier na podstawie zasugerowanych parametrów.\"\"\"\n",
    "    try:\n",
    "        qda_inst = QDA(reg_param=params['qda_reg_param'])\n",
    "        hgb_inst = HistGradientBoostingClassifier(\n",
    "            learning_rate=params['hgb_learning_rate'], max_leaf_nodes=params['hgb_max_leaf_nodes'],\n",
    "            random_state=RANDOM_STATE)\n",
    "        et_inst = ExtraTreesClassifier(\n",
    "            n_estimators=params['et_n_estimators'], max_depth=params['et_max_depth'],\n",
    "            min_samples_split=params['et_min_samples_split'],\n",
    "            min_samples_leaf=params['et_min_samples_leaf'],\n",
    "            random_state=RANDOM_STATE, n_jobs=1) # ET może używać wielu rdzeni\n",
    "        current_estimators = [('qda', qda_inst), ('hgb', hgb_inst), ('et', et_inst)]\n",
    "        # n_jobs=1 dla VotingClassifier dla stabilności, zwłaszcza przy n_jobs > 1 w Optunie\n",
    "        voting_clf_inst = VotingClassifier(estimators=current_estimators, voting='soft', n_jobs=-1)\n",
    "        return voting_clf_inst\n",
    "    except Exception as e:\n",
    "        logging.error(f\"BŁĄD podczas budowania klasyfikatora: {e}\", exc_info=True)\n",
    "        raise # Rzuć wyjątek dalej\n",
    "\n",
    "# --- ZOPTYMALIZOWANA i ZREFAKTORYZOWANA funkcja objective ---\n",
    "def objective(\n",
    "    trial: optuna.Trial, X_train_data: np.ndarray, y_train_data: np.ndarray,\n",
    "    X_val_data: np.ndarray, y_val_data: np.ndarray, X_train_reduced_for_lof_data: np.ndarray,\n",
    "    use_umap_flag: bool, evaluator_instance: ModelEvaluator, augmenter_instance: ImageAugmenter\n",
    ") -> float:\n",
    "    \"\"\"Funkcja celu dla Optuna - zrefaktoryzowana.\"\"\"\n",
    "    trial_num = trial.number\n",
    "    logging.info(f\"--- Rozpoczęcie Trial {trial_num} (use_umap={use_umap_flag}) ---\")\n",
    "    score = 0.0\n",
    "\n",
    "    try:\n",
    "        # Krok 1: Sugerowanie hiperparametrów\n",
    "        logging.info(f\"  Trial {trial_num}: Sugerowanie hiperparametrów...\")\n",
    "        params = _suggest_hyperparameters(trial, use_umap_flag)\n",
    "        logging.info(f\"  Trial {trial_num}: Sugerowanie parametrów zakończone.\")\n",
    "        # Logowanie tylko kluczowych\n",
    "        logging.info(f\"  Trial {trial_num}: Kluczowe parametry: Aug(n={params['n_augmentations_per_sample']}, thr={params['weak_threshold_factor']:.2f}), \"\n",
    "                     f\"LOF(c={params['contamination_lof']:.3f}), IF(c={params['contamination_forest']:.3f}), UMAP={use_umap_flag}\")\n",
    "\n",
    "        # Krok 2: Utworzenie modeli i pipeline\n",
    "        model_instance = _build_classifier(params)\n",
    "        umap_params = {k: v for k, v in params.items() if k.startswith('umap_') or k in ['n_components', 'n_neighbors', 'min_dist', 'spread']}\n",
    "        pipeline = create_pipeline(use_umap=use_umap_flag, model_instance=model_instance, **umap_params)\n",
    "        logging.info(f\"  Trial {trial_num}: Pipeline utworzony.\")\n",
    "\n",
    "        # Krok 3: Identyfikacja próbek do augmentacji\n",
    "        indices_to_augment = np.array([], dtype=int)\n",
    "        try:\n",
    "            weak_indices = find_weakly_described_indices(\n",
    "                X_train_reduced_for_lof_data, y_train_data, threshold_factor=params['weak_threshold_factor']\n",
    "            )\n",
    "            outlier_indices = _detect_outliers(\n",
    "                X_train_data, X_train_reduced_for_lof_data,\n",
    "                params['n_neighbors_lof'], params['contamination_lof'], params['contamination_forest']\n",
    "            )\n",
    "            indices_to_augment = np.unique(np.concatenate((weak_indices, outlier_indices))).astype(int)\n",
    "            logging.info(f\"  Trial {trial_num}: Znaleziono {len(indices_to_augment)} unikalnych indeksów do augmentacji.\")\n",
    "        except Exception as e_detect: logging.error(f\"BŁĄD identyfikacji indeksów: {e_detect}\", exc_info=True)\n",
    "\n",
    "        # Krok 4: Augmentacja Danych (użycie instancji Augmentera)\n",
    "        logging.info(f\"  Trial {trial_num}: Rozpoczynam augmentację danych...\")\n",
    "        X_train_aug, y_train_aug = X_train_data.copy(), y_train_data.copy() # Zacznij od kopii\n",
    "        if indices_to_augment.size > 0:\n",
    "            try:\n",
    "                # Wywołaj metodę z instancji augmentera\n",
    "                X_train_aug, y_train_aug = augmenter_instance.augment_batch(\n",
    "                    X_train_data.copy(), y_train_data.copy(),\n",
    "                    indices_to_augment, n_augmentations_per_sample=params['n_augmentations_per_sample']\n",
    "                )\n",
    "            except Exception as e_aug: logging.error(f\"BŁĄD augmentacji: {e_aug}\", exc_info=True); return 0.0\n",
    "        logging.info(f\"  Trial {trial_num}: Augmentacja zakończona. Rozmiar po: {len(y_train_aug)}\")\n",
    "\n",
    "        # Krok 5: Ewaluacja modelu\n",
    "        logging.info(f\"  Trial {trial_num}: Rozpoczynam ewaluację modelu...\")\n",
    "        eval_results = evaluator_instance.evaluate(\n",
    "            pipeline=pipeline, X_train=X_train_aug, y_train=y_train_aug,\n",
    "            X_val=X_val_data, y_val=y_val_data, add_flatten_step=False,\n",
    "            calculate_proba=False, verbose=False\n",
    "        )\n",
    "        logging.info(f\"  Trial {trial_num}: Ewaluacja zakończona.\")\n",
    "\n",
    "        # Krok 6: Obliczanie wyniku\n",
    "        metrics_df = eval_results.get('metrics')\n",
    "        if metrics_df is not None and not metrics_df.empty and not metrics_df.isnull().all().all(): # Sprawdź czy nie jest pusty/NaN\n",
    "             req_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "             valid_scores = [metrics_df[m].iloc[0] for m in req_metrics if m in metrics_df.columns and pd.notna(metrics_df[m].iloc[0])]\n",
    "             if valid_scores: score = np.mean(valid_scores)\n",
    "             else: logging.warning(f\"Trial {trial_num}: Brak poprawnych metryk.\"); score = 0.0\n",
    "        else: logging.warning(f\"Trial {trial_num}: Brak 'metrics' DF lub same NaN.\"); score = 0.0\n",
    "        logging.info(f\"  Trial {trial_num}: Obliczony wynik = {score:.6f}\")\n",
    "\n",
    "        # Pruning (opcjonalne, ale dobre dla Optuny)\n",
    "        trial.report(score, trial_num)\n",
    "        if trial.should_prune():\n",
    "             logging.info(f\"Trial {trial_num} pruned.\")\n",
    "             raise optuna.TrialPruned()\n",
    "\n",
    "    except optuna.TrialPruned: # Przechwyć i rzuć dalej\n",
    "         raise\n",
    "    except Exception as e_trial:\n",
    "        logging.error(f\"BŁĄD KRYTYCZNY w Trial {trial_num}: {e_trial}\", exc_info=True)\n",
    "        score = 0.0 \n",
    "\n",
    "    # Zwalnianie pamięci\n",
    "    vars_to_delete = ['X_train_aug', 'y_train_aug', 'pipeline', 'eval_results', 'model_instance',\n",
    "                      'current_estimators', 'qda_inst', 'hgb_inst', 'et_inst', 'metrics_df',\n",
    "                      'valid_scores', 'indices_to_augment', 'weak_indices', 'outlier_indices', 'params', 'umap_params']\n",
    "    for var_name in vars_to_delete:\n",
    "        if var_name in locals(): del locals()[var_name]\n",
    "    gc.collect()\n",
    "    return score\n",
    "\n",
    "# --- Inicjalizacja Augmentera ---\n",
    "augmenter = ImageAugmenter(config=AUGMENTATION_CONFIG, target_size=TARGET_IMG_SIZE)\n",
    "\n",
    "# --- Optymalizacja: Podział danych RAZ ---\n",
    "logging.info(\"Dzielenie upscalowanych danych na zbiory treningowe i walidacyjne...\")\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    smoothed_X_train, y_train, test_size=0.2,\n",
    "    stratify=y_train, random_state=RANDOM_STATE\n",
    ")\n",
    "logging.info(f\"Rozmiary zbiorów: Trn={X_train_split.shape}/{y_train_split.shape}, Val={X_val_split.shape}/{y_val_split.shape}\")\n",
    "del smoothed_X_train, y_train; gc.collect()\n",
    "\n",
    "# --- Optymalizacja: Pre-obliczenie UMAP dla LOF/Weak RAZ ---\n",
    "logging.info(f\"Pre-obliczanie UMAP ({PRE_UMAP_CONFIG['n_components']} komp.) dla LOF/Weak...\")\n",
    "X_train_flat_for_pre_umap = flatten(X_train_split) # Użyj funkcji flatten\n",
    "pre_umap_reducer = umap.UMAP(**PRE_UMAP_CONFIG) # Użyj konfiguracji\n",
    "try:\n",
    "    X_train_reduced_pre_calc = pre_umap_reducer.fit_transform(X_train_flat_for_pre_umap)\n",
    "    logging.info(f\"UMAP embedding (LOF/Weak) obliczony. Kształt: {X_train_reduced_pre_calc.shape}\")\n",
    "    del X_train_flat_for_pre_umap; gc.collect()\n",
    "except Exception as e_umap_pre: logging.error(f\"BŁĄD UMAP dla LOF/Weak: {e_umap_pre}\", exc_info=True); exit()\n",
    "\n",
    "# --- Konfiguracja i uruchomienie Optuny ---\n",
    "storage = optuna.storages.RDBStorage(OPTUNA_CONFIG['db_path'])\n",
    "try:\n",
    "    if OPTUNAHUB_AVAILABLE: sampler = optunahub.load_module(package=\"samplers/auto_sampler\").AutoSampler()\n",
    "    else: raise ImportError(\"Optunahub niezaładowany\")\n",
    "    logging.info(\"Używam AutoSampler z optunahub.\")\n",
    "except Exception: logging.warning(f\"Używam TPESampler.\"); sampler = optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
    "\n",
    "# Dynamiczne nazwy studiów\n",
    "study_name_umap = f\"{OPTUNA_CONFIG['study_name_prefix_umap']}\"\n",
    "study_name_no_umap = f\"{OPTUNA_CONFIG['study_name_prefix_no_umap']}\"\n",
    "\n",
    "# --- Wybór i uruchomienie JEDNEGO study na podstawie flagi ---\n",
    "if RUN_WITH_UMAP_IN_PIPELINE:\n",
    "    study_name = study_name_umap\n",
    "    use_umap_objective_flag = True\n",
    "    n_trials = OPTUNA_CONFIG['n_trials_umap']\n",
    "    logging.info(f\"--- Konfiguracja: Uruchamianie study Z UMAP w pipeline ('{study_name}') ---\")\n",
    "else:\n",
    "    study_name = study_name_no_umap\n",
    "    use_umap_objective_flag = False\n",
    "    n_trials = OPTUNA_CONFIG['n_trials_no_umap']\n",
    "    logging.info(f\"--- Konfiguracja: Uruchamianie study BEZ UMAP w pipeline ('{study_name}') ---\")\n",
    "\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage, sampler=sampler)\n",
    "    logging.info(f\"Wczytano istniejące study '{study_name}' z {OPTUNA_CONFIG['db_path']}.\")\n",
    "except KeyError:\n",
    "    study = optuna.create_study(direction=\"maximize\", storage=storage, study_name=study_name, sampler=sampler)\n",
    "    logging.info(f\"Utworzono nowe study '{study_name}' w {OPTUNA_CONFIG['db_path']}.\")\n",
    "\n",
    "logging.info(f\"--- Rozpoczynam optymalizację dla '{study_name}' ({n_trials} triali, {OPTUNA_CONFIG['n_jobs']} jobs) ---\")\n",
    "study.optimize(\n",
    "    lambda trial: objective(\n",
    "        trial,\n",
    "        X_train_split, y_train_split, X_val_split, y_val_split,\n",
    "        X_train_reduced_pre_calc,\n",
    "        use_umap_flag=use_umap_objective_flag,\n",
    "        evaluator_instance=evaluator,\n",
    "        augmenter_instance=augmenter # Przekaż instancję augmentera\n",
    "    ),\n",
    "    n_trials=n_trials, n_jobs=OPTUNA_CONFIG['n_jobs'],\n",
    "    gc_after_trial=True, callbacks=[callback]\n",
    ")\n",
    "\n",
    "# --- Wyświetlenie wyników dla uruchomionego study ---\n",
    "print(\"\\n\" + \"=\"*30 + \" WYNIKI OPTYMALIZACJI \" + \"=\"*30)\n",
    "print(f\"\\n--- Wyniki dla Study: {study.study_name} ---\")\n",
    "try:\n",
    "    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    if completed_trials:\n",
    "        print(f\"Liczba ukończonych triali: {len(completed_trials)}\")\n",
    "        print(f\"Najlepsza wartość: {study.best_value:.6f}\")\n",
    "        print(\"Najlepsze parametry:\")\n",
    "        for key, value in study.best_params.items(): print(f\"  {key}: {value}\")\n",
    "    else: print(\"Brak ukończonych triali dla tego study.\")\n",
    "except Exception as e: print(f\"Błąd podczas wyświetlania wyników dla study '{study.study_name}': {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X_train, y_train, n_neighbors_lof, contamination_lof, \n",
    "                    contamination_forest):\n",
    "    \"\"\"\n",
    "    Augmentuje dane treningowe poprzez:\n",
    "        1. Wykrywanie outlierów za pomocą LOF i IsolationForest.\n",
    "        2. Augmentację danych dla outlierów (przesunięcie, obrót, skalowanie, pochylenie).\n",
    "        3. Redukcję wymiarów UMAP dla całego zestawu danych.\n",
    "        4. Wizualizację outlierów i ich augmentacji.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Redukcja wymiarów UMAP dla LOF\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    X_train_reduced = reducer.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "\n",
    "    # 2. Wykrywanie outlierów (LOF)\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors_lof, contamination=contamination_lof)\n",
    "    y_pred_lof = lof.fit_predict(X_train_reduced)\n",
    "\n",
    "    # 3. Isolation Forest na surowych danych\n",
    "    iso_forest = IsolationForest(contamination=contamination_forest)\n",
    "    y_pred_iso = iso_forest.fit_predict(X_train.reshape(X_train.shape[0], -1))\n",
    "\n",
    "    # 4. Połączenie wyników LOF i IsolationForest\n",
    "    y_pred_combined = (y_pred_lof + y_pred_iso) / 2\n",
    "    outlier_indices = np.where(y_pred_combined < 0)[0]\n",
    "    print(f\"Liczba outlinerów: {len(outlier_indices)}\")\n",
    "    \n",
    "    \n",
    "    # 5. Augmentacja danych dla outlierów\n",
    "    X_train_augmented = []\n",
    "    y_train_augmented = []\n",
    "\n",
    "    if len(outlier_indices) > 0:\n",
    "        n_outliers_to_show = min(12, len(outlier_indices))  # Ograniczenie liczby wyświetlanych outlierów\n",
    "        fig, axes = plt.subplots(n_outliers_to_show, 5, figsize=(15, 3 * n_outliers_to_show))\n",
    "        k = 1  # Zmienna do iterowania po kolumnach\n",
    "        for i, idx in enumerate(outlier_indices[:n_outliers_to_show]):\n",
    "            image = X_train[idx]\n",
    "            label = y_train[idx]  # Pobranie etykiety dla outliera\n",
    "\n",
    "            # 1. Binaryzacja oryginalnego obrazu\n",
    "            _, binary_image = cv2.threshold(image.astype(np.uint8), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "            # 2. Znalezienie konturu cyfry\n",
    "            contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if len(contours) > 0:\n",
    "                cnt = contours[0]\n",
    "\n",
    "                # Oryginalny obraz\n",
    "                axes[i, 0].imshow(image, cmap='gray')\n",
    "                axes[i, 0].set_title(f'Outlier {i+1}')\n",
    "                axes[i, 0].axis('off')\n",
    "\n",
    "                # 1. Dodanie paddingu (czarne tło)\n",
    "                padded_image = np.pad(image, pad_width=35, mode='constant')  # Zwiększony padding\n",
    "                for j in range(4):  # Zmienna j zdefiniowana wewnątrz pętli\n",
    "                    # 2. Transformacja afiniczna\n",
    "                    transform = AffineTransform(scale=(np.random.uniform(0.97, 1.04), np.random.uniform(0.97, 1.04)),\n",
    "                                                rotation=np.deg2rad(np.random.randint(-20, 21)),\n",
    "                                                shear=np.deg2rad(np.random.randint(-5, 6))\n",
    "                                                )  \n",
    "                    transformed_image = warp(padded_image, transform, mode='wrap', preserve_range=True)\n",
    "                    \n",
    "                    # 3. Wyizolowanie cyfry po transformacji (Sobel)\n",
    "                    grad_x = cv2.Sobel(transformed_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "                    grad_y = cv2.Sobel(transformed_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "                    grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "                    grad_magnitude = cv2.normalize(grad_magnitude, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                    _, thresh = cv2.threshold(grad_magnitude, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                    contours = find_contours(thresh, 0.75)\n",
    "\n",
    "                    # --- Obsługa błędów ---\n",
    "                    try:\n",
    "                        cnt = contours[0]  # Wybór pierwszego konturu\n",
    "\n",
    "                        # Wyznaczenie bounding box\n",
    "                        minr = int(np.min(cnt[:, 0]))\n",
    "                        minc = int(np.min(cnt[:, 1]))\n",
    "                        maxr = int(np.max(cnt[:, 0]))\n",
    "                        maxc = int(np.max(cnt[:, 1]))\n",
    "\n",
    "                        # Sprawdzenie, czy bounding box ma poprawny kształt\n",
    "                        if minr == maxr or minc == maxc:\n",
    "                            print(\"Nieprawidłowy bounding box - pomijanie augmentacji dla tego obrazu.\")\n",
    "                            continue\n",
    "\n",
    "                    except IndexError:\n",
    "                        print(\"Nie znaleziono konturu - pomijanie augmentacji dla tego obrazu.\")\n",
    "                        continue\n",
    "\n",
    "                    # 4. Wycięcie cyfry na podstawie bounding box\n",
    "                    isolated_digit = transformed_image[minr:maxr, minc:maxc]\n",
    "                \n",
    "                    # --- Erozja ---\n",
    "                    if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                        isolated_digit = erosion(isolated_digit, disk(1))\n",
    "                        \n",
    "                    # --- Otwarcie  ---\n",
    "                    if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                        isolated_digit = opening(isolated_digit, disk(1))\n",
    "                        \n",
    "                    # --- Zamknięcie \n",
    "                    if np.random.rand() < 0.5:  # Zastosuj z prawdopodobieństwem 50%\n",
    "                        isolated_digit = closing(isolated_digit, disk(1))\n",
    "                                    \n",
    "                    \n",
    "                    # 5. Wycentrowanie cyfry w obrazie 42x42\n",
    "                    new_image = np.zeros((42, 42), dtype=np.uint8)\n",
    "                    row_offset = max(0, (42 - (maxr - minr)) // 2)\n",
    "                    col_offset = max(0, (42 - (maxc - minc)) // 2)\n",
    "                    new_image[row_offset:row_offset + maxr - minr, col_offset:col_offset + maxc - minc] = isolated_digit\n",
    "\n",
    "                    isolated_digit = new_image\n",
    "\n",
    "                    # 6. Dodanie do listy\n",
    "                    X_train_augmented.append(isolated_digit)  \n",
    "                    y_train_augmented.append(label)  \n",
    "\n",
    "\n",
    "                    # Wyświetlenie obrazu\n",
    "                    axes[i, k].imshow(new_image, cmap='gray')\n",
    "                    axes[i, k].axis('off')\n",
    "                    k += 1  # Przejście do następnej kolumny\n",
    "                    if k == 5:  # Jeśli osiągnięto ostatnią kolumnę\n",
    "                        k = 1  # Resetuj k do pierwszej kolumny dla następnego outliera\n",
    "            else:\n",
    "                print(\"Nie znaleziono konturu - pomijanie augmentacji dla tego obrazu.\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    X_train_augmented = np.array(X_train_augmented)\n",
    "    y_train_augmented = np.array(y_train_augmented)\n",
    "\n",
    "    # Połączenie danych augmentowanych z oryginalnymi danymi\n",
    "    X_train_augmented = np.concatenate((X_train, X_train_augmented), axis=0)\n",
    "    y_train_augmented = np.concatenate((y_train, y_train_augmented), axis=0)\n",
    "\n",
    "    return X_train_augmented, y_train_augmented, outlier_indices, X_train_reduced\n",
    "\n",
    "\n",
    "X_train_Upsc, X_val_Upsc, y_train_Upsc, y_val_Upsc = train_test_split(smoothed_X_train, y_train, test_size=0.2, random_state=69)\n",
    "\n",
    "# Wywołanie funkcji augment_data\n",
    "X_train_augmented, y_train_augmented, outlier_indices, X_train_reduced = augment_data(\n",
    "    X_train_Upsc,\n",
    "    y_train_Upsc,\n",
    "    n_neighbors_lof=399,\n",
    "    contamination_lof=0.33,\n",
    "    contamination_forest=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetV2B0, EfficientNetV2S, EfficientNetV2M, EfficientNetV2L \n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def create_efficientnetv2_model(\n",
    "    input_shape,\n",
    "    num_classes,\n",
    "    efficientnet_version=\"B0\",\n",
    "    trainable=False,\n",
    "    dropout_rate=0.2,\n",
    "    weights=None\n",
    "):\n",
    "    \"\"\"Tworzy model oparty na EfficientNetV2.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Dane wejściowe.\n",
    "        num_classes: Liczba klas.\n",
    "        efficientnet_version: Wersja EfficientNetV2 ('B0', 'S', 'M', 'L').\n",
    "        trainable: Czy warstwy EfficientNetV2 mają być trenowalne.\n",
    "        dropout_rate: Współczynnik dropout.\n",
    "\n",
    "    Returns:\n",
    "        Model Keras.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wybór wersji EfficientNetV2\n",
    "    if efficientnet_version == \"B0\":\n",
    "        base_model = EfficientNetV2B0(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    elif efficientnet_version == \"S\":\n",
    "        base_model = EfficientNetV2S(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    elif efficientnet_version == \"M\":\n",
    "        base_model = EfficientNetV2M(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    elif efficientnet_version == \"L\":\n",
    "        base_model = EfficientNetV2L(\n",
    "            include_top=False, weights=weights, input_shape=input_shape\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Nieprawidłowa wersja EfficientNetV2.\")\n",
    "\n",
    "    # Ustawienie warstw EfficientNetV2 jako trenowalne\n",
    "    base_model.trainable = trainable\n",
    "\n",
    "    # Budowa modelu\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Baseline data without transformations and agumentation \n",
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(smoothed_X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "\n",
    "#{'n_neighbors_lof': 222, 'contamination_lof': 0.1625, 'contamination_forest': 0.195}.\n",
    "# Odtworzenie augmentacji z najlepszymi parametrami\n",
    "X_train_augmented, y_train_augmented, _, _ = augment_data(\n",
    "    X_train_base, \n",
    "    y_train_base, \n",
    "    n_neighbors_lof=222,\n",
    "    contamination_lof=0.1625,\n",
    "    contamination_forest=0.195\n",
    ")\n",
    "\n",
    "X_train_base = np.expand_dims(X_train_base, axis=-1)\n",
    "X_val_base = np.expand_dims(X_val_base, axis=-1)  \n",
    "X_train_augmented = np.expand_dims(X_train_augmented, axis=-1)\n",
    "\n",
    "X_test = np.squeeze(X_test) \n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "print(\"Shape of X_train_base:\", X_train_base.shape)\n",
    "print(\"Shape of y_train_base:\", y_train_base.shape)\n",
    "\n",
    "print(\"Shape of X_val_base:\", X_val_base.shape)\n",
    "print(\"Shape of y_val_base:\", y_val_base.shape)\n",
    "\n",
    "print(\"Shape of X_train_augmented:\", X_train_augmented.shape)\n",
    "print(\"Shape of y_train_augmented:\", y_train_augmented.shape)\n",
    "\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test_ENC.shape)\n",
    "\n",
    "# --- Trenowanie modeli ---\n",
    "# Model na danych bez redukcji\n",
    "model_base = create_efficientnetv2_model(\n",
    "    input_shape=X_train_base.shape[1:], \n",
    "    num_classes=10, \n",
    "    efficientnet_version=\"S\",\n",
    "    trainable=True,\n",
    "    weights=None,\n",
    ")\n",
    "\n",
    "model_base.compile(\n",
    "    optimizer=\"Lion\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"] \n",
    ")\n",
    "\n",
    "history_base = model_base.fit(\n",
    "    X_train_base,\n",
    "    y_train_base,\n",
    "    epochs=36,\n",
    "    batch_size=512,\n",
    "    validation_data=(X_val_base, y_val_base),\n",
    ")\n",
    "\n",
    "# Model na danych z redukcją\n",
    "model_transformed = create_efficientnetv2_model(\n",
    "    input_shape=X_train_augmented.shape[1:],  \n",
    "    num_classes=10, \n",
    "    efficientnet_version=\"S\",\n",
    "    trainable=True,\n",
    "    weights=None,\n",
    ")\n",
    "\n",
    "model_transformed.compile(\n",
    "    optimizer=\"Lion\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"] \n",
    ")\n",
    "\n",
    "history_transformed = model_transformed.fit(\n",
    "    X_train_augmented,\n",
    "    y_train_augmented,\n",
    "    epochs=36,\n",
    "    batch_size=512,\n",
    "    validation_data=(X_val_base, y_val_base),\n",
    ")\n",
    "\n",
    "# --- Predykcja ---\n",
    "y_pred_base = model_base.predict(X_test)\n",
    "y_pred_transformed = model_transformed.predict(X_test)\n",
    "\n",
    "# --- Porównanie wyników ---\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Oblicza metryki.\"\"\"\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true_labels, y_pred_labels),\n",
    "        \"precision\": precision_score(y_true_labels, y_pred_labels, average=\"macro\"),\n",
    "        \"recall\": recall_score(y_true_labels, y_pred_labels, average=\"macro\"),\n",
    "        \"f1\": f1_score(y_true_labels, y_pred_labels, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "metrics_base = calculate_metrics(y_test_ENC, y_pred_base)\n",
    "metrics_transformed = calculate_metrics(y_test_ENC, y_pred_transformed)\n",
    "\n",
    "print(\"Metryki dla modelu bez agumentacji:\", metrics_base)\n",
    "print(\"Metryki dla modelu z agumentacją:\", metrics_transformed)\n",
    "\n",
    "## Model B0\n",
    "# Dla danych Walidacyjnych (Pochodzące z zbiru danych Treningowych)\n",
    "# Metryki dla modelu bez agumentacji: {'accuracy': 0.9922,  'precision': 0.9921916598777563, 'recall': 0.9922927046235455, 'f1': 0.992236418206011}\n",
    "# Metryki dla modelu z agumentacją:   {'accuracy': 0.9895,  'precision': 0.9896354567051183, 'recall': 0.9895893078591114, 'f1': 0.9895911709150041}\n",
    "\n",
    "# Metryki dla danych testowych\n",
    "#  Metryki dla modelu bez agumentacji: {'accuracy': 0.9816, 'precision': 0.9814288255960995, 'recall': 0.9815351078380796, 'f1': 0.9814747112782948}\n",
    "#  Metryki dla modelu z agumentacją:   {'accuracy': 0.9896, 'precision': 0.9896281452846342, 'recall': 0.9894803578385878, 'f1': 0.9895399474757763}\n",
    "\n",
    "## Model S\n",
    "# Metryki dla danych testowych\n",
    "# Metryki dla modelu bez agumentacji: {'accuracy': 0.9925, 'precision': 0.9924837719191659, 'recall': 0.9924530861244054, 'f1': 0.992463415535193}\n",
    "# Metryki dla modelu z agumentacją:   {'accuracy': 0.9921, 'precision': 0.9920614949658724, 'recall': 0.9920511460346548, 'f1': 0.9920482305787767}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deklaracja funkcji dla ekstrakcji cech z obrazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_hog_parallel(images, orientations=None, pixels_per_cell=None, cells_per_block=None, n_jobs=-1):\n",
    "    \"\"\"Oblicza deskryptory HOG dla obrazu.\"\"\"\n",
    "    \n",
    "     # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if orientations is None:\n",
    "        orientations = 9\n",
    "    if pixels_per_cell is None:\n",
    "        pixels_per_cell = (8, 8)\n",
    "    if cells_per_block is None:\n",
    "        cells_per_block = (2, 2)\n",
    "    \n",
    "    def calculate_hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block):\n",
    "        \n",
    "        image = img_as_float32(image)\n",
    "\n",
    "        fd = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                cells_per_block=cells_per_block, visualize=False, channel_axis=None)\n",
    "\n",
    "        return fd\n",
    "    \n",
    "    return Parallel(n_jobs=n_jobs)(delayed(calculate_hog)(image, orientations, pixels_per_cell, cells_per_block) for image in images)\n",
    "\n",
    "\n",
    "def apply_gabor_filters_parallel(images, thetas=None, sigmas=None, frequencies=None, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Stosuje filtry Gabora do wielu obrazów równolegle.\n",
    "\n",
    "    Argumenty:\n",
    "        images: Lista obrazów wejściowych.\n",
    "        thetas: (Opcjonalnie) Lista kątów orientacji filtrów Gabora (w radianach). Jeśli None, używane są wartości domyślne.\n",
    "        sigmas: (Opcjonalnie) Lista odchyleń standardowych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        frequencies: (Opcjonalnie) Lista częstotliwości przestrzennych filtrów Gabora. Jeśli None, używane są wartości domyślne.\n",
    "        n_jobs: Liczba rdzeni procesora do wykorzystania podczas zrównoleglania. -1 oznacza użycie wszystkich dostępnych rdzeni.\n",
    "\n",
    "    Zwraca:\n",
    "        Listę tablic NumPy zawierających obrazy przefiltrowane filtrami Gabora, o typie danych float32.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ustawienie wartości domyślnych, jeśli parametry nie są podane\n",
    "    if thetas is None:\n",
    "        thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    if sigmas is None:\n",
    "        sigmas = [1, 3]\n",
    "    if frequencies is None:\n",
    "        frequencies = [0.05, 0.25]\n",
    "\n",
    "    def apply_gabor_filters_single(image):\n",
    "        image = img_as_float32(image)\n",
    "        filtered_images = [gabor(image, frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)[0]\n",
    "                           for theta in thetas for sigma in sigmas for frequency in frequencies]\n",
    "        return np.array(filtered_images, dtype=np.float32)\n",
    "\n",
    "    return Parallel(n_jobs=n_jobs)(delayed(apply_gabor_filters_single)(image) for image in images)\n",
    "\n",
    "\n",
    "def extract_features_HOG_Gabor(X, hog_orientations=None, hog_pixels_per_cell=None, hog_cells_per_block=None, \n",
    "                              gabor_thetas=None, gabor_sigmas=None, gabor_frequencies=None):\n",
    "    \"\"\"Generator zwracający cechy HOG i Gabora dla obrazów.\"\"\"\n",
    "\n",
    "    hog_features_list = calculate_hog_parallel(X, orientations=hog_orientations, pixels_per_cell=hog_pixels_per_cell, cells_per_block=hog_cells_per_block)\n",
    "    gabor_features_list = apply_gabor_filters_parallel(X, thetas=gabor_thetas, sigmas=gabor_sigmas, frequencies=gabor_frequencies)\n",
    "\n",
    "    for hog_features, gabor_features in zip(hog_features_list, gabor_features_list):\n",
    "        \n",
    "        gabor_features_flattened = np.array([img.flatten() for img in gabor_features])\n",
    "\n",
    "        # Dopasowujemy kształt hog_features do gabor_features_flattened\n",
    "        hog_features_reshaped = np.repeat(hog_features.reshape(1, -1), gabor_features_flattened.shape[0], axis=0)\n",
    "\n",
    "        # Łączymy cechy HOG i Gabora w poziomie\n",
    "        concatenated_features = np.hstack((hog_features_reshaped, gabor_features_flattened))\n",
    "\n",
    "        concatenated_features = concatenated_features.flatten()\n",
    "\n",
    "        yield concatenated_features\n",
    "\n",
    "#################################################################################################################################################\n",
    "################################################  Directional filters, ORB    ###################################################################\n",
    "\n",
    "def apply_directional_filters(image, kernel_sizes=[3, 5]):\n",
    "    \"\"\"Stosuje filtry kierunkowe do obrazu.\"\"\"\n",
    "    filtered_images = []\n",
    "\n",
    "    # Wykrywanie krawędzi filtrem Canny'ego\n",
    "    edges = cv2.Canny(image, 50, 150)\n",
    "\n",
    "    def process_kernel(kernel_size):\n",
    "        if kernel_size % 2 == 0 or kernel_size > 31:\n",
    "            raise ValueError(\n",
    "                f\"Nieprawidłowy rozmiar jądra: {kernel_size}. Rozmiar jądra musi być nieparzysty i nie większy niż 31.\"\n",
    "            )\n",
    "        kernel_horizontal = cv2.getDerivKernels(1, 0, kernel_size,\n",
    "                                               normalize=True)\n",
    "        kernel_vertical = cv2.getDerivKernels(0, 1, kernel_size,\n",
    "                                             normalize=True)\n",
    "        filtered = cv2.sepFilter2D(edges, cv2.CV_32F, kernel_horizontal[0],\n",
    "                                  kernel_vertical[0])\n",
    "        return filtered.flatten()\n",
    "\n",
    "    with Pool() as pool:\n",
    "        filtered_images = pool.map(process_kernel, kernel_sizes)\n",
    "\n",
    "    # Analiza składowych spójnych\n",
    "    labels = label(edges)\n",
    "    props = regionprops(labels)\n",
    "    features = [prop.area for prop in props]\n",
    "\n",
    "    return np.concatenate(\n",
    "        [np.array(filtered_images).squeeze(), np.array(features)])\n",
    "\n",
    "\n",
    "def extract_orb_features(image, nfeatures=100):\n",
    "    \"\"\"Wyodrębnia cechy ORB z obrazu.\"\"\"\n",
    "    orb = cv2.ORB_create(nfeatures=nfeatures,\n",
    "                         fastThreshold=12,\n",
    "                         edgeThreshold=12)\n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        return []\n",
    "    return descriptors.flatten()\n",
    "\n",
    "\n",
    "def extract_combined_features(image, kernel_sizes=[3, 5], nfeatures=100):\n",
    "    \"\"\"Łączy cechy z filtrów kierunkowych i ORB.\"\"\"\n",
    "    directional_features = apply_directional_filters(image, kernel_sizes)\n",
    "    orb_features = extract_orb_features(image, nfeatures)\n",
    "\n",
    "    # Łączenie cech\n",
    "    combined_features = np.concatenate((directional_features, orb_features))\n",
    "\n",
    "    return combined_features\n",
    "   \n",
    "#################################################################################################################################################\n",
    "################################################  GLCM, Zernike    ##############################################################################\n",
    "def extract_glcm_features(image, distances=[1], angles=[0], properties=['contrast', 'energy', 'homogeneity', 'correlation']):\n",
    "    \"\"\"Wyodrębnia cechy GLCM z obrazu.\"\"\"\n",
    "    if len(image.shape) > 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    glcm = graycomatrix(image, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    features = []\n",
    "    for prop in properties:\n",
    "        features.extend(graycoprops(glcm, prop).flatten())\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTUNA dla HOG, Gabor, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", module=\"optuna\")\n",
    "\n",
    "\n",
    "# def objective(trial, X_train, y_train):\n",
    "#     \"\"\"\n",
    "#     Funkcja celu dla optymalizacji Optuna. \n",
    "#     Przyjmuje próbę (trial), dane treningowe (X_train) i etykiety (y_train).\n",
    "#     Zwraca dokładność QDA i SVM na danych walidacyjnych po zastosowaniu sekwencji transformacji.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Parametry HOG\n",
    "#     orientations      = trial.suggest_int(\"hog_orientations\", 2, 16)\n",
    "#     pixels_per_cell   = trial.suggest_categorical(\"hog_pixels_per_cell\", [str((2, 2)), str((2, 3)), str((3, 2)), str((3, 3)), str((4, 4)), str((5, 5)),str((6, 6)), str((7, 7))])\n",
    "#     cells_per_block   = trial.suggest_categorical(\"hog_cells_per_block\", [str((1, 1)), str((2, 2)), str((3, 3)), str((4, 4))])\n",
    "\n",
    "#     # Parametry filtrów Gabora\n",
    "#     gabor_thetas      = trial.suggest_categorical(\"gabor_thetas\", ([0],  [np.pi / 14],  [np.pi / 12], [0, np.pi / 14 ], [0, np.pi / 12 ] ))\n",
    "#     gabor_sigmas      = trial.suggest_categorical(\"gabor_sigmas\", ([0.33], [0.66], [1], [0.33 , 1.25], [0.66, 2.35], [1, 3]))\n",
    "#     gabor_frequencies = trial.suggest_categorical(\"gabor_frequencies\", ([0.025], [0.05], [0.125], [0.025, 0.125], [0.05, 0.175], [0.05, 0.25]))\n",
    "        \n",
    "#     #PCA\n",
    "#     PCA_n_components  =  trial.suggest_int(\"PCA_Components\", 4, 228)\n",
    "#     kernel = trial.suggest_categorical(\"kernel_pca_kernel\", ['linear', 'poly', 'rbf', 'sigmoid', 'cosine'])\n",
    "#     gamma = None\n",
    "#     if kernel in ['poly', 'rbf']:\n",
    "#         gamma = trial.suggest_loguniform(\"kernel_pca_gamma\", 0.001, 1)\n",
    "        \n",
    "#     # Powiększanie i wygładzanie\n",
    "#     # X_train_split = np.array([enlarge_and_smooth(img, new_size) for img in X_train])\n",
    "#     # X_val = np.array([enlarge_and_smooth(img, new_size) for img in X_val])\n",
    "    \n",
    "#     # Ekstrakcja cech HOG i Gabor\n",
    "#     X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=orientations, \n",
    "#         hog_pixels_per_cell=eval(pixels_per_cell), # Konwersja stringa na krotkę\n",
    "#         hog_cells_per_block=eval(cells_per_block),  # Konwersja stringa na krotkę\n",
    "#         gabor_thetas=gabor_thetas, \n",
    "#         gabor_sigmas=gabor_sigmas, \n",
    "#         gabor_frequencies=gabor_frequencies\n",
    "#     )))\n",
    "\n",
    "#     # Podział na zbiór treningowy i walidacyjny\n",
    "#     X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train, test_size=0.2, random_state=69)\n",
    "    \n",
    "    \n",
    "#     # Kernel PCA\n",
    "#     kpca = KernelPCA(n_components=PCA_n_components, kernel=kernel, gamma=gamma)\n",
    "#     X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "#     X_val_reduced = kpca.transform(X_val_filters)\n",
    "                \n",
    "#     # SVC\n",
    "#     svc = SVC(kernel='rbf')\n",
    "#     svc.fit(X_train_reduced, y_train_filters)\n",
    "#     y_pred_svm = svc.predict(X_val_reduced)\n",
    "#     accuracy_svm = accuracy_score(y_val_filters, y_pred_svm)\n",
    "\n",
    "#     return accuracy_svm\n",
    "\n",
    "# # Optymalizacji\n",
    "                            \n",
    "# study = optuna.create_study(directions=[\"maximize\"], sampler=optuna.samplers.CmaEsSampler() ) \n",
    "# study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, n_jobs=1)\n",
    "\n",
    "\n",
    "# print(\"Best parameters:\", study.best_params)\n",
    "# print(\"Best QDA accuracy:\", study.best_trial.values[0]) \n",
    "# print(\"Best SVM accuracy:\", study.best_trial.values[1]) \n",
    "\n",
    "# # [I 2024-09-17 05:43:37,964] Trial 13 finished with value: 0.99133 and parameters: {'hog_orientations': 16, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(3, 3)', 'gabor_thetas': [0.2243994752564138], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.025], 'PCA_Components': 209, 'kernel_pca_kernel': 'cosine'}. Best is trial 13 with value: 0.9913333333333333.\n",
    "# # [I 2024-09-17 08:45:58,345] Trial 19 finished with value: 0.99175 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(2, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.025], 'PCA_Components': 132, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 19 with value: 0.99175.\n",
    "# # [I 2024-09-17 10:45:29,605] Trial 23 finished with value: 0.99225 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(3, 3)', 'hog_cells_per_block': '(2, 2)', 'gabor_thetas': [0], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 150, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:24:59,082] Trial 28 finished with value: 0.99108 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(1, 1)', 'gabor_thetas': [0, 0.2617993877991494], 'gabor_sigmas': [1], 'gabor_frequencies': [0.05, 0.25], 'PCA_Components': 190, 'kernel_pca_kernel': 'sigmoid'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 13:54:44,248] Trial 29 finished with value: 0.9915 and parameters: {'hog_orientations': 7, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0.2617993877991494], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05], 'PCA_Components': 164, 'kernel_pca_kernel': 'cosine'}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 18:02:03,444] Trial 37 finished with value: 0.99066 and parameters: {'hog_orientations': 9, 'hog_pixels_per_cell': '(3, 2)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0], 'gabor_sigmas': [0.66, 2.35], 'gabor_frequencies': [0.05, 0.175], 'PCA_Components': 169, 'kernel_pca_kernel': 'rbf', 'kernel_pca_gamma': 0.0032186977490984547}. Best is trial 23 with value: 0.99225.\n",
    "# # [I 2024-09-17 19:02:58,946] Trial 39 finished with value: 0.9915 and parameters: {'hog_orientations': 6, 'hog_pixels_per_cell': '(4, 4)', 'hog_cells_per_block': '(4, 4)', 'gabor_thetas': [0, 0.2243994752564138], 'gabor_sigmas': [1, 3], 'gabor_frequencies': [0.125], 'PCA_Components': 148, 'kernel_pca_kernel': 'linear'}. Best is trial 23 with value: 0.99225.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystanie wcześniej otrzymanch hiperparamatrów dla HOG i filtrów Gabora po KPCA w celu stworzenia wektora cech z obrazów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_features = np.array(list(extract_features_HOG_Gabor(\n",
    "#         X_train, \n",
    "#         hog_orientations=6, \n",
    "#         hog_pixels_per_cell=eval('(3, 3)'), \n",
    "#         hog_cells_per_block=eval('(2, 2)'),  \n",
    "#         gabor_thetas=[0], \n",
    "#         gabor_sigmas=[1], \n",
    "#         gabor_frequencies=[0.05, 0.25]\n",
    "#     )))\n",
    "\n",
    "\n",
    "# kpca = KernelPCA(n_components=150, kernel='cosine')\n",
    "\n",
    "# X_train_filters, X_val_filters, y_train_filters, y_val_filters = train_test_split(X_train_features, y_train_ENC, test_size=0.2, random_state=69)\n",
    "\n",
    "# X_train_reduced = kpca.fit_transform(X_train_filters)\n",
    "# X_val_reduced = kpca.transform(X_val_filters)\n",
    "\n",
    "\n",
    "# np.save('X_train_reduced.npy', X_train_reduced)\n",
    "# np.save('X_val_reduced.npy', X_val_reduced)\n",
    "# np.save('y_train_filters.npy', y_train_filters)\n",
    "# np.save('y_val_filters.npy', y_val_filters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pierwszy model na sieci konwolucyjnej dla bazy z HOG i Gabora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LayerNormalization , BatchNormalization, PReLU , Input, ActivityRegularization, GlobalAveragePooling1D, SeparableConv1D \n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Lion\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "X_train_reduced = np.load('X_train_reduced.npy')\n",
    "X_val_reduced = np.load('X_val_reduced.npy')\n",
    "y_train_filters = np.load('y_train_filters.npy')\n",
    "y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "def calculate_max_pool_size(input_length, prev_pool_size, min_output_size=2):\n",
    "    calculated_pool_size = input_length // prev_pool_size\n",
    "    return max(min_output_size, calculated_pool_size)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Funkcja celu dla Optuny.\"\"\"\n",
    "    #os.environ['TF_NUM_INTRAOP_THREADS'] = '1' ## Czy Kers działa na pojedycznym wątku ????\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    activation_functions = ['relu', 'elu', 'leaky_relu', 'tanh']\n",
    "    activation = trial.suggest_categorical('activation', activation_functions)\n",
    "\n",
    "    # Definiowanie modelu\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_reduced.shape[1], 1)))\n",
    "\n",
    "    # Pierwsza warstwa konwolucyjna\n",
    "    model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_1', 48, 450),\n",
    "                     kernel_size=trial.suggest_int('kernel_size_1', 2, 16),\n",
    "                     #activation=activation,\n",
    "                     padding='same', \n",
    "                     #dilation_rate=trial.suggest_int('dilation_rate_1', 1, 4)\n",
    "                     )\n",
    "              )\n",
    "    model.add(PReLU())                #$$$$$$$ LAYER przed PRelu\n",
    "    model.add(LayerNormalization())   ######\n",
    "    model.add(ActivityRegularization(l1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True), \n",
    "                                     l2=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "                                     )\n",
    "              )\n",
    "    pool_size_1 = trial.suggest_int('pool_size_1', 2, 10)\n",
    "    model.add(MaxPooling1D(pool_size=pool_size_1))\n",
    "\n",
    "\n",
    "\n",
    "    # Druga warstwa konwolucyjna\n",
    "    input_length_2 = X_train_reduced.shape[1] // pool_size_1\n",
    "    model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_2', 24, 400),\n",
    "                     kernel_size=trial.suggest_int('kernel_size_2', 2, 14),\n",
    "                     padding='same',\n",
    "                     #activation=activation,\n",
    "                     #dilation_rate=trial.suggest_int('dilation_rate_2', 1, 4)\n",
    "                     )\n",
    "              )\n",
    "    model.add(PReLU())\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(ActivityRegularization(l1=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True), \n",
    "                                     l2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "                                     )\n",
    "              )\n",
    "    pool_size_2 = trial.suggest_int('pool_size_2', 2, input_length_2 // 2)\n",
    "    #pool_size_2 = trial.suggest_int('pool_size_2', 2, 10)\n",
    "    model.add(MaxPooling1D(pool_size=pool_size_2))\n",
    "\n",
    "\n",
    "\n",
    "    # Trzecia warstwa konwolucyjna\n",
    "    #input_length_3 = input_length_2 // pool_size_2\n",
    "    model.add(Conv1D(filters=trial.suggest_int('conv1d_filters_3', 32, 300),\n",
    "                     kernel_size=trial.suggest_int('kernel_size_3', 2, 12),\n",
    "                     padding='same',\n",
    "                     )\n",
    "              )\n",
    "    model.add(PReLU())\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Dropout(rate=trial.suggest_float('dropout_rate_3', 0.01, 0.5)\n",
    "                      )\n",
    "              )\n",
    "    #pool_size_3 = trial.suggest_int('pool_size_3', 2, max(2, input_length_3 // 2))\n",
    "    #pool_size_3 = trial.suggest_int('pool_size_3', 2, 8)\n",
    "    #model.add(MaxPooling1D(pool_size=pool_size_3))\n",
    "\n",
    "    # Warstwy Dense\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "    model.add(Dense(units=trial.suggest_int('dense_units_1', 40, 300), activation=activation))\n",
    "    model.add(Dropout(rate=trial.suggest_float('dropout_rate_4', 0.01, 0.75)))\n",
    "    \n",
    "    model.add(Dense(units=trial.suggest_int('dense_units_2', 20, 200), activation=activation))\n",
    "    model.add(Dropout(rate=trial.suggest_float('dropout_rate_5', 0.01, 0.65)))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Optymalizator\n",
    "    lr_initial = trial.suggest_float(\"lr_initial\", 1e-5, 1e-1, log=True)\n",
    "    decay_steps = trial.suggest_int(\"decay_steps\", 10, 150)\n",
    "    decay_rate = trial.suggest_float(\"decay_rate\", 0.01, 0.99)\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate=lr_initial,\n",
    "                                  decay_steps=decay_steps,\n",
    "                                  decay_rate=decay_rate)\n",
    "\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Lion'])\n",
    "    optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    optimizer = optimizer_class(learning_rate=lr_schedule)\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=16, start_from_epoch=10, min_delta = 0.00005)\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    model.fit(X_train_reduced, y_train_filters,\n",
    "              epochs=trial.suggest_int('epochs', 40, 150),\n",
    "              batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "              validation_data=(X_val_reduced, y_val_filters),\n",
    "              verbose=1,\n",
    "              callbacks=[early_stopping])\n",
    "\n",
    "    # Ewaluacja modelu\n",
    "    _, accuracy = model.evaluate(X_val_reduced, y_val_filters, verbose=0)\n",
    "\n",
    "    del model\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n",
    "print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "#[I 2024-10-04 05:18:28,377] Trial 26 finished with value: 0.9810000061988831 and parameters: {'activation': 'elu', 'conv1d_filters_1': 347, 'kernel_size_1': 7,  'l1_1': 0.0002141584459916536, 'l2_1': 4.201061472934882e-06, 'pool_size_1': 6, 'conv1d_filters_2': 77, 'kernel_size_2': 14, 'l1_2': 3.203529032832527e-06, 'l2_2': 0.00015793896479891322, 'pool_size_2': 2, 'conv1d_filters_3': 258, 'kernel_size_3': 4, 'dropout_rate_3': 0.3297488074504043, 'dense_units_1': 271, 'dropout_rate_4': 0.5097723961373417, 'dense_units_2': 78, 'dropout_rate_5': 0.15473996425601394, 'lr_initial': 0.0004175922071628326, 'decay_steps': 33, 'decay_rate': 0.9780884395210858, 'optimizer': 'Lion', 'epochs': 123, 'batch_size': 264}. Best is trial 26 with value: 0.9810000061988831.\n",
    "#[I 2024-10-04 14:58:14,236] Trial 61 finished with value: 0.9797499775886536 and parameters: {'activation': 'elu', 'conv1d_filters_1': 424, 'kernel_size_1': 13, 'l1_1': 0.0005501543495144062, 'l2_1': 0.02794377888054429,   'pool_size_1': 7, 'conv1d_filters_2': 79, 'kernel_size_2': 12, 'l1_2': 1.3699264627909325e-06, 'l2_2': 6.577814668979489e-06, 'pool_size_2': 3, 'conv1d_filters_3': 280, 'kernel_size_3': 3, 'dropout_rate_3': 0.21341586312675653, 'dense_units_1': 292, 'dropout_rate_4': 0.22415600475665626, 'dense_units_2': 106, 'dropout_rate_5': 0.09686035013603865, 'lr_initial': 0.0004430745906851504, 'decay_steps': 22, 'decay_rate': 0.9872365423551039, 'optimizer': 'Lion', 'epochs': 150, 'batch_size': 216}. Best is trial 26 with value: 0.9810000061988831.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieć CONV2D z zamrożonym PCA dla danych pierwotnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# import tensorflow_probability as tfp\n",
    "# import optuna\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "# X_train_image = X_train_image.astype('float32')\n",
    "# X_val_image = X_val_image.astype('float32')\n",
    "# X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "# X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "# def objective(trial):\n",
    "#     # Przestrzeń hiperparametrów\n",
    "#     num_filters1 = trial.suggest_int(\"num_filters1\", 32, 256)    # Pierwsza warstwa Conv2d\n",
    "#     num_filters2 = trial.suggest_int(\"num_filters2\", 64, 256)    # Druga warstwa Conv2d\n",
    "#     num_filters3 = trial.suggest_int(\"num_filters3\", 128, 256)   # Trzecia warstwa Conv2d\n",
    "#     n_components = trial.suggest_int(\"PCA_components\", 350, 500) # PCA\n",
    "\n",
    "#     l1_1=trial.suggest_float('l1_1', 1e-6, 1e-1, log=True)\n",
    "#     l2_1=trial.suggest_float('l2_1', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     l1_2=trial.suggest_float('l1_2', 1e-6, 1e-1, log=True)\n",
    "#     l2_2=trial.suggest_float('l2_2', 1e-6, 1e-1, log=True)\n",
    "\n",
    "#     l1_3=trial.suggest_float('l1_3', 1e-6, 1e-1, log=True)\n",
    "#     l2_3=trial.suggest_float('l2_3', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     activation_functions = ['relu', 'gelu', 'leaky_relu', 'silu', 'mish']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "    \n",
    "    \n",
    "#     # Warstwy konwolucyjne 2D\n",
    "#     conv_layers = [\n",
    "#         # Pierwsza warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters1,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_1),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_1),\n",
    "#                                 padding='same',\n",
    "#                                 input_shape=(X_train_image.shape[1:])\n",
    "#                                 ),\n",
    "        \n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2,strides=1,\n",
    "#                                  ),\n",
    "#         # Druga warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters2,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_2),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_2),                                \n",
    "#                                 padding='same',\n",
    "#                                 ),\n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "#         # Trzecia warstwa Conv2d\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#                                 num_filters3,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation=activation,\n",
    "#                                 kernel_regularizer=tf.keras.regularizers.l2(l2_3),\n",
    "#                                 bias_regularizer=tf.keras.regularizers.l1(l1_3),                                \n",
    "#                                 padding='same'\n",
    "#                                 ),\n",
    "#         tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        \n",
    "#         tf.keras.layers.Flatten(),\n",
    "#                 ]\n",
    "\n",
    "#     ### Obliczenia dla zamrożonego PCA na podstawie pierwszej inicjalizacji modelu\n",
    "#     conv_model = keras.models.Sequential(conv_layers)  \n",
    "#     X_train_PCA = conv_model.predict(X_train_image)\n",
    "    \n",
    "#     X_train_PCA = tf.reshape(X_train_PCA, [X_train_PCA.shape[0], -1])\n",
    "\n",
    "#     # Macierz kowariancji\n",
    "#     covariance_matrix = tfp.stats.covariance(X_train_PCA, sample_axis=0, event_axis=-1)\n",
    "#     # Wektory i wartości własne\n",
    "#     eigenvalues, eigenvectors = tf.linalg.eigh(covariance_matrix)\n",
    "#     sorted_indices = tf.argsort(eigenvalues, direction='DESCENDING')\n",
    "#     eigenvalues = tf.gather(eigenvalues, sorted_indices)\n",
    "#     eigenvectors = tf.gather(eigenvectors, sorted_indices, axis=1)\n",
    "    \n",
    "#     #### Opcja druga na SVD, ale wymaga jeszcze zabawy\n",
    "#     # s, u, v = tf.linalg.svd(covariance_matrix)\n",
    "#     # eigenvectors = v\n",
    "#     # eigenvalues = tf.square(s) \n",
    "    \n",
    "#     # Warstwa PCA\n",
    "#     model = keras.models.Sequential(conv_layers + [\n",
    "#         tf.keras.layers.Lambda(lambda x, eigenvectors: tf.matmul(x, eigenvectors), arguments={'eigenvectors': eigenvectors[:, :n_components]})\n",
    "#     ])\n",
    "\n",
    "#     # Output (klasyfikacja)\n",
    "#     model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "#     #model.summary()\n",
    "#     tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    \n",
    "\n",
    "#     # Optymalizator\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "#     # Początkową wartość learning rate\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "#     ## Kompilacja modelu\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=8, start_from_epoch=8, min_delta=0.0001)\n",
    "    \n",
    "#     # ReduceLROnPlateau\n",
    "#     reduce_lr = ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.9, step=0.1),\n",
    "#         patience=trial.suggest_int(\"reduce_lr_patience\", 2, 12),\n",
    "#         min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "#     )\n",
    "    \n",
    "#     # Trenowanie i ocena modelu\n",
    "#     history = model.fit(X_train_image, y_train_image,\n",
    "#                         epochs=trial.suggest_int('epochs', 15, 80),\n",
    "#                         batch_size=trial.suggest_int('batch_size', 64, 1920),\n",
    "#                         validation_data=(X_val_image, y_val_image),\n",
    "#                         callbacks=[early_stopping, reduce_lr]\n",
    "#                         )\n",
    "    \n",
    "#     accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) \n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "# ### dwa tygodnie więc lepiej aby te wartości stanowiły dobrą podstawę...\n",
    "# #Trial 76 finished with value:  0.9906666874885559 and parameters: {'num_filters1': 194, 'num_filters2': 167, 'num_filters3': 221, 'PCA_components': 383, 'l1_1': 1.4917571854023011e-06, 'l2_1': 0.0003844462705458024,  'l1_2': 0.0011921068803372,     'l2_2': 0.00021948428596782946, 'l1_3': 0.05677951548917238,    'l2_3': 7.714986268695782e-05,  'activation': 'relu', 'optimizer': 'AdamW', 'lr_initial': 0.006869840110235172, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.028747844465778e-06,  'epochs': 51, 'batch_size': 479}.\n",
    "# #Trial 99 finished with value:  0.9911666512489319 and parameters: {'num_filters1': 222, 'num_filters2': 220, 'num_filters3': 202, 'PCA_components': 367, 'l1_1': 4.875844948451946e-06,  'l2_1': 0.00014963179483695857, 'l1_2': 2.3622464527905993e-06, 'l2_2': 0.0007506481920246097,  'l1_3': 0.030678868126237845,   'l2_3': 0.00016514043221099142, 'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.00653453290497133,  'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.8941869280860333e-06, 'epochs': 54, 'batch_size': 331}.\n",
    "# #Trial 108 finished with value: 0.9915000200271606 and parameters: {'num_filters1': 205, 'num_filters2': 206, 'num_filters3': 220, 'PCA_components': 370, 'l1_1': 9.95600828812184e-06,   'l2_1': 4.680137051239973e-05,  'l1_2': 4.9076000996726795e-06, 'l2_2': 8.638060092445147e-05,  'l1_3': 0.0361709248993415,     'l2_3': 0.0007397174884371887,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006269166650314777, 'reduce_lr_factor': 0.2, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 5.203896150606987e-06,  'epochs': 62, 'batch_size': 239}.\n",
    "# #Trial 121 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 189, 'num_filters2': 200, 'num_filters3': 220, 'PCA_components': 387, 'l1_1': 8.32443244291399e-05,   'l2_1': 5.963850113018811e-05,  'l1_2': 4.8017415472934674e-06, 'l2_2': 3.9423405128005956e-05, 'l1_3': 0.00047971791859799064, 'l2_3': 0.0014900662405900767,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006533479568732247, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 6, 'reduce_lr_min_lr': 7.315164005922789e-06,  'epochs': 58, 'batch_size': 249}.\n",
    "# #Trial 134 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 180, 'num_filters2': 196, 'num_filters3': 234, 'PCA_components': 390, 'l1_1': 4.148854564296224e-05,  'l2_1': 2.3209137723337733e-05, 'l1_2': 0.0009209306936723504,  'l2_2': 1.9816087702628798e-05, 'l1_3': 0.0008993453039409338,  'l2_3': 0.002218879345590759,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007404316715937198, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 3.862546224445297e-07,  'epochs': 61, 'batch_size': 368}.\n",
    "# #Trial 151 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 201, 'num_filters2': 189, 'num_filters3': 229, 'PCA_components': 403, 'l1_1': 6.15892270118e-05,      'l2_1': 1.6056284141459273e-05, 'l1_2': 0.0009167391639733724,  'l2_2': 1.4409315946656477e-05, 'l1_3': 0.0008899963895277467,  'l2_3': 0.005080838638011425,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007378956521980373, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 1.9759552872681682e-07, 'epochs': 63, 'batch_size': 416}.\n",
    "# #Trial 153 finished with value: 0.9905833601951599 and parameters: {'num_filters1': 158, 'num_filters2': 190, 'num_filters3': 229, 'PCA_components': 405, 'l1_1': 3.317865892202326e-05,  'l2_1': 1.715760099034611e-05,  'l1_2': 0.00028475050803045533, 'l2_2': 7.144046413508896e-06,  'l1_3': 0.0010057548651594868,  'l2_3': 0.001544820248650564,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.007667044780464368, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 1.6284942102179344e-07, 'epochs': 64, 'batch_size': 420}.\n",
    "# #Trial 189 finished with value: 0.9904166460037231 and parameters: {'num_filters1': 187, 'num_filters2': 194, 'num_filters3': 191, 'PCA_components': 381, 'l1_1': 0.0001222508587314991,  'l2_1': 5.2900939483419044e-05, 'l1_2': 4.050810455502403e-05,  'l2_2': 0.00017281194315162931, 'l1_3': 0.0014341620209873955,  'l2_3': 0.0025511470455798512,  'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006996548564114207, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 6, 'reduce_lr_min_lr': 9.663271049317225e-07,  'epochs': 60, 'batch_size': 219}.\n",
    "# #Trial 205 finished with value: 0.9904999732971191 and parameters: {'num_filters1': 210, 'num_filters2': 189, 'num_filters3': 198, 'PCA_components': 483, 'l1_1': 8.253178154361272e-05,  'l2_1': 1.7866032331171518e-05, 'l1_2': 0.0008019764970918962,  'l2_2': 1.0316537375038554e-05, 'l1_3': 0.000838618983241391,   'l2_3': 0.008249547665798275,   'activation': 'mish', 'optimizer': 'AdamW', 'lr_initial': 0.006570979651986537, 'reduce_lr_factor': 0.1, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 2.6114120319881016e-07, 'epochs': 63, 'batch_size': 560}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2 dynamic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# import tensorflow_probability as tfp\n",
    "# import optuna\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau \n",
    "\n",
    "# X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "# X_train_image = X_train_image.astype('float32')\n",
    "# X_val_image = X_val_image.astype('float32')\n",
    "# X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "# X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    " \n",
    "# def objective(trial):\n",
    "#     # Przestrzeń hiperparametrów\n",
    "#     num_filters1 = trial.suggest_int(\"num_filters1\", 180, 230)    # Pierwsza warstwa Conv2d\n",
    "#     num_filters2 = trial.suggest_int(\"num_filters2\", 180, 230)    # Druga warstwa Conv2d\n",
    "#     num_filters3 = trial.suggest_int(\"num_filters3\", 180, 260)    # Trzecia warstwa Conv2d\n",
    "#     n_components = trial.suggest_int(\"PCA_components\", 350, 420)  # PCA\n",
    "    \n",
    "#     l1_1=trial.suggest_float('l1_1', 1e-6, 5e-3, log=True)\n",
    "#     l2_1=trial.suggest_float('l2_1', 5e-5, 5e-3, log=True)\n",
    "    \n",
    "#     l1_2=trial.suggest_float('l1_2', 1e-6, 1e-3, log=True)\n",
    "#     l2_2=trial.suggest_float('l2_2', 1e-6, 1e-5, log=True)\n",
    "\n",
    "#     l1_3=trial.suggest_float('l1_3', 1e-6, 1e-1, log=True)\n",
    "#     l2_3=trial.suggest_float('l2_3', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "#     activation_functions = ['gelu', 'leaky_relu', 'silu', 'mish']\n",
    "#     activation = trial.suggest_categorical('activation', activation_functions)\n",
    "    \n",
    "    \n",
    "#     class DynamicPCA(keras.layers.Layer):\n",
    "#         def __init__(self, n_components, layer, update_freq=2, threshold=0.5, **kwargs):  \n",
    "#             super(DynamicPCA, self).__init__(**kwargs)\n",
    "#             self.n_components = n_components\n",
    "#             self.threshold = threshold\n",
    "#             self.layer = layer  \n",
    "#             self.update_freq = update_freq\n",
    "#             self.val_loss = None\n",
    "#             self.eigenvectors = None\n",
    "#             self.epoch_count = 0\n",
    "\n",
    "#         def build(self, input_shape):\n",
    "#             self.eigenvectors = self.add_weight(\n",
    "#                 shape=(input_shape[-1], self.n_components),\n",
    "#                 initializer=\"zeros\",\n",
    "#                 trainable=True,\n",
    "#             )\n",
    "\n",
    "#         def call(self, inputs):\n",
    "#             self.epoch_count += 1\n",
    "#             if (self.val_loss is not None and \n",
    "#                 self.val_loss < self.threshold and \n",
    "#                 self.epoch_count % self.update_freq == 0):\n",
    "#                 print(\"DynamicPCA: Aktualizacja wektorów własnych!\")\n",
    "#                 print(f\"val_loss: {self.val_loss}, threshold: {self.threshold}, epoch_count: {self.epoch_count}, update_freq: {self.update_freq}\")\n",
    "                \n",
    "#                 # Pobierz wagi bezpośrednio z warstwy\n",
    "#                 weights = self.layer.get_weights()  \n",
    "#                 flat_weights = tf.concat([tf.reshape(w, [-1]) for w in weights], axis=0)\n",
    "#                 # Normalizacja wag\n",
    "#                 flat_weights = tf.math.l2_normalize(flat_weights)\n",
    "#                 # Obliczanie nowych wektorów własnych\n",
    "#                 new_eigenvectors = self.calculate_pca(flat_weights)\n",
    "#                 # Aktualizacja wag (wektorów własnych)\n",
    "#                 self.eigenvectors.assign(new_eigenvectors)\n",
    "\n",
    "#             # Mnożenie przez wektory własne\n",
    "#             output = tf.matmul(inputs, self.eigenvectors)\n",
    "#             return output\n",
    "\n",
    "#         def calculate_pca(self, inputs):\n",
    "#             print(flat_weights.shape)\n",
    "#             # Obliczanie macierzy kowariancji\n",
    "#             covariance_matrix = tf.linalg.cov(inputs)  \n",
    "#             # Wektory i wartości własne\n",
    "#             eigenvalues, eigenvectors = tf.linalg.eigh(covariance_matrix)\n",
    "#             sorted_indices = tf.argsort(eigenvalues, direction='DESCENDING')\n",
    "#             eigenvectors = tf.gather(eigenvectors, sorted_indices, axis=1)\n",
    "#             return eigenvectors[:, :self.n_components]\n",
    "\n",
    "#     class MyCallback(keras.callbacks.Callback):\n",
    "#         def on_epoch_end(self, epoch, logs=None):\n",
    "#             pca_layer = self.model.get_layer('dynamic_pca')\n",
    "#             pca_layer.val_loss = logs.get('val_loss')\n",
    "#             print(logs.get('val_loss'))\n",
    "\n",
    "#     # Definiowanie wejścia\n",
    "#     image_input = keras.Input(shape=X_train_image.shape[1:])  \n",
    "\n",
    "#     # Warstwy konwolucyjne\n",
    "#     conv1 = keras.layers.Conv2D(num_filters1, kernel_size=(2, 2), activation=activation)(image_input)\n",
    "#     pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "#     conv2 = keras.layers.Conv2D(num_filters2, kernel_size=(2, 2), activation=activation)(pool1)\n",
    "#     pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "#     conv3 = keras.layers.Conv2D(num_filters3, kernel_size=(2, 2), activation=activation)(pool2)\n",
    "#     pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "#     flat_cnn = keras.layers.Flatten()(pool3)\n",
    "\n",
    "#     # Dense 1\n",
    "#     dense = keras.layers.Dense(units=trial.suggest_int('dense_units_1', 60, 240), activation=activation)(flat_cnn) \n",
    "#     dense = keras.layers.Dropout(rate=trial.suggest_float('dropout_rate_1', 0.01, 0.75))(dense)\n",
    "\n",
    "#     # Warstwa DynamicPCA \n",
    "#     pca_output = DynamicPCA(n_components=n_components, layer=conv3, name='dynamic_pca', threshold=1.25)(flat_cnn)  \n",
    "    \n",
    "#     merged = keras.layers.Concatenate()([dense, pca_output])\n",
    "\n",
    "#     dense2 = keras.layers.Dense(units=trial.suggest_int('dense_units_2', 60, 240), activation=activation)(merged)  \n",
    "#     dense2 = keras.layers.Dropout(rate=trial.suggest_float('dropout_rate_2', 0.01, 0.75))(dense2)\n",
    "\n",
    "#     # Output (klasyfikacja)\n",
    "#     outputs = keras.layers.Dense(10, activation='softmax')(dense2)\n",
    "\n",
    "#     # Utworzenie modelu\n",
    "#     model = keras.Model(inputs=image_input, outputs=outputs)\n",
    "\n",
    "#     # Utworzenie modelu\n",
    "#     model = keras.Model(inputs=image_input, outputs=outputs)\n",
    "#     #model.summary()\n",
    "#     tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    \n",
    "\n",
    "#     # Optymalizator\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['AdamW'])\n",
    "#     optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "#     # Początkową wartość learning rate\n",
    "#     lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "#     optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "#     ## Kompilacja modelu\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "#     # Early Stopping\n",
    "#     early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=10, start_from_epoch=8, min_delta=0.0001)\n",
    "    \n",
    "#     # ReduceLROnPlateau\n",
    "#     reduce_lr = ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.5, step=0.1),\n",
    "#         patience=trial.suggest_int(\"reduce_lr_patience\", 4, 8),\n",
    "#         min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-5, log=True)\n",
    "#     )\n",
    "    \n",
    "#     # Trenowanie i ocena modelu\n",
    "#     history = model.fit(X_train_image, y_train_image,\n",
    "#                         epochs=trial.suggest_int('epochs', 20, 80),\n",
    "#                         batch_size=trial.suggest_int('batch_size', 164, 768),\n",
    "#                         validation_data=(X_val_image, y_val_image),\n",
    "#                         validation_freq=1,\n",
    "#                         callbacks=[early_stopping, reduce_lr, MyCallback()]\n",
    "#                         )\n",
    "    \n",
    "#     accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "# study.optimize(objective, n_trials=600, n_jobs=1, gc_after_trial=True) \n",
    "\n",
    "# # Najlepsze parametry i wynik\n",
    "# print(\"Najlepsze parametry:\", study.best_params)\n",
    "# print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model bazujący na dwóch potokach danych. Surowych przechodzących przez Conv2d, oraz LSTM dla danych z HOG i Gabor. Wykorzystanie mechanizmu atencji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "\n",
    "from tensorflow.keras.layers import ( Input, LSTM, GlobalAveragePooling1D, Permute, PReLU, Flatten, Reshape, Conv2D, MaxPooling2D, Concatenate, Attention, Dense,\n",
    "                                     LayerNormalization, ActivityRegularization, Dropout, GlobalAveragePooling2D, AveragePooling2D, AveragePooling1D, GlobalAveragePooling1D, \n",
    "                                     GlobalMaxPool1D, GlobalMaxPool2D, MaxPool2D, SeparableConv1D, RandomRotation, BatchNormalization\n",
    "                                    )\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import gc\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "X_train_reduced = np.load('X_train_reduced.npy')\n",
    "X_val_reduced = np.load('X_val_reduced.npy')\n",
    "y_train_filters = np.load('y_train_filters.npy')\n",
    "y_val_filters = np.load('y_val_filters.npy')\n",
    "\n",
    "X_train_reduced = X_train_reduced.reshape(X_train_reduced.shape[0], X_train_reduced.shape[1], 1)\n",
    "X_val_reduced = X_val_reduced.reshape(X_val_reduced.shape[0], X_val_reduced.shape[1], 1)\n",
    "\n",
    "X_train_image, X_val_image, y_train_image, y_val_image = train_test_split(X_train, y_train_ENC, test_size=0.2, random_state=69)\n",
    "X_train_image = X_train_image.astype('float32')\n",
    "X_val_image = X_val_image.astype('float32')\n",
    "X_train_image = np.expand_dims(X_train_image, axis=-1)  \n",
    "X_val_image = np.expand_dims(X_val_image, axis=-1) \n",
    "\n",
    "class SpectralNorm(Constraint):\n",
    "    \"\"\"\n",
    "    Spectral Normalization constraint.\n",
    "\n",
    "    Args:\n",
    "        iteration: Number of iterations for power iteration.\n",
    "    \"\"\"\n",
    "    def __init__(self, iteration=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.iteration = iteration\n",
    "        self.u = None\n",
    "\n",
    "    def __call__(self, w):\n",
    "        w_shape = w.shape.as_list()\n",
    "        w = tf.reshape(w, [-1, w_shape[-1]])\n",
    "\n",
    "        if self.u is None:\n",
    "            self.u = tf.Variable(\n",
    "                initial_value=tf.keras.initializers.RandomNormal()(shape=[1, w_shape[-1]]),\n",
    "                trainable=False\n",
    "            )\n",
    "\n",
    "        if self.iteration == 1:\n",
    "            v_hat = tf.einsum('ij,kj->ik', self.u, w) \n",
    "            v_hat = v_hat / tf.linalg.norm(v_hat, ord=2, axis=-1, keepdims=True)\n",
    "            u_hat = tf.einsum('ij,jk->ik', v_hat, w)\n",
    "            u_hat = u_hat / tf.linalg.norm(u_hat, ord=2, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            def body(i, u_hat):\n",
    "                v_hat = tf.einsum('ij,kj->ik', u_hat, w)\n",
    "                v_hat = v_hat / tf.linalg.norm(v_hat, ord=2, axis=-1, keepdims=True)\n",
    "                u_hat = tf.einsum('ij,jk->ik', v_hat, w)\n",
    "                u_hat = u_hat / tf.linalg.norm(u_hat, ord=2, axis=-1, keepdims=True)\n",
    "                return i + 1, u_hat\n",
    "\n",
    "            _, u_hat = tf.while_loop(\n",
    "                lambda i, _: i < self.iteration,\n",
    "                body,\n",
    "                loop_vars=[0, self.u]\n",
    "            )\n",
    "\n",
    "        self.u.assign(u_hat)  # Aktualizacja self.u po pętli\n",
    "\n",
    "        w_norm = w / tf.linalg.norm(w, ord=2, axis=0, keepdims=True)\n",
    "        w_norm = tf.reshape(w_norm, w_shape)\n",
    "\n",
    "        return w_norm\n",
    "    \n",
    "def create_sepconv1d_layer(units, activation, l1, l2):\n",
    "    return SeparableConv1D(\n",
    "        units,\n",
    "        kernel_size=1,\n",
    "        padding='same',\n",
    "        activation=activation,\n",
    "        depthwise_constraint=SpectralNorm,\n",
    "        bias_regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    )\n",
    "\n",
    "def create_conv2d_layer(filters, activation, l1, l2):\n",
    "    return Conv2D(\n",
    "        filters,\n",
    "        kernel_size=(2, 2),\n",
    "        activation=activation,\n",
    "        kernel_constraint=SpectralNorm,\n",
    "        bias_regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    )    \n",
    "    \n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    # Dane wejściowe\n",
    "    image_input = Input(shape=X_train_image.shape[1:])\n",
    "    flattened_input = Input(shape=X_train_reduced.shape[1:])\n",
    "\n",
    "    ### Hiperparamatry \n",
    "    activation_functions = ['gelu', 'leaky_relu', 'silu', 'mish']\n",
    "    activation = trial.suggest_categorical('activation', activation_functions)\n",
    "        \n",
    "    filters1_1D=trial.suggest_int('Sep-Conv2d filters1', 32, 384)  # Pierwsza warstwa SeparableConv1D\n",
    "    filters2_1D=trial.suggest_int('Sep-Conv2d filters2', 32, 384)  # Druga warstwa SeparableConv1D\n",
    "    filters3_1D=trial.suggest_int('Sep-Conv2d filters3', 32, 384)  # Trzecia warstwa SeparableConv1D\n",
    "    \n",
    "    l1_1_1D=trial.suggest_float('l1_1_1D', 1e-6, 1e-1, log=True)\n",
    "    l2_1_1D=trial.suggest_float('l2_1_1D', 1e-6, 1e-1, log=True)\n",
    "    l1_2_1D=trial.suggest_float('l1_2_1D', 1e-6, 1e-1, log=True)\n",
    "    l2_2_1D=trial.suggest_float('l2_2_1D', 1e-6, 1e-1, log=True)\n",
    "    l1_3_1D=trial.suggest_float('l1_3_1D', 1e-6, 1e-1, log=True)\n",
    "    l2_3_1D=trial.suggest_float('l2_3_1D', 1e-6, 1e-1, log=True)\n",
    "       \n",
    "    filters1_2D = trial.suggest_int(\"Conv2d filters1\", 32, 384)    # Pierwsza warstwa Conv2d\n",
    "    filters2_2D = trial.suggest_int(\"Conv2d filters2\", 32, 384)    # Druga warstwa Conv2d\n",
    "    filters3_2D = trial.suggest_int(\"Conv2d filters3\", 32, 384)    # Trzecia warstwa Conv2d\n",
    "\n",
    "    l1_1_2D=trial.suggest_float('l1_1_2D', 1e-6, 1e-1, log=True)\n",
    "    l2_1_2D=trial.suggest_float('l2_1_2D', 1e-6, 1e-1, log=True)\n",
    "    l1_2_2D=trial.suggest_float('l1_2_2D', 1e-6, 1e-1, log=True)\n",
    "    l2_2_2D=trial.suggest_float('l2_2_2D', 1e-6, 1e-1, log=True)\n",
    "    l1_3_2D=trial.suggest_float('l1_3_2D', 1e-6, 1e-1, log=True)\n",
    "    l2_3_2D=trial.suggest_float('l2_3_2D', 1e-6, 1e-1, log=True)\n",
    "\n",
    "    # Gałąź SeparableConv1D\n",
    "    Sep_Conv1D_1 = create_sepconv1d_layer(filters1_1D, activation, l1_1_1D, l2_1_1D)(flattened_input)\n",
    "    Sep_Conv1D_2 = create_sepconv1d_layer(filters2_1D, activation, l1_2_1D, l2_2_1D)(Sep_Conv1D_1)\n",
    "    Sep_Conv1D_3 = create_sepconv1d_layer(filters3_1D, activation, l1_3_1D, l2_3_1D)(Sep_Conv1D_2)\n",
    "    # Pooling \n",
    "    Sep_pool_Conv1D = GlobalAveragePooling1D()(Sep_Conv1D_3)    \n",
    "\n",
    "    # Gałąź Conv2D\n",
    "        ## Pierwsza warstwa\n",
    "    conv1 = create_conv2d_layer(filters1_2D, activation, l1_1_2D, l2_1_2D)(image_input)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        ## Druga warstwazz\n",
    "    conv2 = create_conv2d_layer(filters2_2D, activation, l1_2_2D, l2_2_2D)(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        ## Trzecia warstwa\n",
    "    conv3 = create_conv2d_layer(filters3_2D, activation, l1_3_2D, l2_3_2D)(pool2)\n",
    "    pool3 = MaxPool2D(pool_size=(3, 3))(conv3)  \n",
    "    \n",
    "    flat_cnn = Flatten()(pool3)\n",
    "\n",
    "    #Dense gałąź SeparableConv1D\n",
    "    dense1 = Dense(units=trial.suggest_int('dense SeparableConv1D', 32, 384), activation=activation)(Sep_pool_Conv1D)\n",
    "    dense1 = Dropout(rate=trial.suggest_float('dropout_rate_1', 0.0, 0.75))(dense1)  \n",
    "\n",
    "    #Dense gałąź Conv2D    \n",
    "    dense2 = Dense(units=trial.suggest_int('dense Conv2D', 32, 384), activation=activation)(flat_cnn)\n",
    "    dense2 = Dropout(rate=trial.suggest_float('dropout_rate_2', 0.0, 0.75))(dense2)\n",
    "    \n",
    "    # Połączenie gałęzi\n",
    "    merged = Concatenate()([dense1, dense2])\n",
    "\n",
    "    merged = Reshape((merged.shape[1], 1))(merged)\n",
    "\n",
    "    # Attention Layer\n",
    "    attention_output = Attention()([merged, merged])\n",
    "    attention_output = Reshape((attention_output.shape[1],))(attention_output)\n",
    "    \n",
    "    #Dense\n",
    "    dense3 = Dense(units=trial.suggest_int('dense przed Output', 36, 384), activation=activation)(attention_output)\n",
    "    dense3 = Dropout(rate=trial.suggest_float('dropout przed Output', 0.0, 0.75))(dense3)\n",
    "\n",
    "    #Output\n",
    "    output = Dense(10, activation='softmax')(dense3)\n",
    "    model = Model(inputs=[image_input, flattened_input], outputs=output)\n",
    "    \n",
    "    #tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')\n",
    "    #model.summary()\n",
    "    \n",
    "    # ReduceLROnPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.6, step=0.1),\n",
    "        patience=trial.suggest_int(\"reduce_lr_patience\", 2, 14),\n",
    "        min_lr=trial.suggest_float(\"reduce_lr_min_lr\", 1e-7, 1e-4, log=True)\n",
    "    )\n",
    "\n",
    "    # Optymalizator\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['AdamW', 'Lion'])\n",
    "    optimizer_class = getattr(tf.keras.optimizers, optimizer_name)\n",
    "    \n",
    "    # Początkową wartość learning rate\n",
    "    lr_initial = trial.suggest_float(\"lr_initial\", 5e-3, 1e-1, log=True)\n",
    "    optimizer = optimizer_class(learning_rate=lr_initial)  \n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6, start_from_epoch=12, min_delta=0.0001)\n",
    "    #log_dir = \"logs/\"  # Katalog, w którym będą zapisywane dane TensorBoard\n",
    "    #tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1 )\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    history = model.fit(\n",
    "        [X_train_image, X_train_reduced], \n",
    "        y_train_filters,\n",
    "        epochs=trial.suggest_int('epochs', 20, 90),\n",
    "        batch_size=trial.suggest_int('batch_size', 164, 768),\n",
    "        validation_data=([X_val_image, X_val_reduced], y_val_filters),\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, reduce_lr],# , tensorboard\n",
    "    )\n",
    "\n",
    "    best_val_accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "    # Ewaluacja modelu na danych testowych\n",
    "    #loss, accuracy = model.evaluate([X_test, X_test], y_test_ENC, verbose=1)\n",
    "    #print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    del model\n",
    "    return best_val_accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=200, n_jobs=1, gc_after_trial=True) ## Czy keras działa na wszystkich wątkach, ale optuna na pojedynczym. \n",
    "\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "print(\"Najlepsze parametry:\", study.best_params)\n",
    "print(\"Najlepsza dokładność:\", study.best_value)\n",
    "\n",
    "#[I 2024-10-25 02:17:14,226] Trial 55 finished with value:  0.9590833187103271 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 23,  'Sep-Conv2d filters2': 58,  'Sep-Conv2d filters3': 85, 'l1_1_1D': 0.0002263330199021373,   'l2_1_1D': 4.2547366301065874e-06,  'l1_2_1D': 1.8831463765106328e-05,  'l2_2_1D': 2.6849906489521574e-06, 'l1_3_1D': 5.376750360962702e-06, 'l2_3_1D': 4.5507289814394546e-05, 'Conv2d filters1': 27,  'Conv2d filters2': 75, 'Conv2d filters3': 66,  'l1_1_2D': 0.03570924628671723,    'l2_1_2D': 0.00013702268815391088,'l1_2_2D': 0.004180188847206343,   'l2_2_2D': 0.015697748113667306,   'l1_3_2D': 1.1093102758419453e-06, 'l2_3_2D': 2.3039556898400452e-05, 'dense SeparableConv1D': 113, 'dropout_rate_1': 0.25988069757258775, 'dense Conv2D': 55,  'dropout_rate_2': 0.1729409318039701,   'dense przed Output': 213, 'dropout przed Output': 0.692278722477647,    'reduce_lr_factor': 0.1, 'reduce_lr_patience': 8, 'reduce_lr_min_lr': 1.2382721420683838e-05, 'optimizer': 'AdamW', 'lr_initial': 0.007665780845145814, 'epochs': 81, 'batch_size': 173}. Best is trial 55 with value: 0.9590833187103271.\n",
    "#[I 2024-10-25 05:23:25,094] Trial 82 finished with value:  0.9622499942779541 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 22,  'Sep-Conv2d filters2': 28,  'Sep-Conv2d filters3': 100, 'l1_1_1D': 6.186293277794836e-05,  'l2_1_1D': 0.0003913398305461908,   'l1_2_1D': 1.5927923474696962e-06,  'l2_2_1D': 7.009282960520817e-06,  'l1_3_1D': 7.676137877709806e-06, 'l2_3_1D': 0.00022663346915615475, 'Conv2d filters1': 24,  'Conv2d filters2': 74, 'Conv2d filters3': 92,  'l1_1_2D': 0.0993169614612255,     'l2_1_2D': 0.0006883072766765725, 'l1_2_2D': 0.0005616491418975441,  'l2_2_2D': 1.0689411586574192e-05, 'l1_3_2D': 4.1278686535916746e-05, 'l2_3_2D': 4.837146281888261e-06,  'dense SeparableConv1D': 155, 'dropout_rate_1': 0.0509776183623675,  'dense Conv2D': 29,  'dropout_rate_2': 0.024717745882008837, 'dense przed Output': 178, 'dropout przed Output': 0.7445169215013963, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 4, 'reduce_lr_min_lr': 6.725637729646196e-06,  'optimizer': 'AdamW', 'lr_initial': 0.01728017154953551,  'epochs': 85, 'batch_size': 191}. Best is trial 82 with value: 0.9622499942779541.\n",
    "#[I 2024-10-25 15:22:24,767] Trial 142 finished with value: 0.9635000228881836 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 22,  'Sep-Conv2d filters2': 28,  'Sep-Conv2d filters3': 107, 'l1_1_1D': 6.120269265952697e-05,  'l2_1_1D': 7.956576920829512e-06,   'l1_2_1D': 1.2494724596667829e-06,  'l2_2_1D': 7.6581362167544e-06,    'l1_3_1D': 0.0039961697402018225, 'l2_3_1D': 0.00021250335195961356, 'Conv2d filters1': 28,  'Conv2d filters2': 72, 'Conv2d filters3': 99 , 'l1_1_2D': 0.07544375319709569,    'l2_1_2D': 0.000936229620640654,  'l1_2_2D': 0.001250431002984649,   'l2_2_2D': 0.01324640444949233,    'l1_3_2D': 0.0012955495310992596,  'l2_3_2D': 6.618279886291623e-06,  'dense SeparableConv1D': 151, 'dropout_rate_1': 0.08650725184529029, 'dense Conv2D': 29,  'dropout_rate_2': 0.0500476697674726,   'dense przed Output': 205, 'dropout przed Output': 0.7360088478595872,   'reduce_lr_factor': 0.4, 'reduce_lr_patience': 7, 'reduce_lr_min_lr': 9.207463977521836e-07,  'optimizer': 'AdamW', 'lr_initial': 0.01209682198565378,  'epochs': 88, 'batch_size': 228}. Best is trial 142 with value: 0.9635000228881836.\n",
    "#[I 2024-11-01 00:07:23,390] Trial 24 finished with value:  0.9449999928474426 and parameters: {'activation': 'mish',        'Sep-Conv2d filters1': 123, 'Sep-Conv2d filters2': 245, 'Sep-Conv2d filters3': 289, 'l1_1_1D': 0.08989218874411514,    'l2_1_1D': 1.1472596089503972e-06,  'l1_2_1D': 0.0026057385621594867,   'l2_2_1D': 0.00018978883951458408, 'l1_3_1D': 1.191033349842715e-06, 'l2_3_1D': 0.00014292949482563498, 'Conv2d filters1': 308, 'Conv2d filters2': 81, 'Conv2d filters3': 326, 'l1_1_2D': 2.9840369417992957e-06, 'l2_1_2D': 0.0014037071972283193, 'l1_2_2D': 0.0002640492511862146,  'l2_2_2D': 0.08499221386872775,    'l1_3_2D': 0.020374031922578733,   'l2_3_2D': 0.00010028036351417339, 'dense SeparableConv1D': 379, 'dropout_rate_1': 0.0544686509758308,  'dense Conv2D': 138, 'dropout_rate_2': 0.09593865740145555,  'dense przed Output': 384, 'dropout przed Output': 0.6382663359384837, 'reduce_lr_factor': 0.3, 'reduce_lr_patience': 4, 'reduce_lr_min_lr': 7.741233462096205e-06,  'optimizer': 'AdamW', 'lr_initial': 0.006707953904631179, 'epochs': 65, 'batch_size': 257}. Best is trial 24 with value: 0.9449999928474426.\n",
    "#[I 2024-11-05 15:57:39,996] Trial 101 finished with value: 0.9495833516120911 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 148, 'Sep-Conv2d filters2': 32,  'Sep-Conv2d filters3': 63, 'l1_1_1D': 0.0004897725865476358,   'l2_1_1D': 0.0003049226492905848,   'l1_2_1D': 5.00894966144464e-06,    'l2_2_1D': 0.00014051697363473006, 'l1_3_1D': 0.051135669099956614,  'l2_3_1D': 0.0009950758155242705,  'Conv2d filters1': 52,  'Conv2d filters2': 58, 'Conv2d filters3': 249, 'l1_1_2D': 0.014787666006730659,   'l2_1_2D': 0.0005948328558539133, 'l1_2_2D': 5.343054689024963e-06,  'l2_2_2D': 4.5277461875969285e-06, 'l1_3_2D': 1.9548002422161054e-06, 'l2_3_2D': 0.0009410443070360079,  'dense SeparableConv1D': 319, 'dropout_rate_1': 0.2533943711642646,  'dense Conv2D': 311, 'dropout_rate_2': 0.10208101037728201,  'dense przed Output': 90,  'dropout przed Output': 0.481006546242034,   'reduce_lr_factor': 0.5, 'reduce_lr_patience': 2, 'reduce_lr_min_lr': 5.559247664816983e-05,  'optimizer': 'AdamW', 'lr_initial': 0.00964928371074687,  'epochs': 66, 'batch_size': 423}. Best is trial 101 with value: 0.9495833516120911.\n",
    "#[I 2024-11-06 02:29:49,053] Trial 121 finished with value: 0.9535833597183228 and parameters: {'activation': 'leaky_relu',  'Sep-Conv2d filters1': 119, 'Sep-Conv2d filters2': 49,  'Sep-Conv2d filters3': 158, 'l1_1_1D': 0.00023280964689116444, 'l2_1_1D': 0.0008751788463496611,   'l1_2_1D': 0.04610519254480619,     'l2_2_1D': 2.4445376899197913e-06, 'l1_3_1D': 0.06955230495640688,   'l2_3_1D': 0.004678025581652909,   'Conv2d filters1': 37,  'Conv2d filters2': 54, 'Conv2d filters3': 253, 'l1_1_2D': 0.016409226434426858,  'l2_1_2D': 0.00015292748828121323, 'l1_2_2D': 1.177904875672071e-05,  'l2_2_2D': 2.6668468645182726e-06, 'l1_3_2D': 6.7648129463989756e-06, 'l2_3_2D': 0.0007509210891800366,  'dense SeparableConv1D': 347, 'dropout_rate_1': 0.2855797128973145,  'dense Conv2D': 368, 'dropout_rate_2': 0.07539865516979521,  'dense przed Output': 73,  'dropout przed Output': 0.5059616088424707, 'reduce_lr_factor': 0.4, 'reduce_lr_patience': 2, 'reduce_lr_min_lr': 3.887676551167433e-05, 'optimizer': 'AdamW', 'lr_initial': 0.00852437932688485, 'epochs': 73, 'batch_size': 407}. Best is trial 121 with value: 0.9535833597183228."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analiza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
