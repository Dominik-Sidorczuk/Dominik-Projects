{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a23608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# === SCRAPER ===\n",
    "def extract_mid(url_or_iframe: str) -> str:\n",
    "    mid_match = re.search(r\"mid=([a-zA-Z0-9\\-_]+)\", url_or_iframe)\n",
    "    if not mid_match:\n",
    "        raise ValueError(\"Nie udało się znaleźć mid w podanym URL/iframe.\")\n",
    "    return mid_match.group(1)\n",
    "\n",
    "def scrape_kml(mid: str) -> pd.DataFrame:\n",
    "    url = f\"https://www.google.com/maps/d/u/0/kml?forcekml=1&mid={mid}\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    root = ET.fromstring(r.content)\n",
    "    ns = {\"kml\": \"http://www.opengis.net/kml/2.2\"}\n",
    "\n",
    "    records = []\n",
    "    for pm in root.findall(\".//kml:Placemark\", ns):\n",
    "        name = pm.findtext(\"kml:name\", default=\"\", namespaces=ns)\n",
    "        desc = pm.findtext(\"kml:description\", default=\"\", namespaces=ns)\n",
    "        coords = pm.find(\".//kml:coordinates\", ns)\n",
    "\n",
    "        lon, lat, *_ = coords.text.strip().split(\",\") if coords is not None else (None, None)\n",
    "\n",
    "        record = {\n",
    "            \"Title\": name,\n",
    "            \"Description\": desc,\n",
    "            \"Longitude\": lon,\n",
    "            \"Latitude\": lat\n",
    "        }\n",
    "\n",
    "        # ExtendedData\n",
    "        ext = pm.find(\".//kml:ExtendedData\", ns)\n",
    "        if ext is not None:\n",
    "            for data in ext.findall(\"kml:Data\", ns):\n",
    "                key = data.attrib.get(\"name\")\n",
    "                val = data.findtext(\"kml:value\", default=\"\", namespaces=ns)\n",
    "                record[key] = val\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# === CLEANER ===\n",
    "def clean_csv(input_filename: str, output_filename: str):\n",
    "    try:\n",
    "        with open(input_filename, 'r', encoding='utf-8') as csvfile:\n",
    "            dialect = csv.Sniffer().sniff(csvfile.read(1024))\n",
    "            csvfile.seek(0)\n",
    "            separator = dialect.delimiter\n",
    "            print(f\"Wykryto separator kolumn: '{separator}'\")\n",
    "\n",
    "        df = pd.read_csv(input_filename, sep=separator)\n",
    "        print(\"\\nPoprawnie wczytano plik. Nagłówki kolumn:\")\n",
    "        print(list(df.columns))\n",
    "\n",
    "        # Kolumny do dropa\n",
    "        columns_to_drop = ['Description', 'Longitude', 'Latitude']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "        if existing_columns_to_drop:\n",
    "            df = df.drop(columns=existing_columns_to_drop)\n",
    "            print(f\"\\nUsunięto kolumny: {', '.join(existing_columns_to_drop)}\")\n",
    "        else:\n",
    "            print(\"\\nBrak kolumn do usunięcia.\")\n",
    "\n",
    "        # Drop pustych kolumn\n",
    "        initial_column_count = len(df.columns)\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        final_column_count = len(df.columns)\n",
    "        if initial_column_count > final_column_count:\n",
    "            print(\"Usunięto puste kolumny.\")\n",
    "        else:\n",
    "            print(\"Nie było całkowicie pustych kolumn.\")\n",
    "\n",
    "        df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n✅ Oczyszczone dane zapisane w '{output_filename}' ({len(df)} rekordów).\")\n",
    "\n",
    "        print(\"\\nPodgląd 5 pierwszych wierszy:\")\n",
    "        print(df.head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Błąd: Plik '{input_filename}' nie został znaleziony.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Wystąpił nieoczekiwany błąd: {e}\")\n",
    "\n",
    "# === ENTRYPOINT COLAB ===\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT = \"https://www.google.com/maps/d/embed?mid=1GDQK9SeuuqI07MFnngUMASRKvb0ZG4U&ehbc=2E312F\"\n",
    "    MAP_ID = extract_mid(INPUT)\n",
    "    print(f\"Using MAP_ID: {MAP_ID}\")\n",
    "\n",
    "    # Scraping\n",
    "    df = scrape_kml(MAP_ID)\n",
    "    raw_file = \"edge_scraped.csv\"\n",
    "    df.to_csv(raw_file, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ Surowe dane zapisane do '{raw_file}' ({len(df)} rekordów).\")\n",
    "\n",
    "    # Cleaning\n",
    "    clean_csv(raw_file, \"oczyszczone_dane.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_run.py\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# --- Konfiguracja ---\n",
    "INPUT_CSV_PATH = \"oczyszczone_dane.csv\"\n",
    "OUTPUT_CSV_PATH = \"wyniki_analizy.csv\"  \n",
    "SCRIPT_TO_RUN = \"Agent_Run.py\"\n",
    "\n",
    "# --- Logika skryptu ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Główna funkcja uruchomieniowa.\"\"\"\n",
    "\n",
    "    print(\"--- PRZYGOTOWANIE DO URUCHOMIENIA ANALIZY ---\", flush=True)\n",
    "\n",
    "    # 1. Sprawdzenia plików\n",
    "    if not os.path.exists(INPUT_CSV_PATH):\n",
    "        print(f\"❌ BŁĄD: Plik wejściowy '{INPUT_CSV_PATH}' nie został znaleziony.\")\n",
    "        print(\"     Upewnij się, że plik istnieje i jest w tym samym katalogu.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not os.path.exists(SCRIPT_TO_RUN):\n",
    "        print(f\"❌ BŁĄD: Główny skrypt '{SCRIPT_TO_RUN}' nie został znaleziony.\")\n",
    "        print(\"     Sprawdź ścieżkę do skryptu run_analysis.py.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 2. Przygotowanie polecenia\n",
    "    command = [\n",
    "        sys.executable,\n",
    "        \"-u\",  # Wymusza tryb bez buforowania\n",
    "        SCRIPT_TO_RUN,\n",
    "        \"--input-file\",\n",
    "        INPUT_CSV_PATH,\n",
    "        \"--output-file\",\n",
    "        OUTPUT_CSV_PATH\n",
    "    ]\n",
    "\n",
    "    print(\"✅ Konfiguracja poprawna. Rozpoczynam analizę...\")\n",
    "    print(f\"▶️ Uruchamiam polecenie: {' '.join(command)}\\n\")\n",
    "\n",
    "    # 3. Uruchomienie procesu i przechwytywanie wyjścia na żywo\n",
    "    try:\n",
    "        with subprocess.Popen(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            encoding='utf-8',\n",
    "            bufsize=1\n",
    "        ) as process:\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                print(line, end='')\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            raise subprocess.CalledProcessError(process.returncode, process.args)\n",
    "\n",
    "        print(f\"\\n--- Analiza zakończona pomyślnie ---\")\n",
    "        if os.path.exists(OUTPUT_CSV_PATH):\n",
    "            print(f\"🎉 Wyniki zostały zapisane w pliku: {os.path.abspath(OUTPUT_CSV_PATH)}\")\n",
    "        else:\n",
    "            print(f\"🤔 OSTRZEŻENIE: Proces zakończył się, ale plik wyjściowy '{OUTPUT_CSV_PATH}' nie został utworzony.\")\n",
    "\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"\\n❌ BŁĄD: Wystąpił problem podczas wykonywania skryptu '{SCRIPT_TO_RUN}'.\")\n",
    "        print(\"     Sprawdź powyższe logi, aby znaleźć przyczynę błędu.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⏹️ Analiza przerwana przez użytkownika.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ BŁĄD: Wystąpił nieoczekiwany błąd: {e}\")\n",
    "    \n",
    "    print(\"\\n--- SKRYPT GŁÓWNY ZAKOŃCZONY ---\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e962fd",
   "metadata": {},
   "source": [
    "## llama.cpp ROCm -->TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9350d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-05 16:40:23,009][__main__][INFO] Model 'embedding' wczytany poprawnie.\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
      "[2025-09-05 16:40:23,260][__main__][INFO] Model 'finance_instruct' wczytany poprawnie.\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (196608) -- the full capacity of the model will not be utilized\n",
      "[2025-09-05 16:40:23,419][__main__][INFO] Model 'base' wczytany poprawnie.\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "[2025-09-05 16:40:23,460][__main__][INFO] [Pamięć] Dodano wpis od 'Użytkownik'. Całkowita liczba wpisów: 1.\n",
      "[2025-09-05 16:40:23,460][__main__][INFO] --- Rozpoczynam symulację rozmowy (v7 Higiena Pamięci) ---\n",
      "[2025-09-05 16:40:23,461][__main__][INFO] \u001b[1;33mUżytkownik: Nasz nowy system logistyczny oparty na AI zredukował koszty o 20%, ale wymaga drogiej infrastruktury chmurowej. Zastanawiam się nad jego rentownością.\u001b[0m\n",
      "[2025-09-05 16:40:23,461][__main__][INFO] [Agent Ogólny] Brak istotnych wspomnień, odpowiadam na podstawie bieżącego kontekstu.\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "[2025-09-05 16:40:27,814][__main__][INFO] [Pamięć] Dodano wpis od 'Agent Ogólny'. Całkowita liczba wpisów: 2.\n",
      "[2025-09-05 16:40:27,815][__main__][INFO] \u001b[1;34mAgent Ogólny: <think> Okay, the user is asking about the profitability of their new AI-based logistics system that cut costs by 20% but requires expensive cloud infrastructure. Let me break this down. First, I need to address both aspects: cost reduction and infrastructure expenses. The main point here is understanding how these two factors interact. Even though they saved 20%, if the cloud costs are higher than expected, that could offset savings. I should consider their goals. They probably want to know whether the overall investment in the AI system is justified despite the cloud expenses. Maybe they're planning to scale or expand operations soon. If they do, the per-unit cost of infrastructure might become more manageable. Also, looking at possible angles: economies of scale could play a role once they start processing large amounts of data regularly. Long-term savings from efficiency improvements like reduced waste or faster delivery times might outweigh initial costs. Maybe mention ROI timelines and how long it'll take to see returns. I need to keep the response concise in Polish, so avoid technical jargon but still be precise. Check for any hidden factors they might not have mentioned yet, like potential gains from automating certain tasks or integrating with existing systems more efficiently. Maybe suggest areas where further optimizations could help balance the costs.\u001b[0m\n",
      "[2025-09-05 16:40:28,816][__main__][INFO] [Agent Finansowy] Brak istotnych wspomnień, odpowiadam na podstawie bieżącego kontekstu.\n",
      "[2025-09-05 16:40:29,194][__main__][WARNING] Odpowiedź odrzucona (za krótka): ''\n",
      "[2025-09-05 16:40:29,194][__main__][WARNING] Agent Finansowy nie wygenerował poprawnej odpowiedzi. Koniec tury.\n",
      "[2025-09-05 16:40:30,195][__main__][INFO] [Agent Ogólny] Brak istotnych wspomnień, odpowiadam na podstawie bieżącego kontekstu.\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "[2025-09-05 16:40:34,635][__main__][INFO] [Pamięć] Dodano wpis od 'Agent Ogólny'. Całkowita liczba wpisów: 3.\n",
      "[2025-09-05 16:40:34,636][__main__][INFO] \u001b[1;34mAgent Ogólny: Okay, let's tackle the user's question. They mentioned that their AI logistics system reduced costs by 20% but needs expensive cloud infrastructure. So, they're wondering if it's profitable overall. First, I should acknowledge the cost reduction as a positive point. A 20% savings is significant. Then, address the cloud infrastructure expense. Without specific numbers, maybe ask for more details to make a better assessment. They might need to calculate net savings. For example, if they invested X amount in the AI system and Y in cloud costs, which factor dominates? Also, consider operational factors like time saved, scalability benefits, or efficiency gains beyond cost reduction. Possible angles: If they're scaling up quickly, maybe the infrastructure cost will decrease per unit as demand increases. They could also look into optimizing their AI usage to reduce resource consumption, like processing less data during off-hours. Another thought: Maybe compare it with other models. For instance, if they transitioned from an old system that had higher costs or inefficiencies, perhaps the total savings are better than just 20%. Also, consider hidden costs. Some companies might have additional expenses when implementing new tech, so maybe the user isn't factoring those in yet. Long-term perspective:\u001b[0m\n",
      "[2025-09-05 16:40:35,637][__main__][INFO] [Agent Finansowy] Odnaleziono 1 istotnych wspomnień.\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "[2025-09-05 16:40:37,837][__main__][INFO] [Pamięć] Dodano wpis od 'Agent Finansowy'. Całkowita liczba wpisów: 4.\n",
      "[2025-09-05 16:40:37,838][__main__][INFO] \u001b[1;32mAgent Finansowy: </think> Aby ocenić rentowność systemu, potrzebuję więcej szczegółów – np. koszt implementacji AI, utrzymania chmurowego i skuteczności w dłuższej perspektywie. Jeśli 20% zredukowane koszty przewyższają koszty infrastruktury, to system jest rentowny. W przeciwnym wypadku warto rozważać optymalizacje (np. redukcję danych przetwarzanych przez AI lub wydajniejsze rozwiązanie chmurowe).\u001b[0m\n",
      "[2025-09-05 16:40:38,839][__main__][INFO] [Agent Ogólny] Odnaleziono 1 istotnych wspomnień.\n",
      "[2025-09-05 16:40:39,964][__main__][WARNING] Odpowiedź odrzucona (za krótka): ''\n",
      "[2025-09-05 16:40:39,964][__main__][WARNING] Agent Ogólny nie wygenerował poprawnej odpowiedzi. Koniec tury.\n",
      "[2025-09-05 16:40:40,965][__main__][INFO] [Agent Finansowy] Odnaleziono 1 istotnych wspomnień.\n",
      "[2025-09-05 16:40:41,700][__main__][WARNING] Odpowiedź odrzucona (za krótka): ''\n",
      "[2025-09-05 16:40:41,700][__main__][WARNING] Agent Finansowy nie wygenerował poprawnej odpowiedzi. Koniec tury.\n",
      "[2025-09-05 16:40:42,701][__main__][INFO] --- Koniec symulacji rozmowy ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ulepszona wersja skryptu do symulacji rozmowy między modelami LLM.\n",
    "\n",
    "Główne zmiany w wersji 8.0 (Lepsze Prompty i Parser):\n",
    "1.  **Inteligentny Parser Odpowiedzi (`_parse_and_validate_response`)**:\n",
    "    Zastępuje prostą funkcję czyszczącą. Potrafi teraz aktywnie usuwać\n",
    "    całe bloki \"myśli\" (np. <think>...</think>) i wyodrębniać właściwą\n",
    "    odpowiedź z potencjalnie zaszumionego wyjścia modelu.\n",
    "2.  **Nowa Technika Promptowania (\"Role-playing with examples\")**:\n",
    "    Prompt został całkowicie przebudowany. Zamiast listy suchych instrukcji,\n",
    "    teraz jasno definiuje rolę agenta i, co najważniejsze, zawiera krótki\n",
    "    przykład idealnej interakcji (tzw. few-shot prompting). Jest to\n",
    "    znacznie skuteczniejsza metoda instruowania modeli.\n",
    "3.  **Zwiększona Odporność**: Połączenie lepszego promptu na wejściu\n",
    "    i inteligentniejszego parsera na wyjściu powinno radykalnie zwiększyć\n",
    "    odporność symulacji na \"lekkie\" błędy modeli i zapewnić płynniejszy\n",
    "    przebieg rozmowy.\n",
    "\"\"\"\n",
    "import logging\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import nest_asyncio\n",
    "\n",
    "# --- Sprawdzenie dostępności llama_cpp i definicje klas pozornych ---\n",
    "try:\n",
    "    from llama_cpp import Llama, LlamaCache, LLAMA_SPLIT_MODE_NONE\n",
    "    LLAMA_CPP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    logging.warning(\"Biblioteka 'llama_cpp' nie znaleziona. Używam obiektów pozornych (mock).\")\n",
    "    LLAMA_CPP_AVAILABLE = False\n",
    "    \n",
    "    class Llama:\n",
    "        def __init__(self, model_path: str = \"mock_path\", **kwargs):\n",
    "            self._model_name = Path(model_path).name\n",
    "            logging.info(f\"[MOCK] Inicjalizacja Llama dla modelu: {self._model_name}\")\n",
    "        def create_completion(self, prompt: str, **kwargs) -> Dict[str, Any]:\n",
    "            response_text = f\"<think>Model {self._model_name} myśli...</think> A oto dynamiczna, pozorowana odpowiedź po polsku.\"\n",
    "            return {'choices': [{'text': response_text}]}\n",
    "        def embed(self, text: str) -> List[float]:\n",
    "            logging.info(f\"[MOCK] Tworzenie osadzeń dla tekstu: '{text[:40]}...'\")\n",
    "            np.random.seed(sum(ord(c) for c in text))\n",
    "            return np.random.randn(128).tolist()\n",
    "\n",
    "    class LlamaCache: pass\n",
    "    LLAMA_SPLIT_MODE_NONE = 0\n",
    "\n",
    "# --- Aliasy i Konfiguracja (bez zmian) ---\n",
    "ConversationHistory = List[Tuple[str, str]]\n",
    "MemoryEntry = Tuple[str, str, List[float]]\n",
    "\n",
    "@dataclass\n",
    "class LlamaParams:\n",
    "    n_gpu_layers: int = -1; main_gpu: int = 0; n_ctx: int = 4096\n",
    "    n_threads: int = 8; embedding: bool = False; verbose: bool = False\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    path: str; is_embedding: bool = False\n",
    "\n",
    "@dataclass\n",
    "class ModelsConfig:\n",
    "    \"\"\"Konfiguracja wszystkich modeli.\"\"\"\n",
    "    models: Dict[str, ModelConfig] = field(default_factory=lambda: {\n",
    "        \"base\": ModelConfig(path=os.getenv(\"BASE_MODEL_PATH\", \"/run/media/dominik/F2B452FDB452C42F/LLM models/Qwen3-Esper3-Reasoning-Instruct-6B-Brainstorm20x-Enhanced-E32-192k-ctx.i1-Q4_K_M.gguf\")),\n",
    "        \"finance_instruct\": ModelConfig(path=os.getenv(\"FINANCE_MODEL_PATH\", \"/run/media/dominik/F2B452FDB452C42F/LLM models/qwen3-4B-Claude-Sonnet-4-Reasoning-Distill_Q4_K_M.gguf\")),\n",
    "        \"embedding\": ModelConfig(path=os.getenv(\"EMBEDDING_MODEL_PATH\", \"/run/media/dominik/F2B452FDB452C42F/LLM models/nomic-embed-text-v1.5-Q4_K_M.gguf\"), is_embedding=True)\n",
    "    })\n",
    "\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    models: ModelsConfig = field(default_factory=ModelsConfig)\n",
    "    llama_params: LlamaParams = field(default_factory=LlamaParams)\n",
    "    max_tokens: int = 256\n",
    "    embedding_cache_size: int = 100\n",
    "    memory_retrieval_threshold: float = 0.65\n",
    "    repeat_penalty: float = 1.15\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s][%(name)s][%(levelname)s] %(message)s\", force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def cosine_similarity(v1: List[float], v2: List[float]) -> float:\n",
    "    vec1, vec2 = np.array(v1), np.array(v2)\n",
    "    dot = np.dot(vec1, vec2); norm = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot / norm if norm > 0 else 0.0\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    return text.strip().replace(\"\\n\", \" \")[:512]\n",
    "\n",
    "# --- Zarządzanie Modelami (bez zmian) ---\n",
    "class ModelLoader:\n",
    "    def __init__(self, config: AppConfig):\n",
    "        self.config = config; self.models: Dict[str, Llama] = {}\n",
    "        self.locks: Dict[str, asyncio.Lock] = {}; self.embedding_cache: Dict[str, List[float]] = {}\n",
    "\n",
    "    def _setup_single_model(self, model_name: str, model_config: ModelConfig) -> Optional[Llama]:\n",
    "        path = Path(model_config.path)\n",
    "        if not model_config.path or (LLAMA_CPP_AVAILABLE and not path.is_file()):\n",
    "            logger.warning(f\"Plik modelu '{model_name}' nie istnieje. Używam mocka.\")\n",
    "            return Llama(model_path=model_config.path, embedding=model_config.is_embedding)\n",
    "        try:\n",
    "            params = self.config.llama_params\n",
    "            llm = Llama(model_path=model_config.path, n_gpu_layers=params.n_gpu_layers,\n",
    "                        main_gpu=params.main_gpu, n_ctx=params.n_ctx, n_threads=params.n_threads,\n",
    "                        embedding=model_config.is_embedding, split_mode=LLAMA_SPLIT_MODE_NONE,\n",
    "                        cache=LlamaCache() if LLAMA_CPP_AVAILABLE else None, verbose=params.verbose)\n",
    "            logger.info(f\"Model '{model_name}' wczytany poprawnie.\")\n",
    "            return llm\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Błąd ładowania modelu '{model_name}': {e}\"); return None\n",
    "\n",
    "    async def load_all_models(self) -> bool:\n",
    "        load_tasks = {name: asyncio.to_thread(self._setup_single_model, name, mc) for name, mc in self.config.models.models.items()}\n",
    "        results = await asyncio.gather(*load_tasks.values())\n",
    "        all_loaded = True\n",
    "        for (name, _), model in zip(load_tasks.items(), results):\n",
    "            if model: self.models[name], self.locks[name] = model, asyncio.Lock()\n",
    "            else: all_loaded = False\n",
    "        return all_loaded\n",
    "\n",
    "    def unload_all_models(self):\n",
    "        self.models.clear(); self.locks.clear(); self.embedding_cache.clear()\n",
    "\n",
    "    def get_model_and_lock(self, name: str) -> Optional[Tuple[Llama, asyncio.Lock]]:\n",
    "        return self.models.get(name), self.locks.get(name)\n",
    "\n",
    "# --- Pamięć Wektorowa (bez zmian) ---\n",
    "class VectorMemory:\n",
    "    def __init__(self):\n",
    "        self.entries: List[MemoryEntry] = []\n",
    "\n",
    "    def add_entry(self, speaker: str, text: str, embedding: List[float]):\n",
    "        self.entries.append((speaker, text, embedding))\n",
    "        logger.info(f\"[Pamięć] Dodano wpis od '{speaker}'. Całkowita liczba wpisów: {len(self.entries)}.\")\n",
    "\n",
    "    def search_similar(self, query_embedding: List[float], top_k: int = 2, threshold: float = 0.65) -> List[MemoryEntry]:\n",
    "        if not self.entries: return []\n",
    "        similarities = [cosine_similarity(query_embedding, entry[2]) for entry in self.entries]\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        results = []\n",
    "        for i in sorted_indices:\n",
    "            if len(results) >= top_k: break\n",
    "            if similarities[i] >= threshold and i < len(self.entries) - 1:\n",
    "                results.append(self.entries[i])\n",
    "        return results\n",
    "\n",
    "# --- Agent Konwersacyjny (ZMODYFIKOWANY) ---\n",
    "class ConversationalAgent:\n",
    "    def __init__(self, name: str, model_loader: ModelLoader, completion_model: str, memory: VectorMemory):\n",
    "        self.name = name; self.model_loader = model_loader\n",
    "        self.completion_model = completion_model; self.memory = memory\n",
    "\n",
    "    async def _get_embedding(self, text: str) -> Optional[List[float]]:\n",
    "        model, lock = self.model_loader.get_model_and_lock(\"embedding\")\n",
    "        if not model: return None\n",
    "        preprocessed = preprocess_text(text)\n",
    "        if preprocessed in self.model_loader.embedding_cache:\n",
    "            return self.model_loader.embedding_cache[preprocessed]\n",
    "        try:\n",
    "            async with lock:\n",
    "                embedding = await asyncio.to_thread(model.embed, preprocessed)\n",
    "                if len(self.model_loader.embedding_cache) < self.model_loader.config.embedding_cache_size:\n",
    "                    self.model_loader.embedding_cache[preprocessed] = embedding\n",
    "                return embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Błąd tworzenia osadzenia dla '{text[:30]}...': {e}\"); return None\n",
    "\n",
    "    def _parse_and_validate_response(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Inteligentnie parsuje i waliduje odpowiedź modelu.\"\"\"\n",
    "        # 1. Usuń bloki <think>\n",
    "        cleaned_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
    "        \n",
    "        # 2. Podziel na linie i weź ostatnią niepustą - często to jest właściwa odpowiedź\n",
    "        lines = [line.strip() for line in cleaned_text.split('\\n') if line.strip()]\n",
    "        if not lines:\n",
    "            logger.warning(\"Odpowiedź odrzucona (pusta po parsowaniu).\")\n",
    "            return None\n",
    "        \n",
    "        final_response = lines[-1]\n",
    "        \n",
    "        # 3. Dodatkowe czyszczenie artefaktów\n",
    "        final_response = re.sub(r'^\\s*(Użytkownik|Agent Ogólny|Agent Finansowy):\\s*', '', final_response, flags=re.IGNORECASE).strip()\n",
    "        \n",
    "        # 4. Walidacja\n",
    "        if len(final_response) < 15:\n",
    "            logger.warning(f\"Odpowiedź odrzucona (za krótka po parsowaniu): '{final_response}'\")\n",
    "            return None\n",
    "        \n",
    "        return final_response\n",
    "\n",
    "    async def _generate_response(self, history: ConversationHistory, retrieved_memories: List[MemoryEntry]) -> Optional[str]:\n",
    "        model, lock = self.model_loader.get_model_and_lock(self.completion_model)\n",
    "        if not model: return None\n",
    "        \n",
    "        formatted_history = \"\\n\".join([f\"{s}: {t}\" for s, t in history[-4:]])\n",
    "        \n",
    "        memory_section = \"Brak.\"\n",
    "        if retrieved_memories:\n",
    "            memory_texts = [f\"- {s} wcześniej powiedział: '{t}'\" for s, t, _ in retrieved_memories]\n",
    "            memory_section = \"\\n\".join(memory_texts)\n",
    "\n",
    "        prompt = (\n",
    "            f\"Twoim jedynym zadaniem jest odegrać rolę asystenta AI o nazwie '{self.name}' w rozmowie.\\n\"\n",
    "            f\"Odpowiedź MUSI być JEDYNIE tekstem wypowiedzi w języku polskim. NIE dodawaj swoich myśli, tagów ani niczego innego.\\n\\n\"\n",
    "            f\"### PRZYKŁAD POPRAWNEJ ODPOWIEDZI:\\n\"\n",
    "            f\"Użytkownik: Myślę o nowym projekcie.\\n\"\n",
    "            f\"{self.name}: Brzmi interesująco, czy możesz opowiedzieć o nim coś więcej?\\n\\n\"\n",
    "            f\"### KONIEC PRZYKŁADU ###\\n\\n\"\n",
    "            f\"--- AKTUALNA ROZMOWA ---\\n\"\n",
    "            f\"### Kluczowe fakty z tej rozmowy:\\n{memory_section}\\n\\n\"\n",
    "            f\"### Ostatnie wiadomości:\\n{formatted_history}\\n\\n\"\n",
    "            f\"--- KONIEC ROZMOWY ---\\n\\n\"\n",
    "            f\"{self.name}:\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            async with lock:\n",
    "                stop = [f\"{name}:\" for name in [\"Użytkownik\", \"Agent Ogólny\", \"Agent Finansowy\"]]\n",
    "                output = await asyncio.to_thread(model.create_completion, prompt,\n",
    "                    max_tokens=self.model_loader.config.max_tokens,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    stop=stop,\n",
    "                    repeat_penalty=self.model_loader.config.repeat_penalty\n",
    "                )\n",
    "                raw_text = output['choices'][0]['text'].strip()\n",
    "                return self._parse_and_validate_response(raw_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Błąd generowania odpowiedzi: {e}\"); return None\n",
    "\n",
    "    async def respond(self, history: ConversationHistory) -> Optional[str]:\n",
    "        if not history: return None\n",
    "        last_utterance = history[-1][1]\n",
    "        query_embedding = await self._get_embedding(last_utterance)\n",
    "        if not query_embedding:\n",
    "            logger.warning(\"Nie udało się stworzyć wektora zapytania. Odpowiadam bez pamięci.\")\n",
    "            return await self._generate_response(history, [])\n",
    "        retrieved_memories = self.memory.search_similar(query_embedding, top_k=2, threshold=self.model_loader.config.memory_retrieval_threshold)\n",
    "        if retrieved_memories:\n",
    "            logger.info(f\"[{self.name}] Odnaleziono {len(retrieved_memories)} istotnych wspomnień.\")\n",
    "        else:\n",
    "            logger.info(f\"[{self.name}] Brak istotnych wspomnień, odpowiadam na podstawie bieżącego kontekstu.\")\n",
    "        return await self._generate_response(history, retrieved_memories)\n",
    "\n",
    "# --- Symulator Rozmowy (bez zmian) ---\n",
    "class ConversationSimulator:\n",
    "    def __init__(self, agents: List[ConversationalAgent], memory: VectorMemory, initial_prompt: Tuple[str, str], max_turns: int = 4):\n",
    "        self.agents = agents; self.memory = memory\n",
    "        self.history: ConversationHistory = []\n",
    "        self.initial_prompt = initial_prompt\n",
    "        self.max_turns = max_turns\n",
    "\n",
    "    async def initialize(self):\n",
    "        speaker, text = self.initial_prompt\n",
    "        embedding = await self.agents[0]._get_embedding(text)\n",
    "        if embedding:\n",
    "            self.history.append((speaker, text))\n",
    "            self.memory.add_entry(speaker, text, embedding)\n",
    "        else:\n",
    "            raise RuntimeError(\"Nie udało się zwektoryzować początkowego promptu.\")\n",
    "\n",
    "    async def run(self):\n",
    "        await self.initialize()\n",
    "        logger.info(\"--- Rozpoczynam symulację rozmowy (v8 Lepsze Prompty i Parser) ---\")\n",
    "        logger.info(f\"\\033[1;33m{self.history[0][0]}: {self.history[0][1]}\\033[0m\")\n",
    "        \n",
    "        current_agent_idx = 0\n",
    "        for _ in range(self.max_turns * len(self.agents)):\n",
    "            agent = self.agents[current_agent_idx]\n",
    "            response = await agent.respond(self.history)\n",
    "            \n",
    "            if response:\n",
    "                embedding = await agent._get_embedding(response)\n",
    "                if embedding:\n",
    "                    self.history.append((agent.name, response))\n",
    "                    self.memory.add_entry(agent.name, response, embedding)\n",
    "                    color = \"\\033[1;34m\" if current_agent_idx == 0 else \"\\033[1;32m\"\n",
    "                    logger.info(f\"{color}{agent.name}: {response}\\033[0m\")\n",
    "                else:\n",
    "                    logger.warning(f\"Nie udało się zwektoryzować ZWALIDOWANEJ odpowiedzi od {agent.name}.\")\n",
    "            else:\n",
    "                logger.warning(f\"{agent.name} nie wygenerował poprawnej odpowiedzi. Koniec tury.\")\n",
    "            \n",
    "            current_agent_idx = (current_agent_idx + 1) % len(self.agents)\n",
    "            await asyncio.sleep(1)\n",
    "        logger.info(\"--- Koniec symulacji rozmowy ---\")\n",
    "\n",
    "# --- Główna Funkcja Aplikacji (bez zmian) ---\n",
    "async def main():\n",
    "    config = AppConfig()\n",
    "    loader = ModelLoader(config)\n",
    "    try:\n",
    "        if await loader.load_all_models():\n",
    "            vector_memory = VectorMemory()\n",
    "            agent_a = ConversationalAgent(\"Agent Ogólny\", loader, \"base\", vector_memory)\n",
    "            agent_b = ConversationalAgent(\"Agent Finansowy\", loader, \"finance_instruct\", vector_memory)\n",
    "            \n",
    "            initial_prompt = (\"Użytkownik\", \"Nasz nowy system logistyczny oparty na AI zredukował koszty o 20%, ale wymaga drogiej infrastruktury chmurowej. Zastanawiam się nad jego rentownością.\")\n",
    "            simulator = ConversationSimulator([agent_a, agent_b], vector_memory, initial_prompt, max_turns=3)\n",
    "            await simulator.run()\n",
    "    finally:\n",
    "        loader.unload_all_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        nest_asyncio.apply()\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cf90a",
   "metadata": {},
   "source": [
    "## Web Scraper - TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Any\n",
    "from urllib.parse import urlparse, quote_plus, parse_qs, unquote\n",
    "\n",
    "import httpx\n",
    "import cloudscraper\n",
    "from selectolax.parser import HTMLParser\n",
    "from trafilatura import extract\n",
    "import nest_asyncio\n",
    "\n",
    "# Nowy, potężniejszy arsenał\n",
    "from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "from playwright_stealth import Stealth\n",
    "\n",
    "# Zakładamy, że plik Agent_Config.py z klasą AppConfig istnieje w tym samym folderze\n",
    "from Agent_Config import AppConfig\n",
    "\n",
    "# --- Konfiguracja ---\n",
    "config = AppConfig()\n",
    "\n",
    "# --- Ustawienia Logowania ---\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s][%(levelname)s] %(message)s\")\n",
    "for logger_name in [\"httpx\", \"selenium\", \"urllib3\", \"playwright\"]:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(\"ULTIMATE_SCRAPER\")\n",
    "\n",
    "# --- Katalog na Zrzuty Ekranu ---\n",
    "SCREENSHOT_DIR = \"screenshots\"\n",
    "os.makedirs(SCREENSHOT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "class UltimateScraper:\n",
    "    def __init__(self, app_config: AppConfig):\n",
    "        self.config = app_config\n",
    "        self._client: Optional[httpx.AsyncClient] = None\n",
    "        self._semaphore = asyncio.Semaphore(self.config.network.CONCURRENCY_LIMIT)\n",
    "        self.last_ip_rotation = 0\n",
    "        self.search_engines = [\n",
    "            {\n",
    "                \"name\": \"DuckDuckGo HTML\",\n",
    "                \"url\": \"https://html.duckduckgo.com/html/?q={query}\",\n",
    "                \"selector\": \"a.result__a\",\n",
    "                \"decode\": True\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    async def _rotate_ip(self) -> bool:\n",
    "        \"\"\"Zmienia adres IP Tora, wysyłając sygnał NEWNYM.\"\"\"\n",
    "        if not self.config.tor.USE_TOR or time.time() - self.last_ip_rotation < 10:\n",
    "            return False\n",
    "\n",
    "        logger.info(\"🧅 Wysyłam żądanie nowej tożsamości Tor (NEWNYM)...\")\n",
    "        writer = None\n",
    "        try:\n",
    "            reader, writer = await asyncio.open_connection(\n",
    "                self.config.tor.TOR_CONTROL_HOST, self.config.tor.TOR_CONTROL_PORT\n",
    "            )\n",
    "            auth_command = f'AUTHENTICATE \"{self.config.tor.TOR_CONTROL_PASSWORD}\"\\r\\n'.encode()\n",
    "            writer.write(auth_command)\n",
    "            await writer.drain()\n",
    "            if b\"250 OK\" not in await reader.read(4096):\n",
    "                logger.error(\"❌ Błąd autoryzacji w ControlPort Tora.\")\n",
    "                return False\n",
    "\n",
    "            writer.write(b\"SIGNAL NEWNYM\\r\\n\")\n",
    "            await writer.drain()\n",
    "            if b\"250 OK\" in await reader.read(4096):\n",
    "                logger.info(\"✅ Nowa tożsamość Tora przyznana. Resetuję sesję klienta.\")\n",
    "                if self._client and not self._client.is_closed:\n",
    "                    await self._client.aclose()\n",
    "                self._client = None\n",
    "                self.last_ip_rotation = time.time()\n",
    "                await asyncio.sleep(self.config.tor.NEWNYM_INTERVAL)\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(\"❌ Nie udało się wysłać sygnału NEWNYM.\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Krytyczny błąd podczas rotacji IP Tora: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if writer:\n",
    "                writer.close()\n",
    "                await writer.wait_closed()\n",
    "\n",
    "    async def _get_client(self) -> httpx.AsyncClient:\n",
    "        if self._client is None or self._client.is_closed:\n",
    "            proxy = self.config.tor.TOR_SOCKS_PROXY if self.config.tor.USE_TOR else None\n",
    "            transport = httpx.AsyncHTTPTransport(proxy=proxy) if proxy else None\n",
    "            self._client = httpx.AsyncClient(transport=transport, verify=False, timeout=self.config.network.REQ_TIMEOUT, follow_redirects=True, http2=True)\n",
    "            logger.info(f\"Zainicjowano nową sesję klienta httpx. Proxy: {proxy if transport else 'Brak'}\")\n",
    "        return self._client\n",
    "\n",
    "    def _decode_ddg_url(self, url: str) -> str:\n",
    "        if \"duckduckgo.com/l/\" in url:\n",
    "            try:\n",
    "                qs = parse_qs(urlparse(url).query); decoded = qs.get('uddg', [''])[0]\n",
    "                if decoded: return unquote(decoded)\n",
    "            except Exception: return url\n",
    "        return url\n",
    "\n",
    "    async def search(self, query: str) -> List[str]:\n",
    "        client = await self._get_client()\n",
    "        for engine in self.search_engines:\n",
    "            logger.info(f\"Wyszukuję '{query}' używając {engine['name']}...\")\n",
    "            search_url = engine['url'].format(query=quote_plus(query))\n",
    "            try:\n",
    "                response = await client.get(search_url, headers={\"User-Agent\": random.choice(self.config.network.USER_AGENTS)})\n",
    "                if response.status_code == 200:\n",
    "                    links = set()\n",
    "                    parser = HTMLParser(response.text)\n",
    "                    for node in parser.css(engine['selector']):\n",
    "                        href = node.attributes.get('href')\n",
    "                        if href:\n",
    "                            url = self._decode_ddg_url(href) if engine['decode'] else href\n",
    "                            if url.startswith('http'): links.add(url)\n",
    "                    if links:\n",
    "                        logger.info(f\"✅ {engine['name']} znalazł {len(links)} linków.\")\n",
    "                        return list(links)\n",
    "                else:\n",
    "                    logger.warning(f\"{engine['name']} zwrócił status {response.status_code}. Próbuję następny silnik...\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Błąd podczas wyszukiwania w {engine['name']}: {e}. Próbuję następny silnik...\")\n",
    "        logger.error(f\"Wszystkie silniki wyszukiwania zawiodły dla zapytania: '{query}'.\")\n",
    "        return []\n",
    "\n",
    "    def _extract_content(self, html: str) -> Optional[str]:\n",
    "        content = extract(html, include_comments=False, include_tables=True)\n",
    "        if not content:\n",
    "            logger.warning(\"Ekstrakcja treści zwróciła pusty wynik.\")\n",
    "        else:\n",
    "            logger.info(f\"Ekstrakcja treści udana, długość: {len(content)} znaków.\")\n",
    "        return content\n",
    "\n",
    "    def _fetch_playwright_sync(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Ostateczna metoda pobierania, używająca Playwright w trybie stealth zgodnie z dokumentacją.\"\"\"\n",
    "        \n",
    "        with sync_playwright() as p:\n",
    "            # Konfigurujemy ustawienia proxy dla przeglądarki\n",
    "            proxy_settings = { \"server\": self.config.tor.TOR_SOCKS_PROXY } if self.config.tor.USE_TOR else None\n",
    "            \n",
    "            browser = p.chromium.launch(headless=True, proxy=proxy_settings)\n",
    "            context = browser.new_context(\n",
    "                user_agent=random.choice(self.config.network.USER_AGENTS),\n",
    "                viewport={'width': 1920, 'height': 1080}\n",
    "            )\n",
    "            page = context.new_page()\n",
    "            \n",
    "            stealth = Stealth()\n",
    "            stealth.apply_stealth_sync(page)\n",
    "\n",
    "            try:\n",
    "                logger.info(f\"Playwright (Stealth) nawiguje do: {url}\")\n",
    "                page.goto(url, timeout=90000, wait_until='networkidle')\n",
    "                \n",
    "                html = page.content()\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                safe_url = urlparse(url).netloc.replace('.', '_')\n",
    "                page.screenshot(path=os.path.join(SCREENSHOT_DIR, f\"playwright_{safe_url}_{timestamp}.png\"))\n",
    "                \n",
    "                return html if html and len(html) > 500 else None\n",
    "\n",
    "            except PlaywrightTimeoutError as e:\n",
    "                logger.error(f\"Playwright TimeoutError: {e}\")\n",
    "                return None\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Playwright ogólny błąd: {e}\")\n",
    "                return None\n",
    "            finally:\n",
    "                browser.close()\n",
    "\n",
    "    async def fetch(self, url: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:\n",
    "        for retry in range(max_retries):\n",
    "            logger.info(f\"Próba fetch {retry + 1}/{max_retries} dla {url}\")\n",
    "            async with self._semaphore:\n",
    "                try:\n",
    "                    # Etap 1: httpx\n",
    "                    logger.info(f\"[1/3] Próba pobrania (httpx): {url}\")\n",
    "                    client = await self._get_client()\n",
    "                    response = await client.get(url, headers={\"User-Agent\": random.choice(self.config.network.USER_AGENTS)})\n",
    "                    if response.status_code == 200 and \"js-challenge\" not in response.text:\n",
    "                        content = self._extract_content(response.text)\n",
    "                        if content:\n",
    "                            logger.info(f\"✅ Sukces (httpx): {url}\")\n",
    "                            return {\"url\": url, \"content\": content, \"source\": \"httpx\"}\n",
    "                    else:\n",
    "                        logger.warning(f\"httpx zwrócił status {response.status_code} dla {url}\")\n",
    "                except (httpx.ProxyError, httpx.ConnectTimeout) as e:\n",
    "                    logger.error(f\"httpx błąd sieci/proxy dla {url}: {e}. Rotuję IP i ponawiam.\")\n",
    "                    if self.config.tor.USE_TOR:\n",
    "                        await self._rotate_ip()\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"httpx napotkał błąd dla {url}: {e}\")\n",
    "\n",
    "                # Etap 2: cloudscraper\n",
    "                logger.warning(f\"[2/3] httpx nie powiódł się. Próba (ulepszony cloudscraper): {url}\")\n",
    "                try:\n",
    "                    loop = asyncio.get_running_loop()\n",
    "                    scraper = cloudscraper.create_scraper(\n",
    "                        browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True},\n",
    "                        delay=10, \n",
    "                        enable_stealth=True, \n",
    "                        stealth_options={'human_like_delays': True, 'randomize_headers': True, 'browser_quirks': True},\n",
    "                        auto_refresh_on_403=True,\n",
    "                        max_403_retries=2,\n",
    "                        rotate_tls_ciphers=True\n",
    "                    )\n",
    "                    proxies = {\"http\": self.config.tor.TOR_SOCKS_PROXY, \"https\": self.config.tor.TOR_SOCKS_PROXY} if self.config.tor.USE_TOR else None\n",
    "                    response = await loop.run_in_executor(None, lambda: scraper.get(url, proxies=proxies, timeout=self.config.network.REQ_TIMEOUT))\n",
    "                    if response.status_code == 200:\n",
    "                        content = self._extract_content(response.text)\n",
    "                        if content:\n",
    "                            logger.info(f\"✅ Sukces (cloudscraper): {url}\")\n",
    "                            return {\"url\": url, \"content\": content, \"source\": \"cloudscraper_v2\"}\n",
    "                    else:\n",
    "                        logger.warning(f\"cloudscraper zwrócił status {response.status_code} dla {url}\")\n",
    "                except (httpx.ProxyError, httpx.ConnectTimeout) as e:\n",
    "                    logger.error(f\"cloudscraper błąd sieci/proxy dla {url}: {e}. Rotuję IP i ponawiam.\")\n",
    "                    if self.config.tor.USE_TOR:\n",
    "                        await self._rotate_ip()\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Ulepszony cloudscraper również zawiódł dla {url}: {e}\")\n",
    "\n",
    "                # Etap 3: Playwright\n",
    "                logger.warning(f\"[3/3] cloudscraper nie powiódł się. Ostateczna próba (Playwright): {url}\")\n",
    "                try:\n",
    "                    loop = asyncio.get_running_loop()\n",
    "                    html = await asyncio.wait_for(\n",
    "                        loop.run_in_executor(None, self._fetch_playwright_sync, url),\n",
    "                        timeout=180.0  # Zwiększony timeout dla wolnych połączeń Tor\n",
    "                    )\n",
    "                    if html:\n",
    "                        content = self._extract_content(html)\n",
    "                        if content:\n",
    "                            logger.info(f\"✅ Sukces (Playwright): {url}\")\n",
    "                            return {\"url\": url, \"content\": content, \"source\": \"playwright\"}\n",
    "                except asyncio.TimeoutError:\n",
    "                    logger.error(f\"❌ Ostateczna próba (playwright) dla {url} przekroczyła limit czasu 180s. Rotuję IP i ponawiam.\")\n",
    "                    if self.config.tor.USE_TOR:\n",
    "                        await self._rotate_ip()\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Ostateczna próba (playwright) dla {url} nie powiodła się: {e}\")\n",
    "\n",
    "            logger.warning(f\"Wszystkie etapy nie powiodły się w próbie {retry + 1}. Ponawiam...\")\n",
    "\n",
    "        logger.critical(f\"Pobranie {url} nie powiodło się po {max_retries} próbach.\")\n",
    "        return None\n",
    "            \n",
    "    async def run(self, query: str, max_links: int = 7, failure_threshold: int = 4):\n",
    "        start_time = time.time()\n",
    "        await self.get_public_ip()\n",
    "        \n",
    "        links = await self.search(query)\n",
    "        if not links:\n",
    "            logger.critical(\"Nie udało się znaleźć żadnych linków. Zatrzymuję pracę.\")\n",
    "            return\n",
    "\n",
    "        ROTATE_EVERY_N_REQUESTS = 2\n",
    "        fetch_counter = 0\n",
    "        failed_count = 0\n",
    "        tasks = [self.fetch(url) for url in links[:max_links]]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        valid_results = []\n",
    "        for i, res in enumerate(results):\n",
    "            fetch_counter += 1\n",
    "            if isinstance(res, Exception):\n",
    "                logger.error(f\"Pobranie linku nr {i+1} ({links[i]}) nie powiodło się: {res}\")\n",
    "                failed_count += 1\n",
    "            elif res is None:\n",
    "                logger.error(f\"Pobranie linku nr {i+1} ({links[i]}) zwróciło None.\")\n",
    "                failed_count += 1\n",
    "            else:\n",
    "                logger.info(f\"Dodano wynik dla {links[i]} (Źródło: {res['source']}, Długość treści: {len(res['content'])})\")\n",
    "                valid_results.append(res)\n",
    "            \n",
    "            if failed_count >= failure_threshold:\n",
    "                logger.critical(f\"Przekroczono próg błędów ({failed_count}/{len(tasks)}). Rotuję IP.\")\n",
    "                if self.config.tor.USE_TOR:\n",
    "                    await self._rotate_ip()\n",
    "                break\n",
    "\n",
    "            if fetch_counter % ROTATE_EVERY_N_REQUESTS == 0 and fetch_counter < len(tasks):\n",
    "                logger.info(f\"Osiągnięto próg {ROTATE_EVERY_N_REQUESTS} zapytań. Prewencyjna rotacja IP.\")\n",
    "                if self.config.tor.USE_TOR:\n",
    "                    await self._rotate_ip()\n",
    "\n",
    "        logger.info(f\"\\n--- ZAKOŃCZONO W {time.time() - start_time:.2f}s ---\")\n",
    "        logger.info(f\"Pomyślnie pobrano {len(valid_results)} z {len(tasks)} stron.\")\n",
    "        \n",
    "        if not valid_results:\n",
    "            logger.warning(\"Brak wyników do wyświetlenia.\")\n",
    "        else:\n",
    "            logger.info(f\"Liczba ważnych wyników: {len(valid_results)}\")\n",
    "            for i, doc in enumerate(valid_results):\n",
    "                logger.info(f\"Wyświetlam dokument {i+1} (Źródło: {doc['source']})\")\n",
    "                logger.info(f\"\\n===== DOKUMENT {i+1} (Źródło: {doc['source']}) =====\\nURL: {doc['url']}\\nTreść: {doc['content'][:500]}...\\n\")\n",
    "        \n",
    "        if self._client and not self._client.is_closed:\n",
    "            await self._client.aclose()\n",
    "            \n",
    "    # Metoda get_public_ip bez zmian...\n",
    "    async def get_public_ip(self) -> Optional[str]:\n",
    "        try:\n",
    "            client = await self._get_client()\n",
    "            response = await client.get(\"https://check.torproject.org/api/ip\")\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            logger.info(f\"Kontrola IP: Twój publiczny adres IP to {data.get('IP')} (Używasz Tora: {data.get('IsTor', False)})\")\n",
    "            return data.get('IP')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Błąd podczas kontroli IP: {e}\")\n",
    "            return None\n",
    "\n",
    "async def main():\n",
    "    app_config = AppConfig()\n",
    "    scraper = UltimateScraper(app_config)\n",
    "    await scraper.run(query=\"CLEOPATRA PREMIUM SPÓŁKA Z OGRANICZONĄ ODPOWIEDZIALNOŚCIĄ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26050d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
