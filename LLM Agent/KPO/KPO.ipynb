{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a23608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# === SCRAPER ===\n",
    "def extract_mid(url_or_iframe: str) -> str:\n",
    "    mid_match = re.search(r\"mid=([a-zA-Z0-9\\-_]+)\", url_or_iframe)\n",
    "    if not mid_match:\n",
    "        raise ValueError(\"Nie uda≈Ço siƒô znale≈∫ƒá mid w podanym URL/iframe.\")\n",
    "    return mid_match.group(1)\n",
    "\n",
    "def scrape_kml(mid: str) -> pd.DataFrame:\n",
    "    url = f\"https://www.google.com/maps/d/u/0/kml?forcekml=1&mid={mid}\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    root = ET.fromstring(r.content)\n",
    "    ns = {\"kml\": \"http://www.opengis.net/kml/2.2\"}\n",
    "\n",
    "    records = []\n",
    "    for pm in root.findall(\".//kml:Placemark\", ns):\n",
    "        name = pm.findtext(\"kml:name\", default=\"\", namespaces=ns)\n",
    "        desc = pm.findtext(\"kml:description\", default=\"\", namespaces=ns)\n",
    "        coords = pm.find(\".//kml:coordinates\", ns)\n",
    "\n",
    "        lon, lat, *_ = coords.text.strip().split(\",\") if coords is not None else (None, None)\n",
    "\n",
    "        record = {\n",
    "            \"Title\": name,\n",
    "            \"Description\": desc,\n",
    "            \"Longitude\": lon,\n",
    "            \"Latitude\": lat\n",
    "        }\n",
    "\n",
    "        # ExtendedData\n",
    "        ext = pm.find(\".//kml:ExtendedData\", ns)\n",
    "        if ext is not None:\n",
    "            for data in ext.findall(\"kml:Data\", ns):\n",
    "                key = data.attrib.get(\"name\")\n",
    "                val = data.findtext(\"kml:value\", default=\"\", namespaces=ns)\n",
    "                record[key] = val\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# === CLEANER ===\n",
    "def clean_csv(input_filename: str, output_filename: str):\n",
    "    try:\n",
    "        with open(input_filename, 'r', encoding='utf-8') as csvfile:\n",
    "            dialect = csv.Sniffer().sniff(csvfile.read(1024))\n",
    "            csvfile.seek(0)\n",
    "            separator = dialect.delimiter\n",
    "            print(f\"Wykryto separator kolumn: '{separator}'\")\n",
    "\n",
    "        df = pd.read_csv(input_filename, sep=separator)\n",
    "        print(\"\\nPoprawnie wczytano plik. Nag≈Ç√≥wki kolumn:\")\n",
    "        print(list(df.columns))\n",
    "\n",
    "        # Kolumny do dropa\n",
    "        columns_to_drop = ['Description', 'Longitude', 'Latitude']\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "        if existing_columns_to_drop:\n",
    "            df = df.drop(columns=existing_columns_to_drop)\n",
    "            print(f\"\\nUsuniƒôto kolumny: {', '.join(existing_columns_to_drop)}\")\n",
    "        else:\n",
    "            print(\"\\nBrak kolumn do usuniƒôcia.\")\n",
    "\n",
    "        # Drop pustych kolumn\n",
    "        initial_column_count = len(df.columns)\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        final_column_count = len(df.columns)\n",
    "        if initial_column_count > final_column_count:\n",
    "            print(\"Usuniƒôto puste kolumny.\")\n",
    "        else:\n",
    "            print(\"Nie by≈Ço ca≈Çkowicie pustych kolumn.\")\n",
    "\n",
    "        df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n‚úÖ Oczyszczone dane zapisane w '{output_filename}' ({len(df)} rekord√≥w).\")\n",
    "\n",
    "        print(\"\\nPodglƒÖd 5 pierwszych wierszy:\")\n",
    "        print(df.head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"B≈ÇƒÖd: Plik '{input_filename}' nie zosta≈Ç znaleziony.\")\n",
    "    except Exception as e:\n",
    "        print(f\"WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd: {e}\")\n",
    "\n",
    "# === ENTRYPOINT COLAB ===\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT = \"https://www.google.com/maps/d/embed?mid=1GDQK9SeuuqI07MFnngUMASRKvb0ZG4U&ehbc=2E312F\"\n",
    "    MAP_ID = extract_mid(INPUT)\n",
    "    print(f\"Using MAP_ID: {MAP_ID}\")\n",
    "\n",
    "    # Scraping\n",
    "    df = scrape_kml(MAP_ID)\n",
    "    raw_file = \"edge_scraped.csv\"\n",
    "    df.to_csv(raw_file, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"‚úÖ Surowe dane zapisane do '{raw_file}' ({len(df)} rekord√≥w).\")\n",
    "\n",
    "    # Cleaning\n",
    "    clean_csv(raw_file, \"oczyszczone_dane.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_run.py\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# --- Konfiguracja ---\n",
    "INPUT_CSV_PATH = \"oczyszczone_dane.csv\"\n",
    "OUTPUT_CSV_PATH = \"wyniki_analizy.csv\"  \n",
    "SCRIPT_TO_RUN = \"Agent_Run.py\"\n",
    "\n",
    "# --- Logika skryptu ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"G≈Ç√≥wna funkcja uruchomieniowa.\"\"\"\n",
    "\n",
    "    print(\"--- PRZYGOTOWANIE DO URUCHOMIENIA ANALIZY ---\", flush=True)\n",
    "\n",
    "    # 1. Sprawdzenia plik√≥w\n",
    "    if not os.path.exists(INPUT_CSV_PATH):\n",
    "        print(f\"‚ùå B≈ÅƒÑD: Plik wej≈õciowy '{INPUT_CSV_PATH}' nie zosta≈Ç znaleziony.\")\n",
    "        print(\"     Upewnij siƒô, ≈ºe plik istnieje i jest w tym samym katalogu.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not os.path.exists(SCRIPT_TO_RUN):\n",
    "        print(f\"‚ùå B≈ÅƒÑD: G≈Ç√≥wny skrypt '{SCRIPT_TO_RUN}' nie zosta≈Ç znaleziony.\")\n",
    "        print(\"     Sprawd≈∫ ≈õcie≈ºkƒô do skryptu run_analysis.py.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 2. Przygotowanie polecenia\n",
    "    command = [\n",
    "        sys.executable,\n",
    "        \"-u\",  # Wymusza tryb bez buforowania\n",
    "        SCRIPT_TO_RUN,\n",
    "        \"--input-file\",\n",
    "        INPUT_CSV_PATH,\n",
    "        \"--output-file\",\n",
    "        OUTPUT_CSV_PATH\n",
    "    ]\n",
    "\n",
    "    print(\"‚úÖ Konfiguracja poprawna. Rozpoczynam analizƒô...\")\n",
    "    print(f\"‚ñ∂Ô∏è Uruchamiam polecenie: {' '.join(command)}\\n\")\n",
    "\n",
    "    # 3. Uruchomienie procesu i przechwytywanie wyj≈õcia na ≈ºywo\n",
    "    try:\n",
    "        with subprocess.Popen(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            encoding='utf-8',\n",
    "            bufsize=1\n",
    "        ) as process:\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                print(line, end='')\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            raise subprocess.CalledProcessError(process.returncode, process.args)\n",
    "\n",
    "        print(f\"\\n--- Analiza zako≈Ñczona pomy≈õlnie ---\")\n",
    "        if os.path.exists(OUTPUT_CSV_PATH):\n",
    "            print(f\"üéâ Wyniki zosta≈Çy zapisane w pliku: {os.path.abspath(OUTPUT_CSV_PATH)}\")\n",
    "        else:\n",
    "            print(f\"ü§î OSTRZE≈ªENIE: Proces zako≈Ñczy≈Ç siƒô, ale plik wyj≈õciowy '{OUTPUT_CSV_PATH}' nie zosta≈Ç utworzony.\")\n",
    "\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"\\n‚ùå B≈ÅƒÑD: WystƒÖpi≈Ç problem podczas wykonywania skryptu '{SCRIPT_TO_RUN}'.\")\n",
    "        print(\"     Sprawd≈∫ powy≈ºsze logi, aby znale≈∫ƒá przyczynƒô b≈Çƒôdu.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Analiza przerwana przez u≈ºytkownika.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå B≈ÅƒÑD: WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd: {e}\")\n",
    "    \n",
    "    print(\"\\n--- SKRYPT G≈Å√ìWNY ZAKO≈ÉCZONY ---\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e962fd",
   "metadata": {},
   "source": [
    "## llama.cpp ROCm -->TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9350d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-05 16:40:23,009][__main__][INFO] Model 'embedding' wczytany poprawnie.\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
      "[2025-09-05 16:40:23,260][__main__][INFO] Model 'finance_instruct' wczytany poprawnie.\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (196608) -- the full capacity of the model will not be utilized\n",
      "[2025-09-05 16:40:23,419][__main__][INFO] Model 'base' wczytany poprawnie.\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "[2025-09-05 16:40:23,460][__main__][INFO] [Pamiƒôƒá] Dodano wpis od 'U≈ºytkownik'. Ca≈Çkowita liczba wpis√≥w: 1.\n",
      "[2025-09-05 16:40:23,460][__main__][INFO] --- Rozpoczynam symulacjƒô rozmowy (v7 Higiena Pamiƒôci) ---\n",
      "[2025-09-05 16:40:23,461][__main__][INFO] \u001b[1;33mU≈ºytkownik: Nasz nowy system logistyczny oparty na AI zredukowa≈Ç koszty o 20%, ale wymaga drogiej infrastruktury chmurowej. Zastanawiam siƒô nad jego rentowno≈õciƒÖ.\u001b[0m\n",
      "[2025-09-05 16:40:23,461][__main__][INFO] [Agent Og√≥lny] Brak istotnych wspomnie≈Ñ, odpowiadam na podstawie bie≈ºƒÖcego kontekstu.\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "[2025-09-05 16:40:27,814][__main__][INFO] [Pamiƒôƒá] Dodano wpis od 'Agent Og√≥lny'. Ca≈Çkowita liczba wpis√≥w: 2.\n",
      "[2025-09-05 16:40:27,815][__main__][INFO] \u001b[1;34mAgent Og√≥lny: <think> Okay, the user is asking about the profitability of their new AI-based logistics system that cut costs by 20% but requires expensive cloud infrastructure. Let me break this down. First, I need to address both aspects: cost reduction and infrastructure expenses. The main point here is understanding how these two factors interact. Even though they saved 20%, if the cloud costs are higher than expected, that could offset savings. I should consider their goals. They probably want to know whether the overall investment in the AI system is justified despite the cloud expenses. Maybe they're planning to scale or expand operations soon. If they do, the per-unit cost of infrastructure might become more manageable. Also, looking at possible angles: economies of scale could play a role once they start processing large amounts of data regularly. Long-term savings from efficiency improvements like reduced waste or faster delivery times might outweigh initial costs. Maybe mention ROI timelines and how long it'll take to see returns. I need to keep the response concise in Polish, so avoid technical jargon but still be precise. Check for any hidden factors they might not have mentioned yet, like potential gains from automating certain tasks or integrating with existing systems more efficiently. Maybe suggest areas where further optimizations could help balance the costs.\u001b[0m\n",
      "[2025-09-05 16:40:28,816][__main__][INFO] [Agent Finansowy] Brak istotnych wspomnie≈Ñ, odpowiadam na podstawie bie≈ºƒÖcego kontekstu.\n",
      "[2025-09-05 16:40:29,194][__main__][WARNING] Odpowied≈∫ odrzucona (za kr√≥tka): ''\n",
      "[2025-09-05 16:40:29,194][__main__][WARNING] Agent Finansowy nie wygenerowa≈Ç poprawnej odpowiedzi. Koniec tury.\n",
      "[2025-09-05 16:40:30,195][__main__][INFO] [Agent Og√≥lny] Brak istotnych wspomnie≈Ñ, odpowiadam na podstawie bie≈ºƒÖcego kontekstu.\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "[2025-09-05 16:40:34,635][__main__][INFO] [Pamiƒôƒá] Dodano wpis od 'Agent Og√≥lny'. Ca≈Çkowita liczba wpis√≥w: 3.\n",
      "[2025-09-05 16:40:34,636][__main__][INFO] \u001b[1;34mAgent Og√≥lny: Okay, let's tackle the user's question. They mentioned that their AI logistics system reduced costs by 20% but needs expensive cloud infrastructure. So, they're wondering if it's profitable overall. First, I should acknowledge the cost reduction as a positive point. A 20% savings is significant. Then, address the cloud infrastructure expense. Without specific numbers, maybe ask for more details to make a better assessment. They might need to calculate net savings. For example, if they invested X amount in the AI system and Y in cloud costs, which factor dominates? Also, consider operational factors like time saved, scalability benefits, or efficiency gains beyond cost reduction. Possible angles: If they're scaling up quickly, maybe the infrastructure cost will decrease per unit as demand increases. They could also look into optimizing their AI usage to reduce resource consumption, like processing less data during off-hours. Another thought: Maybe compare it with other models. For instance, if they transitioned from an old system that had higher costs or inefficiencies, perhaps the total savings are better than just 20%. Also, consider hidden costs. Some companies might have additional expenses when implementing new tech, so maybe the user isn't factoring those in yet. Long-term perspective:\u001b[0m\n",
      "[2025-09-05 16:40:35,637][__main__][INFO] [Agent Finansowy] Odnaleziono 1 istotnych wspomnie≈Ñ.\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "[2025-09-05 16:40:37,837][__main__][INFO] [Pamiƒôƒá] Dodano wpis od 'Agent Finansowy'. Ca≈Çkowita liczba wpis√≥w: 4.\n",
      "[2025-09-05 16:40:37,838][__main__][INFO] \u001b[1;32mAgent Finansowy: </think> Aby oceniƒá rentowno≈õƒá systemu, potrzebujƒô wiƒôcej szczeg√≥≈Ç√≥w ‚Äì np. koszt implementacji AI, utrzymania chmurowego i skuteczno≈õci w d≈Çu≈ºszej perspektywie. Je≈õli 20% zredukowane koszty przewy≈ºszajƒÖ koszty infrastruktury, to system jest rentowny. W przeciwnym wypadku warto rozwa≈ºaƒá optymalizacje (np. redukcjƒô danych przetwarzanych przez AI lub wydajniejsze rozwiƒÖzanie chmurowe).\u001b[0m\n",
      "[2025-09-05 16:40:38,839][__main__][INFO] [Agent Og√≥lny] Odnaleziono 1 istotnych wspomnie≈Ñ.\n",
      "[2025-09-05 16:40:39,964][__main__][WARNING] Odpowied≈∫ odrzucona (za kr√≥tka): ''\n",
      "[2025-09-05 16:40:39,964][__main__][WARNING] Agent Og√≥lny nie wygenerowa≈Ç poprawnej odpowiedzi. Koniec tury.\n",
      "[2025-09-05 16:40:40,965][__main__][INFO] [Agent Finansowy] Odnaleziono 1 istotnych wspomnie≈Ñ.\n",
      "[2025-09-05 16:40:41,700][__main__][WARNING] Odpowied≈∫ odrzucona (za kr√≥tka): ''\n",
      "[2025-09-05 16:40:41,700][__main__][WARNING] Agent Finansowy nie wygenerowa≈Ç poprawnej odpowiedzi. Koniec tury.\n",
      "[2025-09-05 16:40:42,701][__main__][INFO] --- Koniec symulacji rozmowy ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ulepszona wersja skryptu do symulacji rozmowy miƒôdzy modelami LLM.\n",
    "\n",
    "G≈Ç√≥wne zmiany w wersji 8.0 (Lepsze Prompty i Parser):\n",
    "1.  **Inteligentny Parser Odpowiedzi (`_parse_and_validate_response`)**:\n",
    "    Zastƒôpuje prostƒÖ funkcjƒô czyszczƒÖcƒÖ. Potrafi teraz aktywnie usuwaƒá\n",
    "    ca≈Çe bloki \"my≈õli\" (np. <think>...</think>) i wyodrƒôbniaƒá w≈Ça≈õciwƒÖ\n",
    "    odpowied≈∫ z potencjalnie zaszumionego wyj≈õcia modelu.\n",
    "2.  **Nowa Technika Promptowania (\"Role-playing with examples\")**:\n",
    "    Prompt zosta≈Ç ca≈Çkowicie przebudowany. Zamiast listy suchych instrukcji,\n",
    "    teraz jasno definiuje rolƒô agenta i, co najwa≈ºniejsze, zawiera kr√≥tki\n",
    "    przyk≈Çad idealnej interakcji (tzw. few-shot prompting). Jest to\n",
    "    znacznie skuteczniejsza metoda instruowania modeli.\n",
    "3.  **Zwiƒôkszona Odporno≈õƒá**: Po≈ÇƒÖczenie lepszego promptu na wej≈õciu\n",
    "    i inteligentniejszego parsera na wyj≈õciu powinno radykalnie zwiƒôkszyƒá\n",
    "    odporno≈õƒá symulacji na \"lekkie\" b≈Çƒôdy modeli i zapewniƒá p≈Çynniejszy\n",
    "    przebieg rozmowy.\n",
    "\"\"\"\n",
    "import logging\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import nest_asyncio\n",
    "\n",
    "# --- Sprawdzenie dostƒôpno≈õci llama_cpp i definicje klas pozornych ---\n",
    "try:\n",
    "    from llama_cpp import Llama, LlamaCache, LLAMA_SPLIT_MODE_NONE\n",
    "    LLAMA_CPP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    logging.warning(\"Biblioteka 'llama_cpp' nie znaleziona. U≈ºywam obiekt√≥w pozornych (mock).\")\n",
    "    LLAMA_CPP_AVAILABLE = False\n",
    "    \n",
    "    class Llama:\n",
    "        def __init__(self, model_path: str = \"mock_path\", **kwargs):\n",
    "            self._model_name = Path(model_path).name\n",
    "            logging.info(f\"[MOCK] Inicjalizacja Llama dla modelu: {self._model_name}\")\n",
    "        def create_completion(self, prompt: str, **kwargs) -> Dict[str, Any]:\n",
    "            response_text = f\"<think>Model {self._model_name} my≈õli...</think> A oto dynamiczna, pozorowana odpowied≈∫ po polsku.\"\n",
    "            return {'choices': [{'text': response_text}]}\n",
    "        def embed(self, text: str) -> List[float]:\n",
    "            logging.info(f\"[MOCK] Tworzenie osadze≈Ñ dla tekstu: '{text[:40]}...'\")\n",
    "            np.random.seed(sum(ord(c) for c in text))\n",
    "            return np.random.randn(128).tolist()\n",
    "\n",
    "    class LlamaCache: pass\n",
    "    LLAMA_SPLIT_MODE_NONE = 0\n",
    "\n",
    "# --- Aliasy i Konfiguracja (bez zmian) ---\n",
    "ConversationHistory = List[Tuple[str, str]]\n",
    "MemoryEntry = Tuple[str, str, List[float]]\n",
    "\n",
    "@dataclass\n",
    "class LlamaParams:\n",
    "    n_gpu_layers: int = -1; main_gpu: int = 0; n_ctx: int = 4096\n",
    "    n_threads: int = 8; embedding: bool = False; verbose: bool = False\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    path: str; is_embedding: bool = False\n",
    "\n",
    "@dataclass\n",
    "class ModelsConfig:\n",
    "    \"\"\"Konfiguracja wszystkich modeli.\"\"\"\n",
    "    models: Dict[str, ModelConfig] = field(default_factory=lambda: {\n",
    "        \"base\": ModelConfig(path=os.getenv(\"BASE_MODEL_PATH\", \"/run/media/dominik/F2B452FDB452C42F/LLM models/Qwen3-Esper3-Reasoning-Instruct-6B-Brainstorm20x-Enhanced-E32-192k-ctx.i1-Q4_K_M.gguf\")),\n",
    "        \"finance_instruct\": ModelConfig(path=os.getenv(\"FINANCE_MODEL_PATH\", \"/run/media/dominik/F2B452FDB452C42F/LLM models/qwen3-4B-Claude-Sonnet-4-Reasoning-Distill_Q4_K_M.gguf\")),\n",
    "        \"embedding\": ModelConfig(path=os.getenv(\"EMBEDDING_MODEL_PATH\", \"/run/media/dominik/F2B452FDB452C42F/LLM models/nomic-embed-text-v1.5-Q4_K_M.gguf\"), is_embedding=True)\n",
    "    })\n",
    "\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    models: ModelsConfig = field(default_factory=ModelsConfig)\n",
    "    llama_params: LlamaParams = field(default_factory=LlamaParams)\n",
    "    max_tokens: int = 256\n",
    "    embedding_cache_size: int = 100\n",
    "    memory_retrieval_threshold: float = 0.65\n",
    "    repeat_penalty: float = 1.15\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s][%(name)s][%(levelname)s] %(message)s\", force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def cosine_similarity(v1: List[float], v2: List[float]) -> float:\n",
    "    vec1, vec2 = np.array(v1), np.array(v2)\n",
    "    dot = np.dot(vec1, vec2); norm = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    return dot / norm if norm > 0 else 0.0\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    return text.strip().replace(\"\\n\", \" \")[:512]\n",
    "\n",
    "# --- ZarzƒÖdzanie Modelami (bez zmian) ---\n",
    "class ModelLoader:\n",
    "    def __init__(self, config: AppConfig):\n",
    "        self.config = config; self.models: Dict[str, Llama] = {}\n",
    "        self.locks: Dict[str, asyncio.Lock] = {}; self.embedding_cache: Dict[str, List[float]] = {}\n",
    "\n",
    "    def _setup_single_model(self, model_name: str, model_config: ModelConfig) -> Optional[Llama]:\n",
    "        path = Path(model_config.path)\n",
    "        if not model_config.path or (LLAMA_CPP_AVAILABLE and not path.is_file()):\n",
    "            logger.warning(f\"Plik modelu '{model_name}' nie istnieje. U≈ºywam mocka.\")\n",
    "            return Llama(model_path=model_config.path, embedding=model_config.is_embedding)\n",
    "        try:\n",
    "            params = self.config.llama_params\n",
    "            llm = Llama(model_path=model_config.path, n_gpu_layers=params.n_gpu_layers,\n",
    "                        main_gpu=params.main_gpu, n_ctx=params.n_ctx, n_threads=params.n_threads,\n",
    "                        embedding=model_config.is_embedding, split_mode=LLAMA_SPLIT_MODE_NONE,\n",
    "                        cache=LlamaCache() if LLAMA_CPP_AVAILABLE else None, verbose=params.verbose)\n",
    "            logger.info(f\"Model '{model_name}' wczytany poprawnie.\")\n",
    "            return llm\n",
    "        except Exception as e:\n",
    "            logger.error(f\"B≈ÇƒÖd ≈Çadowania modelu '{model_name}': {e}\"); return None\n",
    "\n",
    "    async def load_all_models(self) -> bool:\n",
    "        load_tasks = {name: asyncio.to_thread(self._setup_single_model, name, mc) for name, mc in self.config.models.models.items()}\n",
    "        results = await asyncio.gather(*load_tasks.values())\n",
    "        all_loaded = True\n",
    "        for (name, _), model in zip(load_tasks.items(), results):\n",
    "            if model: self.models[name], self.locks[name] = model, asyncio.Lock()\n",
    "            else: all_loaded = False\n",
    "        return all_loaded\n",
    "\n",
    "    def unload_all_models(self):\n",
    "        self.models.clear(); self.locks.clear(); self.embedding_cache.clear()\n",
    "\n",
    "    def get_model_and_lock(self, name: str) -> Optional[Tuple[Llama, asyncio.Lock]]:\n",
    "        return self.models.get(name), self.locks.get(name)\n",
    "\n",
    "# --- Pamiƒôƒá Wektorowa (bez zmian) ---\n",
    "class VectorMemory:\n",
    "    def __init__(self):\n",
    "        self.entries: List[MemoryEntry] = []\n",
    "\n",
    "    def add_entry(self, speaker: str, text: str, embedding: List[float]):\n",
    "        self.entries.append((speaker, text, embedding))\n",
    "        logger.info(f\"[Pamiƒôƒá] Dodano wpis od '{speaker}'. Ca≈Çkowita liczba wpis√≥w: {len(self.entries)}.\")\n",
    "\n",
    "    def search_similar(self, query_embedding: List[float], top_k: int = 2, threshold: float = 0.65) -> List[MemoryEntry]:\n",
    "        if not self.entries: return []\n",
    "        similarities = [cosine_similarity(query_embedding, entry[2]) for entry in self.entries]\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        results = []\n",
    "        for i in sorted_indices:\n",
    "            if len(results) >= top_k: break\n",
    "            if similarities[i] >= threshold and i < len(self.entries) - 1:\n",
    "                results.append(self.entries[i])\n",
    "        return results\n",
    "\n",
    "# --- Agent Konwersacyjny (ZMODYFIKOWANY) ---\n",
    "class ConversationalAgent:\n",
    "    def __init__(self, name: str, model_loader: ModelLoader, completion_model: str, memory: VectorMemory):\n",
    "        self.name = name; self.model_loader = model_loader\n",
    "        self.completion_model = completion_model; self.memory = memory\n",
    "\n",
    "    async def _get_embedding(self, text: str) -> Optional[List[float]]:\n",
    "        model, lock = self.model_loader.get_model_and_lock(\"embedding\")\n",
    "        if not model: return None\n",
    "        preprocessed = preprocess_text(text)\n",
    "        if preprocessed in self.model_loader.embedding_cache:\n",
    "            return self.model_loader.embedding_cache[preprocessed]\n",
    "        try:\n",
    "            async with lock:\n",
    "                embedding = await asyncio.to_thread(model.embed, preprocessed)\n",
    "                if len(self.model_loader.embedding_cache) < self.model_loader.config.embedding_cache_size:\n",
    "                    self.model_loader.embedding_cache[preprocessed] = embedding\n",
    "                return embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"B≈ÇƒÖd tworzenia osadzenia dla '{text[:30]}...': {e}\"); return None\n",
    "\n",
    "    def _parse_and_validate_response(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Inteligentnie parsuje i waliduje odpowied≈∫ modelu.\"\"\"\n",
    "        # 1. Usu≈Ñ bloki <think>\n",
    "        cleaned_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
    "        \n",
    "        # 2. Podziel na linie i we≈∫ ostatniƒÖ niepustƒÖ - czƒôsto to jest w≈Ça≈õciwa odpowied≈∫\n",
    "        lines = [line.strip() for line in cleaned_text.split('\\n') if line.strip()]\n",
    "        if not lines:\n",
    "            logger.warning(\"Odpowied≈∫ odrzucona (pusta po parsowaniu).\")\n",
    "            return None\n",
    "        \n",
    "        final_response = lines[-1]\n",
    "        \n",
    "        # 3. Dodatkowe czyszczenie artefakt√≥w\n",
    "        final_response = re.sub(r'^\\s*(U≈ºytkownik|Agent Og√≥lny|Agent Finansowy):\\s*', '', final_response, flags=re.IGNORECASE).strip()\n",
    "        \n",
    "        # 4. Walidacja\n",
    "        if len(final_response) < 15:\n",
    "            logger.warning(f\"Odpowied≈∫ odrzucona (za kr√≥tka po parsowaniu): '{final_response}'\")\n",
    "            return None\n",
    "        \n",
    "        return final_response\n",
    "\n",
    "    async def _generate_response(self, history: ConversationHistory, retrieved_memories: List[MemoryEntry]) -> Optional[str]:\n",
    "        model, lock = self.model_loader.get_model_and_lock(self.completion_model)\n",
    "        if not model: return None\n",
    "        \n",
    "        formatted_history = \"\\n\".join([f\"{s}: {t}\" for s, t in history[-4:]])\n",
    "        \n",
    "        memory_section = \"Brak.\"\n",
    "        if retrieved_memories:\n",
    "            memory_texts = [f\"- {s} wcze≈õniej powiedzia≈Ç: '{t}'\" for s, t, _ in retrieved_memories]\n",
    "            memory_section = \"\\n\".join(memory_texts)\n",
    "\n",
    "        prompt = (\n",
    "            f\"Twoim jedynym zadaniem jest odegraƒá rolƒô asystenta AI o nazwie '{self.name}' w rozmowie.\\n\"\n",
    "            f\"Odpowied≈∫ MUSI byƒá JEDYNIE tekstem wypowiedzi w jƒôzyku polskim. NIE dodawaj swoich my≈õli, tag√≥w ani niczego innego.\\n\\n\"\n",
    "            f\"### PRZYK≈ÅAD POPRAWNEJ ODPOWIEDZI:\\n\"\n",
    "            f\"U≈ºytkownik: My≈õlƒô o nowym projekcie.\\n\"\n",
    "            f\"{self.name}: Brzmi interesujƒÖco, czy mo≈ºesz opowiedzieƒá o nim co≈õ wiƒôcej?\\n\\n\"\n",
    "            f\"### KONIEC PRZYK≈ÅADU ###\\n\\n\"\n",
    "            f\"--- AKTUALNA ROZMOWA ---\\n\"\n",
    "            f\"### Kluczowe fakty z tej rozmowy:\\n{memory_section}\\n\\n\"\n",
    "            f\"### Ostatnie wiadomo≈õci:\\n{formatted_history}\\n\\n\"\n",
    "            f\"--- KONIEC ROZMOWY ---\\n\\n\"\n",
    "            f\"{self.name}:\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            async with lock:\n",
    "                stop = [f\"{name}:\" for name in [\"U≈ºytkownik\", \"Agent Og√≥lny\", \"Agent Finansowy\"]]\n",
    "                output = await asyncio.to_thread(model.create_completion, prompt,\n",
    "                    max_tokens=self.model_loader.config.max_tokens,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    stop=stop,\n",
    "                    repeat_penalty=self.model_loader.config.repeat_penalty\n",
    "                )\n",
    "                raw_text = output['choices'][0]['text'].strip()\n",
    "                return self._parse_and_validate_response(raw_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"B≈ÇƒÖd generowania odpowiedzi: {e}\"); return None\n",
    "\n",
    "    async def respond(self, history: ConversationHistory) -> Optional[str]:\n",
    "        if not history: return None\n",
    "        last_utterance = history[-1][1]\n",
    "        query_embedding = await self._get_embedding(last_utterance)\n",
    "        if not query_embedding:\n",
    "            logger.warning(\"Nie uda≈Ço siƒô stworzyƒá wektora zapytania. Odpowiadam bez pamiƒôci.\")\n",
    "            return await self._generate_response(history, [])\n",
    "        retrieved_memories = self.memory.search_similar(query_embedding, top_k=2, threshold=self.model_loader.config.memory_retrieval_threshold)\n",
    "        if retrieved_memories:\n",
    "            logger.info(f\"[{self.name}] Odnaleziono {len(retrieved_memories)} istotnych wspomnie≈Ñ.\")\n",
    "        else:\n",
    "            logger.info(f\"[{self.name}] Brak istotnych wspomnie≈Ñ, odpowiadam na podstawie bie≈ºƒÖcego kontekstu.\")\n",
    "        return await self._generate_response(history, retrieved_memories)\n",
    "\n",
    "# --- Symulator Rozmowy (bez zmian) ---\n",
    "class ConversationSimulator:\n",
    "    def __init__(self, agents: List[ConversationalAgent], memory: VectorMemory, initial_prompt: Tuple[str, str], max_turns: int = 4):\n",
    "        self.agents = agents; self.memory = memory\n",
    "        self.history: ConversationHistory = []\n",
    "        self.initial_prompt = initial_prompt\n",
    "        self.max_turns = max_turns\n",
    "\n",
    "    async def initialize(self):\n",
    "        speaker, text = self.initial_prompt\n",
    "        embedding = await self.agents[0]._get_embedding(text)\n",
    "        if embedding:\n",
    "            self.history.append((speaker, text))\n",
    "            self.memory.add_entry(speaker, text, embedding)\n",
    "        else:\n",
    "            raise RuntimeError(\"Nie uda≈Ço siƒô zwektoryzowaƒá poczƒÖtkowego promptu.\")\n",
    "\n",
    "    async def run(self):\n",
    "        await self.initialize()\n",
    "        logger.info(\"--- Rozpoczynam symulacjƒô rozmowy (v8 Lepsze Prompty i Parser) ---\")\n",
    "        logger.info(f\"\\033[1;33m{self.history[0][0]}: {self.history[0][1]}\\033[0m\")\n",
    "        \n",
    "        current_agent_idx = 0\n",
    "        for _ in range(self.max_turns * len(self.agents)):\n",
    "            agent = self.agents[current_agent_idx]\n",
    "            response = await agent.respond(self.history)\n",
    "            \n",
    "            if response:\n",
    "                embedding = await agent._get_embedding(response)\n",
    "                if embedding:\n",
    "                    self.history.append((agent.name, response))\n",
    "                    self.memory.add_entry(agent.name, response, embedding)\n",
    "                    color = \"\\033[1;34m\" if current_agent_idx == 0 else \"\\033[1;32m\"\n",
    "                    logger.info(f\"{color}{agent.name}: {response}\\033[0m\")\n",
    "                else:\n",
    "                    logger.warning(f\"Nie uda≈Ço siƒô zwektoryzowaƒá ZWALIDOWANEJ odpowiedzi od {agent.name}.\")\n",
    "            else:\n",
    "                logger.warning(f\"{agent.name} nie wygenerowa≈Ç poprawnej odpowiedzi. Koniec tury.\")\n",
    "            \n",
    "            current_agent_idx = (current_agent_idx + 1) % len(self.agents)\n",
    "            await asyncio.sleep(1)\n",
    "        logger.info(\"--- Koniec symulacji rozmowy ---\")\n",
    "\n",
    "# --- G≈Ç√≥wna Funkcja Aplikacji (bez zmian) ---\n",
    "async def main():\n",
    "    config = AppConfig()\n",
    "    loader = ModelLoader(config)\n",
    "    try:\n",
    "        if await loader.load_all_models():\n",
    "            vector_memory = VectorMemory()\n",
    "            agent_a = ConversationalAgent(\"Agent Og√≥lny\", loader, \"base\", vector_memory)\n",
    "            agent_b = ConversationalAgent(\"Agent Finansowy\", loader, \"finance_instruct\", vector_memory)\n",
    "            \n",
    "            initial_prompt = (\"U≈ºytkownik\", \"Nasz nowy system logistyczny oparty na AI zredukowa≈Ç koszty o 20%, ale wymaga drogiej infrastruktury chmurowej. Zastanawiam siƒô nad jego rentowno≈õciƒÖ.\")\n",
    "            simulator = ConversationSimulator([agent_a, agent_b], vector_memory, initial_prompt, max_turns=3)\n",
    "            await simulator.run()\n",
    "    finally:\n",
    "        loader.unload_all_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        nest_asyncio.apply()\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cf90a",
   "metadata": {},
   "source": [
    "## Web Scraper - TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Any\n",
    "from urllib.parse import urlparse, quote_plus, parse_qs, unquote\n",
    "\n",
    "import httpx\n",
    "import cloudscraper\n",
    "from selectolax.parser import HTMLParser\n",
    "from trafilatura import extract\n",
    "import nest_asyncio\n",
    "\n",
    "# Nowy, potƒô≈ºniejszy arsena≈Ç\n",
    "from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "from playwright_stealth import Stealth\n",
    "\n",
    "# Zak≈Çadamy, ≈ºe plik Agent_Config.py z klasƒÖ AppConfig istnieje w tym samym folderze\n",
    "from Agent_Config import AppConfig\n",
    "\n",
    "# --- Konfiguracja ---\n",
    "config = AppConfig()\n",
    "\n",
    "# --- Ustawienia Logowania ---\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s][%(levelname)s] %(message)s\")\n",
    "for logger_name in [\"httpx\", \"selenium\", \"urllib3\", \"playwright\"]:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(\"ULTIMATE_SCRAPER\")\n",
    "\n",
    "# --- Katalog na Zrzuty Ekranu ---\n",
    "SCREENSHOT_DIR = \"screenshots\"\n",
    "os.makedirs(SCREENSHOT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "class UltimateScraper:\n",
    "    def __init__(self, app_config: AppConfig):\n",
    "        self.config = app_config\n",
    "        self._client: Optional[httpx.AsyncClient] = None\n",
    "        self._semaphore = asyncio.Semaphore(self.config.network.CONCURRENCY_LIMIT)\n",
    "        self.last_ip_rotation = 0\n",
    "        self.search_engines = [\n",
    "            {\n",
    "                \"name\": \"DuckDuckGo HTML\",\n",
    "                \"url\": \"https://html.duckduckgo.com/html/?q={query}\",\n",
    "                \"selector\": \"a.result__a\",\n",
    "                \"decode\": True\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    async def _rotate_ip(self) -> bool:\n",
    "        \"\"\"Zmienia adres IP Tora, wysy≈ÇajƒÖc sygna≈Ç NEWNYM.\"\"\"\n",
    "        if not self.config.tor.USE_TOR or time.time() - self.last_ip_rotation < 10:\n",
    "            return False\n",
    "\n",
    "        logger.info(\"üßÖ Wysy≈Çam ≈ºƒÖdanie nowej to≈ºsamo≈õci Tor (NEWNYM)...\")\n",
    "        writer = None\n",
    "        try:\n",
    "            reader, writer = await asyncio.open_connection(\n",
    "                self.config.tor.TOR_CONTROL_HOST, self.config.tor.TOR_CONTROL_PORT\n",
    "            )\n",
    "            auth_command = f'AUTHENTICATE \"{self.config.tor.TOR_CONTROL_PASSWORD}\"\\r\\n'.encode()\n",
    "            writer.write(auth_command)\n",
    "            await writer.drain()\n",
    "            if b\"250 OK\" not in await reader.read(4096):\n",
    "                logger.error(\"‚ùå B≈ÇƒÖd autoryzacji w ControlPort Tora.\")\n",
    "                return False\n",
    "\n",
    "            writer.write(b\"SIGNAL NEWNYM\\r\\n\")\n",
    "            await writer.drain()\n",
    "            if b\"250 OK\" in await reader.read(4096):\n",
    "                logger.info(\"‚úÖ Nowa to≈ºsamo≈õƒá Tora przyznana. Resetujƒô sesjƒô klienta.\")\n",
    "                if self._client and not self._client.is_closed:\n",
    "                    await self._client.aclose()\n",
    "                self._client = None\n",
    "                self.last_ip_rotation = time.time()\n",
    "                await asyncio.sleep(self.config.tor.NEWNYM_INTERVAL)\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(\"‚ùå Nie uda≈Ço siƒô wys≈Çaƒá sygna≈Çu NEWNYM.\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Krytyczny b≈ÇƒÖd podczas rotacji IP Tora: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if writer:\n",
    "                writer.close()\n",
    "                await writer.wait_closed()\n",
    "\n",
    "    async def _get_client(self) -> httpx.AsyncClient:\n",
    "        if self._client is None or self._client.is_closed:\n",
    "            proxy = self.config.tor.TOR_SOCKS_PROXY if self.config.tor.USE_TOR else None\n",
    "            transport = httpx.AsyncHTTPTransport(proxy=proxy) if proxy else None\n",
    "            self._client = httpx.AsyncClient(transport=transport, verify=False, timeout=self.config.network.REQ_TIMEOUT, follow_redirects=True, http2=True)\n",
    "            logger.info(f\"Zainicjowano nowƒÖ sesjƒô klienta httpx. Proxy: {proxy if transport else 'Brak'}\")\n",
    "        return self._client\n",
    "\n",
    "    def _decode_ddg_url(self, url: str) -> str:\n",
    "        if \"duckduckgo.com/l/\" in url:\n",
    "            try:\n",
    "                qs = parse_qs(urlparse(url).query); decoded = qs.get('uddg', [''])[0]\n",
    "                if decoded: return unquote(decoded)\n",
    "            except Exception: return url\n",
    "        return url\n",
    "\n",
    "    async def search(self, query: str) -> List[str]:\n",
    "        client = await self._get_client()\n",
    "        for engine in self.search_engines:\n",
    "            logger.info(f\"Wyszukujƒô '{query}' u≈ºywajƒÖc {engine['name']}...\")\n",
    "            search_url = engine['url'].format(query=quote_plus(query))\n",
    "            try:\n",
    "                response = await client.get(search_url, headers={\"User-Agent\": random.choice(self.config.network.USER_AGENTS)})\n",
    "                if response.status_code == 200:\n",
    "                    links = set()\n",
    "                    parser = HTMLParser(response.text)\n",
    "                    for node in parser.css(engine['selector']):\n",
    "                        href = node.attributes.get('href')\n",
    "                        if href:\n",
    "                            url = self._decode_ddg_url(href) if engine['decode'] else href\n",
    "                            if url.startswith('http'): links.add(url)\n",
    "                    if links:\n",
    "                        logger.info(f\"‚úÖ {engine['name']} znalaz≈Ç {len(links)} link√≥w.\")\n",
    "                        return list(links)\n",
    "                else:\n",
    "                    logger.warning(f\"{engine['name']} zwr√≥ci≈Ç status {response.status_code}. Pr√≥bujƒô nastƒôpny silnik...\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"B≈ÇƒÖd podczas wyszukiwania w {engine['name']}: {e}. Pr√≥bujƒô nastƒôpny silnik...\")\n",
    "        logger.error(f\"Wszystkie silniki wyszukiwania zawiod≈Çy dla zapytania: '{query}'.\")\n",
    "        return []\n",
    "\n",
    "    def _extract_content(self, html: str) -> Optional[str]:\n",
    "        content = extract(html, include_comments=False, include_tables=True)\n",
    "        if not content:\n",
    "            logger.warning(\"Ekstrakcja tre≈õci zwr√≥ci≈Ça pusty wynik.\")\n",
    "        else:\n",
    "            logger.info(f\"Ekstrakcja tre≈õci udana, d≈Çugo≈õƒá: {len(content)} znak√≥w.\")\n",
    "        return content\n",
    "\n",
    "    def _fetch_playwright_sync(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Ostateczna metoda pobierania, u≈ºywajƒÖca Playwright w trybie stealth zgodnie z dokumentacjƒÖ.\"\"\"\n",
    "        \n",
    "        with sync_playwright() as p:\n",
    "            # Konfigurujemy ustawienia proxy dla przeglƒÖdarki\n",
    "            proxy_settings = { \"server\": self.config.tor.TOR_SOCKS_PROXY } if self.config.tor.USE_TOR else None\n",
    "            \n",
    "            browser = p.chromium.launch(headless=True, proxy=proxy_settings)\n",
    "            context = browser.new_context(\n",
    "                user_agent=random.choice(self.config.network.USER_AGENTS),\n",
    "                viewport={'width': 1920, 'height': 1080}\n",
    "            )\n",
    "            page = context.new_page()\n",
    "            \n",
    "            stealth = Stealth()\n",
    "            stealth.apply_stealth_sync(page)\n",
    "\n",
    "            try:\n",
    "                logger.info(f\"Playwright (Stealth) nawiguje do: {url}\")\n",
    "                page.goto(url, timeout=90000, wait_until='networkidle')\n",
    "                \n",
    "                html = page.content()\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                safe_url = urlparse(url).netloc.replace('.', '_')\n",
    "                page.screenshot(path=os.path.join(SCREENSHOT_DIR, f\"playwright_{safe_url}_{timestamp}.png\"))\n",
    "                \n",
    "                return html if html and len(html) > 500 else None\n",
    "\n",
    "            except PlaywrightTimeoutError as e:\n",
    "                logger.error(f\"Playwright TimeoutError: {e}\")\n",
    "                return None\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Playwright og√≥lny b≈ÇƒÖd: {e}\")\n",
    "                return None\n",
    "            finally:\n",
    "                browser.close()\n",
    "\n",
    "    async def fetch(self, url: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:\n",
    "        for retry in range(max_retries):\n",
    "            logger.info(f\"Pr√≥ba fetch {retry + 1}/{max_retries} dla {url}\")\n",
    "            async with self._semaphore:\n",
    "                try:\n",
    "                    # Etap 1: httpx\n",
    "                    logger.info(f\"[1/3] Pr√≥ba pobrania (httpx): {url}\")\n",
    "                    client = await self._get_client()\n",
    "                    response = await client.get(url, headers={\"User-Agent\": random.choice(self.config.network.USER_AGENTS)})\n",
    "                    if response.status_code == 200 and \"js-challenge\" not in response.text:\n",
    "                        content = self._extract_content(response.text)\n",
    "                        if content:\n",
    "                            logger.info(f\"‚úÖ Sukces (httpx): {url}\")\n",
    "                            return {\"url\": url, \"content\": content, \"source\": \"httpx\"}\n",
    "                    else:\n",
    "                        logger.warning(f\"httpx zwr√≥ci≈Ç status {response.status_code} dla {url}\")\n",
    "                except (httpx.ProxyError, httpx.ConnectTimeout) as e:\n",
    "                    logger.error(f\"httpx b≈ÇƒÖd sieci/proxy dla {url}: {e}. Rotujƒô IP i ponawiam.\")\n",
    "                    if self.config.tor.USE_TOR:\n",
    "                        await self._rotate_ip()\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"httpx napotka≈Ç b≈ÇƒÖd dla {url}: {e}\")\n",
    "\n",
    "                # Etap 2: cloudscraper\n",
    "                logger.warning(f\"[2/3] httpx nie powi√≥d≈Ç siƒô. Pr√≥ba (ulepszony cloudscraper): {url}\")\n",
    "                try:\n",
    "                    loop = asyncio.get_running_loop()\n",
    "                    scraper = cloudscraper.create_scraper(\n",
    "                        browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True},\n",
    "                        delay=10, \n",
    "                        enable_stealth=True, \n",
    "                        stealth_options={'human_like_delays': True, 'randomize_headers': True, 'browser_quirks': True},\n",
    "                        auto_refresh_on_403=True,\n",
    "                        max_403_retries=2,\n",
    "                        rotate_tls_ciphers=True\n",
    "                    )\n",
    "                    proxies = {\"http\": self.config.tor.TOR_SOCKS_PROXY, \"https\": self.config.tor.TOR_SOCKS_PROXY} if self.config.tor.USE_TOR else None\n",
    "                    response = await loop.run_in_executor(None, lambda: scraper.get(url, proxies=proxies, timeout=self.config.network.REQ_TIMEOUT))\n",
    "                    if response.status_code == 200:\n",
    "                        content = self._extract_content(response.text)\n",
    "                        if content:\n",
    "                            logger.info(f\"‚úÖ Sukces (cloudscraper): {url}\")\n",
    "                            return {\"url\": url, \"content\": content, \"source\": \"cloudscraper_v2\"}\n",
    "                    else:\n",
    "                        logger.warning(f\"cloudscraper zwr√≥ci≈Ç status {response.status_code} dla {url}\")\n",
    "                except (httpx.ProxyError, httpx.ConnectTimeout) as e:\n",
    "                    logger.error(f\"cloudscraper b≈ÇƒÖd sieci/proxy dla {url}: {e}. Rotujƒô IP i ponawiam.\")\n",
    "                    if self.config.tor.USE_TOR:\n",
    "                        await self._rotate_ip()\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Ulepszony cloudscraper r√≥wnie≈º zawi√≥d≈Ç dla {url}: {e}\")\n",
    "\n",
    "                # Etap 3: Playwright\n",
    "                logger.warning(f\"[3/3] cloudscraper nie powi√≥d≈Ç siƒô. Ostateczna pr√≥ba (Playwright): {url}\")\n",
    "                try:\n",
    "                    loop = asyncio.get_running_loop()\n",
    "                    html = await asyncio.wait_for(\n",
    "                        loop.run_in_executor(None, self._fetch_playwright_sync, url),\n",
    "                        timeout=180.0  # Zwiƒôkszony timeout dla wolnych po≈ÇƒÖcze≈Ñ Tor\n",
    "                    )\n",
    "                    if html:\n",
    "                        content = self._extract_content(html)\n",
    "                        if content:\n",
    "                            logger.info(f\"‚úÖ Sukces (Playwright): {url}\")\n",
    "                            return {\"url\": url, \"content\": content, \"source\": \"playwright\"}\n",
    "                except asyncio.TimeoutError:\n",
    "                    logger.error(f\"‚ùå Ostateczna pr√≥ba (playwright) dla {url} przekroczy≈Ça limit czasu 180s. Rotujƒô IP i ponawiam.\")\n",
    "                    if self.config.tor.USE_TOR:\n",
    "                        await self._rotate_ip()\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"‚ùå Ostateczna pr√≥ba (playwright) dla {url} nie powiod≈Ça siƒô: {e}\")\n",
    "\n",
    "            logger.warning(f\"Wszystkie etapy nie powiod≈Çy siƒô w pr√≥bie {retry + 1}. Ponawiam...\")\n",
    "\n",
    "        logger.critical(f\"Pobranie {url} nie powiod≈Ço siƒô po {max_retries} pr√≥bach.\")\n",
    "        return None\n",
    "            \n",
    "    async def run(self, query: str, max_links: int = 7, failure_threshold: int = 4):\n",
    "        start_time = time.time()\n",
    "        await self.get_public_ip()\n",
    "        \n",
    "        links = await self.search(query)\n",
    "        if not links:\n",
    "            logger.critical(\"Nie uda≈Ço siƒô znale≈∫ƒá ≈ºadnych link√≥w. Zatrzymujƒô pracƒô.\")\n",
    "            return\n",
    "\n",
    "        ROTATE_EVERY_N_REQUESTS = 2\n",
    "        fetch_counter = 0\n",
    "        failed_count = 0\n",
    "        tasks = [self.fetch(url) for url in links[:max_links]]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        valid_results = []\n",
    "        for i, res in enumerate(results):\n",
    "            fetch_counter += 1\n",
    "            if isinstance(res, Exception):\n",
    "                logger.error(f\"Pobranie linku nr {i+1} ({links[i]}) nie powiod≈Ço siƒô: {res}\")\n",
    "                failed_count += 1\n",
    "            elif res is None:\n",
    "                logger.error(f\"Pobranie linku nr {i+1} ({links[i]}) zwr√≥ci≈Ço None.\")\n",
    "                failed_count += 1\n",
    "            else:\n",
    "                logger.info(f\"Dodano wynik dla {links[i]} (≈πr√≥d≈Ço: {res['source']}, D≈Çugo≈õƒá tre≈õci: {len(res['content'])})\")\n",
    "                valid_results.append(res)\n",
    "            \n",
    "            if failed_count >= failure_threshold:\n",
    "                logger.critical(f\"Przekroczono pr√≥g b≈Çƒôd√≥w ({failed_count}/{len(tasks)}). Rotujƒô IP.\")\n",
    "                if self.config.tor.USE_TOR:\n",
    "                    await self._rotate_ip()\n",
    "                break\n",
    "\n",
    "            if fetch_counter % ROTATE_EVERY_N_REQUESTS == 0 and fetch_counter < len(tasks):\n",
    "                logger.info(f\"OsiƒÖgniƒôto pr√≥g {ROTATE_EVERY_N_REQUESTS} zapyta≈Ñ. Prewencyjna rotacja IP.\")\n",
    "                if self.config.tor.USE_TOR:\n",
    "                    await self._rotate_ip()\n",
    "\n",
    "        logger.info(f\"\\n--- ZAKO≈ÉCZONO W {time.time() - start_time:.2f}s ---\")\n",
    "        logger.info(f\"Pomy≈õlnie pobrano {len(valid_results)} z {len(tasks)} stron.\")\n",
    "        \n",
    "        if not valid_results:\n",
    "            logger.warning(\"Brak wynik√≥w do wy≈õwietlenia.\")\n",
    "        else:\n",
    "            logger.info(f\"Liczba wa≈ºnych wynik√≥w: {len(valid_results)}\")\n",
    "            for i, doc in enumerate(valid_results):\n",
    "                logger.info(f\"Wy≈õwietlam dokument {i+1} (≈πr√≥d≈Ço: {doc['source']})\")\n",
    "                logger.info(f\"\\n===== DOKUMENT {i+1} (≈πr√≥d≈Ço: {doc['source']}) =====\\nURL: {doc['url']}\\nTre≈õƒá: {doc['content'][:500]}...\\n\")\n",
    "        \n",
    "        if self._client and not self._client.is_closed:\n",
    "            await self._client.aclose()\n",
    "            \n",
    "    # Metoda get_public_ip bez zmian...\n",
    "    async def get_public_ip(self) -> Optional[str]:\n",
    "        try:\n",
    "            client = await self._get_client()\n",
    "            response = await client.get(\"https://check.torproject.org/api/ip\")\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            logger.info(f\"Kontrola IP: Tw√≥j publiczny adres IP to {data.get('IP')} (U≈ºywasz Tora: {data.get('IsTor', False)})\")\n",
    "            return data.get('IP')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"B≈ÇƒÖd podczas kontroli IP: {e}\")\n",
    "            return None\n",
    "\n",
    "async def main():\n",
    "    app_config = AppConfig()\n",
    "    scraper = UltimateScraper(app_config)\n",
    "    await scraper.run(query=\"CLEOPATRA PREMIUM SP√ì≈ÅKA Z OGRANICZONƒÑ ODPOWIEDZIALNO≈öCIƒÑ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26050d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
